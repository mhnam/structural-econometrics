{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Bus Engine Replacement with Unobserved Heterogeneity\n",
    "\n",
    "This notebook implements the bus engine replacement model from:\n",
    "\n",
    "**\"Conditional Choice Probability Estimation of Dynamic Discrete Choice Models With Unobserved Heterogeneity\"**  \n",
    "*Arcidiacono & Miller, Econometrica (2011)*\n",
    "\n",
    "Building on the classic Rust (1987) framework.\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Part 1: Generic Framework\n",
    "- State enumeration and indexing\n",
    "- Fixed-point iteration\n",
    "- Abstract interface for dynamic models\n",
    "\n",
    "### Part 2: Bus Engine Model\n",
    "- State space: mileage × mileage-increment parameter\n",
    "- Transition: exponential mileage increments\n",
    "- Finite dependence: replacement resets mileage\n",
    "- Value function iteration and simulation\n",
    "\n",
    "### Part 3: Estimation Without Heterogeneity\n",
    "- **NFXP**: Nested fixed point (Rust 1987)\n",
    "- **Two-Step CCP**: Hotz-Miller inversion\n",
    "\n",
    "### Part 4: Estimation With Unobserved Heterogeneity\n",
    "- Why Two-Step fails with unknown types\n",
    "- **NFXP + EM**: Full solution with expectation-maximization\n",
    "- **CCP-EM (Data)**: Update CCPs via weighted logit\n",
    "- **CCP-EM (Model)**: Update CCPs via model operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note on Grid Resolution**: This notebook uses a coarse grid (x_step=0.5, z_step=0.05) for fast execution.\n",
    "> For research applications, use the fine grid (x_step=0.125, z_step=0.01) matching the original MATLAB code.\n",
    "> The coarse grid may cause parameter bias but correctly demonstrates the estimation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:49.730899Z",
     "iopub.status.busy": "2026-01-30T04:58:49.730796Z",
     "iopub.status.idle": "2026-01-30T04:58:50.612668Z",
     "shell.execute_reply": "2026-01-30T04:58:50.612181Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import expon\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Tuple, Dict, List, Optional, Callable, Any\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "EULER_CONSTANT = 0.5772156649015329"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Logit Utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.614022Z",
     "iopub.status.busy": "2026-01-30T04:58:50.613906Z",
     "iopub.status.idle": "2026-01-30T04:58:50.617218Z",
     "shell.execute_reply": "2026-01-30T04:58:50.616829Z"
    }
   },
   "source": [
    "def logit_prob(v: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Binary logit: convert value difference to choice probability.\n",
    "    \n",
    "    P(d=1) = exp(v) / (1 + exp(v)) where v = v_1 - v_0\n",
    "    \"\"\"\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    result = np.zeros_like(v)\n",
    "    pos = v >= 0\n",
    "    result[pos] = 1.0 / (1.0 + np.exp(-v[pos]))\n",
    "    result[~pos] = np.exp(v[~pos]) / (1.0 + np.exp(v[~pos]))\n",
    "    return result\n",
    "\n",
    "\n",
    "def log_odds(p: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Inverse logit (Hotz-Miller inversion): p → v_1 - v_0\"\"\"\n",
    "    p = np.clip(p, 1e-10, 1 - 1e-10)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "\n",
    "def emax_logit(v: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expected maximum of logit: E[max(v_0 + ε_0, v_1 + ε_1)]\n",
    "    \n",
    "    With v_0 = 0: log(1 + exp(v_1)) + γ\n",
    "    \"\"\"\n",
    "    # Numerically stable log-sum-exp\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    max_v = np.maximum(0, v)\n",
    "    return max_v + np.log(np.exp(-max_v) + np.exp(v - max_v)) + EULER_CONSTANT\n",
    "\n",
    "\n",
    "def emax_from_ccp(p: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ex-ante value from CCPs (Hotz-Miller).\n",
    "    \n",
    "    V(x) = γ - log(1 - p) where p = P(d=1|x)\n",
    "    (Assumes v_0 = 0 normalization)\n",
    "    \"\"\"\n",
    "    p = np.clip(p, 1e-10, 1 - 1e-10)\n",
    "    return EULER_CONSTANT - np.log(1 - p)\n",
    "\n",
    "\n",
    "# Quick tests\n",
    "print(\"Logit utilities:\")\n",
    "print(f\"  v=0 → p={logit_prob(0):.3f} (should be 0.5)\")\n",
    "print(f\"  v=2 → p={logit_prob(2):.3f}\")\n",
    "print(f\"  p=0.5 → V={emax_from_ccp(0.5):.3f} (should be γ + log(2) ≈ 1.27)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Generic Framework\n",
    "\n",
    "## Class Hierarchy\n",
    "\n",
    "```\n",
    "StateSpace              → State enumeration with index mapping\n",
    "FixedPointSolver        → Solves x* = T(x*) via iteration\n",
    "\n",
    "DynamicModel (ABC)      → Interface for any DDC model\n",
    "    ├── state_space, n_actions, discount\n",
    "    ├── flow_utility(x, d)\n",
    "    └── ccp_operator(ccps), solve()\n",
    "\n",
    "BusEngineModel          → Concrete implementation (Rust 1987)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.1 StateSpace"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.631814Z",
     "iopub.status.busy": "2026-01-30T04:58:50.631721Z",
     "iopub.status.idle": "2026-01-30T04:58:50.634015Z",
     "shell.execute_reply": "2026-01-30T04:58:50.633706Z"
    }
   },
   "source": [
    "class StateSpace:\n",
    "    \"\"\"\n",
    "    Maps states ↔ integer indices for array storage.\n",
    "    \n",
    "    For bus engine: state = (x_bin, z_bin) where\n",
    "    - x_bin: mileage bin index\n",
    "    - z_bin: mileage increment parameter bin index\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, states: List[Any]):\n",
    "        self._states = list(states)\n",
    "        self._index_map = {s: i for i, s in enumerate(self._states)}\n",
    "    \n",
    "    @property\n",
    "    def n_states(self) -> int:\n",
    "        return len(self._states)\n",
    "    \n",
    "    def state_to_index(self, state: Any) -> int:\n",
    "        return self._index_map[state]\n",
    "    \n",
    "    def index_to_state(self, idx: int) -> Any:\n",
    "        return self._states[idx]\n",
    "    \n",
    "    def states(self) -> List[Any]:\n",
    "        return self._states.copy()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._states)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"StateSpace(n_states={self.n_states})\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.2 FixedPointSolver"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.635204Z",
     "iopub.status.busy": "2026-01-30T04:58:50.635115Z",
     "iopub.status.idle": "2026-01-30T04:58:50.639311Z",
     "shell.execute_reply": "2026-01-30T04:58:50.638957Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class FixedPointResult:\n",
    "    \"\"\"Results from fixed-point iteration.\"\"\"\n",
    "    solution: np.ndarray\n",
    "    n_iterations: int\n",
    "    converged: bool\n",
    "    final_diff: float\n",
    "    computation_time: float\n",
    "\n",
    "\n",
    "class FixedPointSolver:\n",
    "    \"\"\"Solve x* = T(x*) by iteration.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 operator: Callable[[np.ndarray], np.ndarray],\n",
    "                 tolerance: float = 1e-8,\n",
    "                 max_iter: int = 1000,\n",
    "                 verbose: bool = True):\n",
    "        self.operator = operator\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def solve(self, initial: np.ndarray) -> FixedPointResult:\n",
    "        start = time.time()\n",
    "        x = initial.copy()\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            x_new = self.operator(x)\n",
    "            diff = np.max(np.abs(x_new - x))\n",
    "            \n",
    "            if self.verbose and (iteration + 1) % 100 == 0:\n",
    "                print(f\"  Iter {iteration+1}: diff = {diff:.2e}\")\n",
    "            \n",
    "            if diff < self.tolerance:\n",
    "                if self.verbose:\n",
    "                    print(f\"  Converged in {iteration+1} iterations\")\n",
    "                return FixedPointResult(x_new, iteration+1, True, diff, time.time()-start)\n",
    "            \n",
    "            x = x_new\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"  Did not converge after {self.max_iter} iterations\")\n",
    "        return FixedPointResult(x, self.max_iter, False, diff, time.time()-start)\n",
    "\n",
    "\n",
    "class DynamicModel(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for dynamic discrete choice models.\n",
    "    \n",
    "    Same interface as Tutorial 2 - ensures consistent structure across tutorials.\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def state_space(self) -> StateSpace:\n",
    "        \"\"\"The state space.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def n_actions(self) -> int:\n",
    "        \"\"\"Number of actions.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def discount(self) -> float:\n",
    "        \"\"\"Discount factor β.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def flow_utility(self, state: Any, action: int) -> float:\n",
    "        \"\"\"Flow utility u(x, d).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transition_probs(self, state: Any, action: int) -> Dict[Any, float]:\n",
    "        \"\"\"Transition probabilities P(x' | x, d).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def ccp_operator(self, ccps: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"CCP operator: maps CCPs to best-response CCPs.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def solve(self, \n",
    "              initial_ccps: np.ndarray = None,\n",
    "              tolerance: float = 1e-8,\n",
    "              max_iter: int = 1000,\n",
    "              verbose: bool = True) -> FixedPointResult:\n",
    "        \"\"\"\n",
    "        Solve for optimal/equilibrium CCPs using FixedPointSolver.\n",
    "        \n",
    "        This is the same pattern used in Tutorial 2.\n",
    "        \"\"\"\n",
    "        if initial_ccps is None:\n",
    "            initial_ccps = 0.5 * np.ones(self.state_space.n_states)\n",
    "        solver = FixedPointSolver(self.ccp_operator, tolerance, max_iter, verbose)\n",
    "        return solver.solve(initial_ccps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Bus Engine Model\n",
    "\n",
    "## The Rust (1987) Framework\n",
    "\n",
    "**Decision**: Each period, Harold Zurcher decides whether to:\n",
    "- $d=0$: Replace engine (reset mileage)\n",
    "- $d=1$: Keep engine (mileage accumulates)\n",
    "\n",
    "**State variables**:\n",
    "- $x$: Accumulated mileage (discretized)\n",
    "- $z$: Mileage increment parameter (bus-specific, time-invariant)\n",
    "- $s \\in \\{0, 1\\}$: Unobserved type (for heterogeneity extension)\n",
    "\n",
    "**Flow utility**:\n",
    "$$u(x, d, s) = \\begin{cases} \n",
    "0 & \\text{if } d = 0 \\text{ (replace, normalized)} \\\\\n",
    "\\alpha_1 + \\alpha_2 x + \\alpha_3 s & \\text{if } d = 1 \\text{ (keep)}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Transition** (mileage increment $\\Delta x \\sim \\text{Exp}(z)$):\n",
    "$$x' = \\begin{cases}\n",
    "\\Delta x & \\text{if } d = 0 \\text{ (reset)} \\\\\n",
    "x + \\Delta x & \\text{if } d = 1\n",
    "\\end{cases}$$\n",
    "\n",
    "**Finite dependence**: Replacement resets mileage → value difference simplifies.\n",
    "\n",
    "> **Convention note:** We use $d=1$ for \"keep\" and $d=0$ for \"replace.\" This differs from AM2011's $d_1$=replace, $d_2$=keep notation, but the economics is identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.640245Z",
     "iopub.status.busy": "2026-01-30T04:58:50.640188Z",
     "iopub.status.idle": "2026-01-30T04:58:50.646298Z",
     "shell.execute_reply": "2026-01-30T04:58:50.645971Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class BusEngineParams:\n",
    "    \"\"\"\n",
    "    Parameters for bus engine replacement model.\n",
    "    \n",
    "    Matches MATLAB busprograms structure.\n",
    "    \n",
    "    Flow Utility (for keep d=1)\n",
    "    ----------------------------------\n",
    "    u(x, s) = intercept + mileage_coef * x + type_coef * s\n",
    "    \n",
    "    where:\n",
    "    - x: mileage (in units of 10,000 miles)\n",
    "    - s: unobserved type (0 or 1)\n",
    "    \n",
    "    Replace (d=0) has utility normalized to 0.\n",
    "    \"\"\"\n",
    "    # Structural parameters\n",
    "    intercept: float = 2.0        # α_1: base replacement value\n",
    "    mileage_coef: float = -0.15   # α_2: mileage effect (negative → replace when high)\n",
    "    type_coef: float = 1.0        # α_3: type effect on replacement\n",
    "    discount: float = 0.9         # β: discount factor\n",
    "    type_prob: float = 0.4        # π: P(type = 0)\n",
    "    \n",
    "    # Initial values for estimation (can be modified for experiments)\n",
    "    theta0_intercept: float = 2.0     # Initial guess for α_1\n",
    "    theta0_mileage: float = -0.1      # Initial guess for α_2\n",
    "    theta0_type: float = 0.5          # Initial guess for α_3\n",
    "    \n",
    "    # State space discretization\n",
    "    x_min: float = 0.0             # Min mileage\n",
    "    x_max: float = 25.0          # Max mileage (25 = 250,000 miles)\n",
    "    x_step: float = 0.125        # May use coarser grid for demo (use 0.125 for full resolution)         # Mileage bin width\n",
    "    z_min: float = 0.25           # Min mileage increment rate\n",
    "    z_max: float = 1.25           # Max mileage increment rate\n",
    "    z_step: float = 0.01          # May use coarser grid for demo (use 0.01 for full resolution)          # Z bin width\n",
    "    \n",
    "    @property\n",
    "    def x_grid(self) -> np.ndarray:\n",
    "        \"\"\"Mileage grid points.\"\"\"\n",
    "        return np.arange(self.x_min, self.x_max + self.x_step/2, self.x_step)\n",
    "    \n",
    "    @property\n",
    "    def z_grid(self) -> np.ndarray:\n",
    "        \"\"\"Z parameter grid points.\"\"\"\n",
    "        return np.arange(self.z_min, self.z_max + self.z_step/2, self.z_step)\n",
    "    \n",
    "    @property\n",
    "    def n_x(self) -> int:\n",
    "        return len(self.x_grid)\n",
    "    \n",
    "    @property\n",
    "    def n_z(self) -> int:\n",
    "        return len(self.z_grid)\n",
    "    \n",
    "    def theta0_vector(self, fix_discount: bool = True) -> np.ndarray:\n",
    "        \"\"\"Initial parameter guess for estimation.\"\"\"\n",
    "        if fix_discount:\n",
    "            return np.array([self.theta0_intercept, self.theta0_mileage, self.theta0_type])\n",
    "        return np.array([self.theta0_intercept, self.theta0_mileage, self.theta0_type, self.discount])\n",
    "    \n",
    "    def theta_vector(self) -> np.ndarray:\n",
    "        \"\"\"Structural parameters as vector [α_1, α_2, α_3, β].\"\"\"\n",
    "        return np.array([self.intercept, self.mileage_coef, self.type_coef, self.discount])\n",
    "    \n",
    "    @classmethod\n",
    "    def from_vector(cls, theta: np.ndarray, **kwargs):\n",
    "        \"\"\"Create from parameter vector.\"\"\"\n",
    "        return cls(\n",
    "            intercept=theta[0],\n",
    "            mileage_coef=theta[1],\n",
    "            type_coef=theta[2] if len(theta) > 2 else 0.0,\n",
    "            discount=theta[3] if len(theta) > 3 else 0.9,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "# Display default parameters\n",
    "from tabulate import tabulate\n",
    "\n",
    "params = BusEngineParams()\n",
    "param_table = [\n",
    "    [\"Structural (α)\", str(params.theta_vector())],\n",
    "    [\"Type probability (π)\", params.type_prob],\n",
    "    [\"Mileage grid\", f\"{params.n_x} bins from {params.x_min} to {params.x_max}\"],\n",
    "    [\"Z grid\", f\"{params.n_z} bins from {params.z_min} to {params.z_max}\"],\n",
    "    [\"Total states per type\", params.n_x * params.n_z]\n",
    "]\n",
    "print(\"Bus Engine Parameters:\\n\")\n",
    "print(tabulate(param_table, headers=[\"Parameter\", \"Value\"], tablefmt=\"simple\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transition Matrices\n",
    "\n",
    "Mileage increment $\\Delta x \\sim \\text{Exp}(z)$ with rate parameter $z$.\n",
    "\n",
    "Discretized transition:\n",
    "$$P(x' | x, z, d=0) = F(x_{j+1} - x) - F(x_j - x)$$\n",
    "\n",
    "where $F(\\cdot)$ is the exponential CDF."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.647415Z",
     "iopub.status.busy": "2026-01-30T04:58:50.647345Z",
     "iopub.status.idle": "2026-01-30T04:58:50.649529Z",
     "shell.execute_reply": "2026-01-30T04:58:50.649180Z"
    }
   },
   "source": [
    "def build_transition_matrix(z: float, x_grid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build mileage transition matrix for given z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : float\n",
    "        Exponential rate parameter\n",
    "    x_grid : array\n",
    "        Mileage grid points\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    P : array, shape (n_x, n_x)\n",
    "        P[i, j] = P(x' = x_grid[j] | x = x_grid[i], d=0)\n",
    "    \"\"\"\n",
    "    n = len(x_grid)\n",
    "    x_ub = np.append(x_grid[1:], np.inf)  # Upper bounds for bins\n",
    "    \n",
    "    P = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        # CDF of increment reaching each bin\n",
    "        cdf = 1 - np.exp(-z * (x_ub - x_grid[i]))\n",
    "        cdf = np.clip(cdf, 0, 1)\n",
    "        \n",
    "        # Probability mass in each bin\n",
    "        P[i, :] = np.diff(np.insert(cdf, 0, 0))\n",
    "        \n",
    "        # Only transitions to higher states (or stay if at max)\n",
    "        P[i, :i] = 0\n",
    "        P[i, :] = P[i, :] / P[i, :].sum()  # Renormalize\n",
    "    \n",
    "    return P\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.650341Z",
     "iopub.status.busy": "2026-01-30T04:58:50.650284Z",
     "iopub.status.idle": "2026-01-30T04:58:50.652488Z",
     "shell.execute_reply": "2026-01-30T04:58:50.652202Z"
    }
   },
   "source": [
    "# Example: transition matrix for z = 0.5\n",
    "x_test = np.arange(0, 5, 0.5)\n",
    "P_test = build_transition_matrix(0.5, x_test)\n",
    "param_table = [\n",
    "    [\"Shape\", f\"{P_test.shape}\"],\n",
    "    [\"Row sums\", f\"{P_test.sum(axis=1)}\"],\n",
    "    [\"P[0,:5] (from x=0)\", f\"{P_test[0, :7].round(3)}\"],\n",
    "    [\"P[3,:7] (from x=1.5)\", f\"{P_test[3, :7].round(3)}\"],\n",
    "]\n",
    "\n",
    "print(\"Transition matrix (z=0.5, small grid):\\n\")\n",
    "print(tabulate(param_table, headers=[\"Type\", \"Value\"], tablefmt=\"simple\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 BusEngineModel Class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.653500Z",
     "iopub.status.busy": "2026-01-30T04:58:50.653439Z",
     "iopub.status.idle": "2026-01-30T04:58:50.658904Z",
     "shell.execute_reply": "2026-01-30T04:58:50.658599Z"
    }
   },
   "source": [
    "class BusEngineModel(DynamicModel):\n",
    "    \"\"\"\n",
    "    Bus engine replacement model (Rust 1987).\n",
    "    \n",
    "    Extends DynamicModel interface from Tutorial 2.\n",
    "    \n",
    "    State: (x_idx, z_idx) - mileage bin and z-parameter bin\n",
    "    Action: d ∈ {0, 1} - replace (0) or keep (1)\n",
    "    \n",
    "    For heterogeneity, we solve separately for each type s.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params: BusEngineParams = None, type_s: int = 0):\n",
    "        self.params = params or BusEngineParams()\n",
    "        self.type_s = type_s  # Type for this instance\n",
    "        self._build_state_space()\n",
    "        self._build_transitions()\n",
    "    \n",
    "    def _build_state_space(self):\n",
    "        \"\"\"Build state space: (x_idx, z_idx) pairs.\"\"\"\n",
    "        p = self.params\n",
    "        states = [\n",
    "            (x_idx, z_idx)\n",
    "            for z_idx in range(p.n_z)\n",
    "            for x_idx in range(p.n_x)\n",
    "        ]\n",
    "        self._state_space = StateSpace(states)\n",
    "        \n",
    "        # Precompute x and z values for each state index\n",
    "        self._x_vals = np.array([p.x_grid[s[0]] for s in states])\n",
    "        self._z_vals = np.array([p.z_grid[s[1]] for s in states])\n",
    "        self._x_idx = np.array([s[0] for s in states])\n",
    "        self._z_idx = np.array([s[1] for s in states])\n",
    "    \n",
    "    def _build_transitions(self):\n",
    "        \"\"\"\n",
    "        Build transition matrices for each z value.\n",
    "        \n",
    "        Stores:\n",
    "        - P_continue[z_idx]: transition if d=1 (keep, mileage accumulates)\n",
    "        - P_replace[z_idx]: transition if d=0 (replace, reset to x=0)\n",
    "        \"\"\"\n",
    "        p = self.params\n",
    "        self._P_continue = []\n",
    "        self._P_replace = []\n",
    "        \n",
    "        for z_idx, z in enumerate(p.z_grid):\n",
    "            P = build_transition_matrix(z, p.x_grid)\n",
    "            self._P_continue.append(P)\n",
    "            self._P_replace.append(P[0, :])  # First row = transitions from x=0\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DynamicModel Interface (from Tutorial 2)\n",
    "    # =========================================================================\n",
    "    \n",
    "    @property\n",
    "    def state_space(self) -> StateSpace:\n",
    "        return self._state_space\n",
    "    \n",
    "    @property\n",
    "    def n_actions(self) -> int:\n",
    "        return 2  # Continue or Replace\n",
    "    \n",
    "    @property\n",
    "    def discount(self) -> float:\n",
    "        return self.params.discount\n",
    "    \n",
    "    def flow_utility(self, state: Tuple[int, int], action: int) -> float:\n",
    "        \"\"\"\n",
    "        Flow utility u(x, d, s).\n",
    "        \n",
    "        Replace (d=0): u_0 = 0 (normalized)\n",
    "        Keep (d=1):    u_1 = α_1 + α_2 * x + α_3 * s\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        x_idx, z_idx = state\n",
    "        p = self.params\n",
    "        x = p.x_grid[x_idx]\n",
    "        # MATLAB: util1 = alpha(1) + alpha(2)*xval(x) + alpha(3)*s\n",
    "        # Uses raw x value (0 to 25), not x/10\n",
    "        return p.intercept + p.mileage_coef * x + p.type_coef * self.type_s\n",
    "    \n",
    "    def transition_probs(self, state: Tuple[int, int], action: int) -> Dict[Tuple[int, int], float]:\n",
    "        \"\"\"\n",
    "        Transition probabilities P(x' | x, d).\n",
    "        \n",
    "        Continue: mileage increases stochastically\n",
    "        Replace: mileage resets to 0, then increases\n",
    "        \"\"\"\n",
    "        x_idx, z_idx = state\n",
    "        p = self.params\n",
    "        \n",
    "        if action == 0:  # Replace (d=0)\n",
    "            P_row = self._P_replace[z_idx]\n",
    "        else:  # Keep (d=1)\n",
    "            P_row = self._P_continue[z_idx][x_idx, :]\n",
    "        \n",
    "        return {(x_new, z_idx): P_row[x_new] for x_new in range(p.n_x) if P_row[x_new] > 1e-10}\n",
    "    \n",
    "    def ccp_operator(self, ccps: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        CCP operator: Ψ(p) maps current CCPs to best-response CCPs.\n",
    "        \n",
    "        This is the KEY method that FixedPointSolver uses.\n",
    "        \n",
    "        For single-agent:\n",
    "        1. Compute value V(x) from CCPs via Hotz-Miller inversion\n",
    "        2. Compute expected continuation values\n",
    "        3. Compute value difference v_1 - v_0\n",
    "        4. Apply logit to get new CCPs\n",
    "        \"\"\"\n",
    "        p = self.params\n",
    "        n_x, n_z = p.n_x, p.n_z\n",
    "        \n",
    "        ccps = ccps.reshape(n_z, n_x)\n",
    "        ccps_new = np.zeros_like(ccps)\n",
    "        \n",
    "        # Flow utility for keep (d=1) (vectorized over x)\n",
    "        # MATLAB uses raw x values\n",
    "        u1 = p.intercept + p.mileage_coef * p.x_grid + p.type_coef * self.type_s\n",
    "        \n",
    "        for z_idx in range(n_z):\n",
    "            # Hotz-Miller: W(x) = γ - log(P(replace|x))\n",
    "            p_replace = np.clip(1 - ccps[z_idx, :], 1e-10, 1 - 1e-10)\n",
    "            V = EULER_CONSTANT - np.log(p_replace)\n",
    "            \n",
    "            # Expected continuation values\n",
    "            P_cont = self._P_continue[z_idx]\n",
    "            P_repl = self._P_replace[z_idx]\n",
    "\n",
    "            EV_continue = P_cont @ V      # E[W' | d=1, keep]\n",
    "            EV_replace = P_repl @ V       # E[W' | d=0, replace] (same for all x since reset)\n",
    "            \n",
    "            # Value difference: v_1 - v_0\n",
    "            v_diff = u1 + p.discount * (EV_continue - EV_replace)\n",
    "            \n",
    "            # New CCPs via logit\n",
    "            ccps_new[z_idx, :] = logit_prob(v_diff)\n",
    "        \n",
    "        return ccps_new.flatten()\n",
    "    \n",
    "    def solve(self, \n",
    "              initial_ccps: np.ndarray = None,\n",
    "              tolerance: float = 1e-8,\n",
    "              max_iter: int = 1000,\n",
    "              verbose: bool = True) -> FixedPointResult:\n",
    "        \"\"\"\n",
    "        Solve for optimal CCPs using FixedPointSolver.\n",
    "        \n",
    "        Finds p* such that p* = Ψ(p*).\n",
    "        \n",
    "        This is inherited pattern from Tutorial 2's DynamicModel.\n",
    "        \"\"\"\n",
    "        if initial_ccps is None:\n",
    "            initial_ccps = 0.1 * np.ones(self.state_space.n_states)\n",
    "        \n",
    "        solver = FixedPointSolver(self.ccp_operator, tolerance, max_iter, verbose)\n",
    "        return solver.solve(initial_ccps)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.659796Z",
     "iopub.status.busy": "2026-01-30T04:58:50.659741Z",
     "iopub.status.idle": "2026-01-30T04:58:50.675132Z",
     "shell.execute_reply": "2026-01-30T04:58:50.674803Z"
    }
   },
   "source": [
    "# Test model construction\n",
    "model = BusEngineModel()\n",
    "param_table = [\n",
    "    [\"States\", f\"{model.state_space.n_states}\"],\n",
    "    [\"State space\", f\"{model.state_space}\"],\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BusEngineModel\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(tabulate(param_table, headers=[\"Type\", \"Value\"], tablefmt=\"simple\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Solve and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.676112Z",
     "iopub.status.busy": "2026-01-30T04:58:50.676050Z",
     "iopub.status.idle": "2026-01-30T04:58:50.832024Z",
     "shell.execute_reply": "2026-01-30T04:58:50.831636Z"
    }
   },
   "source": [
    "# Solve for optimal CCPs using FixedPointSolver (same pattern as Tutorial 2)\n",
    "print(\"=\"*70)\n",
    "print(\"Solving for Optimal CCPs\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create models for each type\n",
    "model_type0 = BusEngineModel(type_s=0)\n",
    "model_type1 = BusEngineModel(type_s=1)\n",
    "\n",
    "# Solve using FixedPointSolver (via model.solve())\n",
    "print(\"\\nType 0:\")\n",
    "result_type0 = model_type0.solve(tolerance=1e-8, verbose=True)\n",
    "\n",
    "print(\"\\nType 1:\")\n",
    "result_type1 = model_type1.solve(tolerance=1e-8, verbose=True)\n",
    "\n",
    "# Extract CCPs\n",
    "ccps_type0 = result_type0.solution\n",
    "ccps_type1 = result_type1.solution\n",
    "\n",
    "print(f\"\\nBoth types converged: {result_type0.converged and result_type1.converged}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reshape for plotting\n",
    "p = model_type0.params\n",
    "ccps0 = ccps_type0.reshape(p.n_z, p.n_x)\n",
    "ccps1 = ccps_type1.reshape(p.n_z, p.n_x)\n",
    "\n",
    "# Plot replacement probability vs mileage for different z values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Sample z values (evenly spaced)\n",
    "z_indices = np.linspace(0, p.n_z - 1, 5, dtype=int)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(z_indices)))\n",
    "\n",
    "for ax, (ccps, title) in zip(axes, [(ccps0, 'Type s=0'), (ccps1, 'Type s=1')]):\n",
    "    for i, z_idx in enumerate(z_indices):\n",
    "        z_val = p.z_grid[z_idx]\n",
    "        ax.plot(p.x_grid, ccps[z_idx, :], color=colors[i], label=f'z={z_val:.2f}')\n",
    "    ax.set_xlabel('Mileage (x)')\n",
    "    ax.set_ylabel('P(Keep)')\n",
    "    ax.set_title(f'Keep Probability - {title}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Simulation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.833048Z",
     "iopub.status.busy": "2026-01-30T04:58:50.832972Z",
     "iopub.status.idle": "2026-01-30T04:58:50.836942Z",
     "shell.execute_reply": "2026-01-30T04:58:50.836582Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class SimulationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for simulation.\n",
    "\n",
    "    Matches MATLAB replication (hetero10ns/shellbusdatacomp420cond.m):\n",
    "      N=2000, T2=10 usable periods, burn-in=10 (total forward sim = 20)\n",
    "    \"\"\"\n",
    "    n_buses: int = 2000\n",
    "    n_periods: int = 10\n",
    "    burn_in: int = 10\n",
    "    seed: int = 42\n",
    "\n",
    "class Simulator:\n",
    "    \"\"\"\n",
    "    Simulate panel data from the bus engine model.\n",
    "    \n",
    "    Same structure as Tutorial 2's Simulator class.\n",
    "    \n",
    "    Generates bus-level observations with state variables and choices.\n",
    "    Each observation includes:\n",
    "    - bus_id: Bus identifier\n",
    "    - period: Time period\n",
    "    - x, x_idx: Mileage and bin index\n",
    "    - z, z_idx: Z parameter and bin index  \n",
    "    - type_s: Unobserved type (0 or 1)\n",
    "    - replace: Replacement decision (0 or 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_type0: BusEngineModel,\n",
    "                 model_type1: BusEngineModel,\n",
    "                 ccps_type0: np.ndarray,\n",
    "                 ccps_type1: np.ndarray,\n",
    "                 config: SimulationConfig):\n",
    "        self.model0 = model_type0\n",
    "        self.model1 = model_type1\n",
    "        self.ccps0 = ccps_type0\n",
    "        self.ccps1 = ccps_type1\n",
    "        self.config = config\n",
    "        self.params = model_type0.params\n",
    "    \n",
    "    def simulate(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Simulate panel data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Columns: bus_id, period, x, x_idx, z, z_idx, type_s, replace\n",
    "        \"\"\"\n",
    "        np.random.seed(self.config.seed)\n",
    "        p = self.params\n",
    "        \n",
    "        # Reshape CCPs for indexing\n",
    "        ccps0 = self.ccps0.reshape(p.n_z, p.n_x)\n",
    "        ccps1 = self.ccps1.reshape(p.n_z, p.n_x)\n",
    "        \n",
    "        # Draw types and z values for each bus\n",
    "        types = (np.random.random(self.config.n_buses) > p.type_prob).astype(int)\n",
    "        z_idx = np.random.randint(0, p.n_z, self.config.n_buses)\n",
    "        \n",
    "        # Initialize mileage at 0\n",
    "        x_idx = np.zeros(self.config.n_buses, dtype=int)\n",
    "        \n",
    "        records = []\n",
    "        total_periods = self.config.burn_in + self.config.n_periods\n",
    "        \n",
    "        for t in range(total_periods):\n",
    "            # Get CCPs for each bus based on type\n",
    "            ccps = np.where(\n",
    "                types == 0,\n",
    "                ccps0[z_idx, x_idx],\n",
    "                ccps1[z_idx, x_idx]\n",
    "            )\n",
    "            \n",
    "            # Draw replacement decisions\n",
    "            replace = (np.random.random(self.config.n_buses) >= ccps).astype(int)  # replace when random >= P(keep)\n",
    "            \n",
    "            # Record observations (after burn-in)\n",
    "            if t >= self.config.burn_in:\n",
    "                for i in range(self.config.n_buses):\n",
    "                    records.append({\n",
    "                        'bus_id': i,\n",
    "                        'period': t - self.config.burn_in,\n",
    "                        'x': p.x_grid[x_idx[i]],\n",
    "                        'x_idx': x_idx[i],\n",
    "                        'z': p.z_grid[z_idx[i]],\n",
    "                        'z_idx': z_idx[i],\n",
    "                        'type_s': types[i],\n",
    "                        'replace': replace[i]\n",
    "                    })\n",
    "            \n",
    "            # Transition: draw next mileage\n",
    "            for i in range(self.config.n_buses):\n",
    "                if replace[i] == 1:\n",
    "                    P_row = self.model0._P_replace[z_idx[i]]\n",
    "                else:\n",
    "                    P_row = self.model0._P_continue[z_idx[i]][x_idx[i], :]\n",
    "                \n",
    "                x_idx[i] = np.random.choice(p.n_x, p=P_row)\n",
    "        \n",
    "        return pd.DataFrame(records)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.837830Z",
     "iopub.status.busy": "2026-01-30T04:58:50.837764Z",
     "iopub.status.idle": "2026-01-30T04:58:50.950395Z",
     "shell.execute_reply": "2026-01-30T04:58:50.950005Z"
    }
   },
   "source": [
    "config = SimulationConfig()\n",
    "simulator = Simulator(model_type0, model_type1, ccps_type0, ccps_type1, config)\n",
    "data = simulator.simulate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Simulate data using Simulator class (same pattern as Tutorial 2)\n",
    "print(\"=\"*70)\n",
    "print(\"Simulating Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration table\n",
    "config_table = [\n",
    "    [\"n_buses\", config.n_buses],\n",
    "    [\"n_periods\", config.n_periods],\n",
    "    [\"burn_in\", config.burn_in],\n",
    "]\n",
    "print(\"\\nConfiguration:\")\n",
    "print(tabulate(config_table, headers=[\"Parameter\", \"Value\"], tablefmt=\"simple\"))\n",
    "\n",
    "# Generated data summary\n",
    "data_table = [\n",
    "    [\"Observations\", len(data)],\n",
    "    [\"Buses\", data['bus_id'].nunique()],\n",
    "    [\"Periods\", data['period'].nunique()],\n",
    "]\n",
    "print(\"\\nGenerated data:\")\n",
    "print(tabulate(data_table, headers=[\"Metric\", \"Value\"], tablefmt=\"simple\"))\n",
    "\n",
    "# Type distribution and replacement rate\n",
    "type_stats = data.groupby('type_s').agg(\n",
    "    count=('bus_id', 'size'),\n",
    "    replace_rate=('replace', 'mean')\n",
    ").reset_index()\n",
    "type_table = [[row['type_s'], row['count'], f\"{row['replace_rate']:.4f}\"]\n",
    "              for _, row in type_stats.iterrows()]\n",
    "print(\"\\nType distribution and replacement rate:\")\n",
    "print(tabulate(type_table, headers=[\"Type\", \"Count\", \"Replace Rate\"], tablefmt=\"simple\"))\n",
    "\n",
    "# Sample observations\n",
    "print(\"\\nSample observations:\")\n",
    "print(tabulate(data.head(10), headers='keys', tablefmt=\"simple\", showindex=False))\n",
    "\n",
    "# Keep reference to model for later use\n",
    "model = model_type0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:50.951739Z",
     "iopub.status.busy": "2026-01-30T04:58:50.951650Z",
     "iopub.status.idle": "2026-01-30T04:58:51.093913Z",
     "shell.execute_reply": "2026-01-30T04:58:51.093544Z"
    }
   },
   "source": [
    "# Visualize simulated data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# 1. Replacement rate by mileage\n",
    "ax = axes[0]\n",
    "for s in [0, 1]:\n",
    "    subset = data[data['type_s'] == s]\n",
    "    bins = pd.cut(subset['x'], bins=20)\n",
    "    rates = subset.groupby(bins)['replace'].mean()\n",
    "    ax.plot(range(len(rates)), rates.values, 'o-', label=f'Type {s}', alpha=0.7)\n",
    "ax.set_xlabel('Mileage bin')\n",
    "ax.set_ylabel('Replacement rate')\n",
    "ax.set_title('Replacement Rate by Mileage')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Mileage distribution\n",
    "ax = axes[1]\n",
    "for s in [0, 1]:\n",
    "    subset = data[data['type_s'] == s]\n",
    "    ax.hist(subset['x'], bins=30, alpha=0.5, label=f'Type {s}', density=True)\n",
    "ax.set_xlabel('Mileage (x)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Mileage Distribution by Type')\n",
    "ax.legend()\n",
    "\n",
    "# 3. Replacement rate over time\n",
    "ax = axes[2]\n",
    "for s in [0, 1]:\n",
    "    subset = data[data['type_s'] == s]\n",
    "    rates = subset.groupby('period')['replace'].mean()\n",
    "    ax.plot(rates.index, rates.values, 'o-', label=f'Type {s}', alpha=0.7)\n",
    "ax.set_xlabel('Period')\n",
    "ax.set_ylabel('Replacement rate')\n",
    "ax.set_title('Replacement Rate Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Estimation Without Heterogeneity\n",
    "\n",
    "We first consider estimation when type $s$ is **observed** (or there is no heterogeneity).\n",
    "\n",
    "## Methods\n",
    "\n",
    "| Method | Solves DP? | Iteration? | Pros | Cons |\n",
    "|--------|------------|------------|------|------|\n",
    "| **NFXP** | Yes (every θ) | No | Consistent, efficient | Slow |\n",
    "| **Two-Step CCP** | No | No | Very fast | Less efficient |\n",
    "\n",
    "Both work when types are known. The key difference:\n",
    "- NFXP: Solve Bellman equation at each parameter guess\n",
    "- Two-Step: Estimate CCPs once, then linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 NFXP Estimator (Rust 1987)\n",
    "\n",
    "**Algorithm**:\n",
    "1. Given parameter guess θ, solve value function V(x; θ)\n",
    "2. Compute choice probabilities P(d|x; θ)\n",
    "3. Evaluate log-likelihood\n",
    "4. Optimize over θ\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\sum_{i,t} \\log P(d_{it} | x_{it}; \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:51.095318Z",
     "iopub.status.busy": "2026-01-30T04:58:51.095232Z",
     "iopub.status.idle": "2026-01-30T04:58:51.099266Z",
     "shell.execute_reply": "2026-01-30T04:58:51.098911Z"
    }
   },
   "source": [
    "class NFXPEstimator:\n    \"\"\"\n    Nested Fixed Point estimator (Rust 1987).\n    \n    For each parameter guess, solves the full dynamic programming problem\n    using the model's solve() method (via FixedPointSolver).\n    \n    Estimates [α₁, α₂, α₃, logit(β)] matching MATLAB likebusML4.m.\n    β is recovered via inverse logit: β = exp(logit_β) / (1 + exp(logit_β)).\n    \"\"\"\n    \n    def __init__(self, data: pd.DataFrame, base_params: BusEngineParams = None):\n        self.data = data\n        self.base_params = base_params or BusEngineParams()\n        \n        # Extract data arrays for fast computation\n        self.x_idx = data['x_idx'].values\n        self.z_idx = data['z_idx'].values\n        self.replace = data['replace'].values\n        self.n_obs = len(data)\n        \n        # If type is observed, use it; otherwise assume homogeneous\n        if 'type_s' in data.columns:\n            self.type_s = data['type_s'].values\n        else:\n            self.type_s = np.zeros(self.n_obs, dtype=int)\n    \n    @staticmethod\n    def _logit_to_beta(logit_beta: float) -> float:\n        \"\"\"Inverse logit transform: β = exp(x)/(1+exp(x)).\"\"\"\n        return np.exp(logit_beta) / (1.0 + np.exp(logit_beta))\n    \n    @staticmethod\n    def _beta_to_logit(beta: float) -> float:\n        \"\"\"Logit transform: logit(β) = log(β) - log(1-β).\"\"\"\n        return np.log(beta) - np.log(1.0 - beta)\n    \n    def _theta_to_params(self, theta: np.ndarray) -> BusEngineParams:\n        \"\"\"\n        Convert optimization vector [α₁, α₂, α₃, logit(β)] to BusEngineParams.\n        Matches MATLAB likebusML4.m logit transform for β.\n        \"\"\"\n        alpha = theta[:3]\n        beta = self._logit_to_beta(theta[-1])\n        return BusEngineParams(\n            intercept=alpha[0],\n            mileage_coef=alpha[1],\n            type_coef=alpha[2],\n            discount=beta,\n        )\n    \n    def neg_log_likelihood(self, theta: np.ndarray) -> float:\n        \"\"\"\n        Compute negative log-likelihood.\n        \n        theta = [α₁, α₂, α₃, logit(β)] where β = σ(logit_β).\n        For each θ, creates models and solves for CCPs using FixedPointSolver.\n        \"\"\"\n        params = self._theta_to_params(theta)\n        \n        # Create and solve for each type\n        model0 = BusEngineModel(params, type_s=0)\n        model1 = BusEngineModel(params, type_s=1)\n        \n        # Use solve() which internally uses FixedPointSolver\n        result0 = model0.solve(tolerance=1e-6, max_iter=200, verbose=False)\n        result1 = model1.solve(tolerance=1e-6, max_iter=200, verbose=False)\n        \n        ccps0 = result0.solution.reshape(params.n_z, params.n_x)\n        ccps1 = result1.solution.reshape(params.n_z, params.n_x)\n        \n        # Get P(keep) for each observation (model computes P(d=1) = P(keep))\n        p_keep = np.where(\n            self.type_s == 0,\n            ccps0[self.z_idx, self.x_idx],\n            ccps1[self.z_idx, self.x_idx]\n        )\n        p_keep = np.clip(p_keep, 1e-10, 1 - 1e-10)\n        \n        # Log-likelihood: P(keep) when replace=0, P(replace)=1-P(keep) when replace=1\n        ll = np.sum(\n            (1 - self.replace) * np.log(p_keep) + \n            self.replace * np.log(1 - p_keep)\n        )\n        \n        return -ll\n    \n    def estimate(self, theta0: np.ndarray = None) -> dict:\n        \"\"\"\n        Estimate parameters [α₁, α₂, α₃, β] via MLE.\n        \n        Optimizes over [α₁, α₂, α₃, logit(β)] and converts back.\n        Matches MATLAB likebusML4.m logit transform for β.\n        \"\"\"\n        if theta0 is None:\n            logit_beta0 = self._beta_to_logit(self.base_params.discount)\n            theta0 = np.array([\n                self.base_params.theta0_intercept,\n                self.base_params.theta0_mileage,\n                self.base_params.theta0_type,\n                logit_beta0,\n            ])\n        \n        start_time = time.time()\n        \n        result = minimize(self.neg_log_likelihood, theta0, method='Nelder-Mead',\n                         options={'maxiter': 500, 'disp': True})\n        \n        # Convert back: replace logit(β) with β in the result vector\n        theta_hat = result.x.copy()\n        theta_hat[-1] = self._logit_to_beta(result.x[-1])\n        \n        return {\n            'theta': theta_hat,\n            'log_likelihood': -result.fun,\n            'converged': result.success,\n            'n_iterations': result.nit,\n            'computation_time': time.time() - start_time\n        }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:51.100217Z",
     "iopub.status.busy": "2026-01-30T04:58:51.100153Z",
     "iopub.status.idle": "2026-01-30T04:58:57.502295Z",
     "shell.execute_reply": "2026-01-30T04:58:57.501897Z"
    }
   },
   "source": [
    "# Run NFXP estimation (with known types)\nprint(\"=\"*70)\nprint(\"NFXP Estimation (types observed)\")\nprint(\"=\"*70)\n\nnfxp = NFXPEstimator(data)\nresult_nfxp = nfxp.estimate()\n\n# Display results using tabulate\ntrue_params = model.params.theta_vector()\nest_params = result_nfxp['theta']\nparam_names = ['alpha_1 (RC)', 'alpha_2 (theta_c)', 'alpha_3 (theta_s)', 'beta (discount)']\n\nresults_table = []\nfor i, name in enumerate(param_names):\n    results_table.append([name, f\"{true_params[i]:.4f}\", f\"{est_params[i]:.4f}\"])\n\nprint(\"\\nParameter Estimates:\")\nprint(tabulate(results_table, headers=[\"Parameter\", \"True\", \"Estimated\"], tablefmt=\"simple\"))\n\nstats_table = [\n    [\"Log-likelihood\", f\"{result_nfxp['log_likelihood']:.2f}\"],\n    [\"Converged\", result_nfxp['converged']],\n    [\"Iterations\", result_nfxp['n_iterations']],\n    [\"Computation time (s)\", f\"{result_nfxp['computation_time']:.2f}\"],\n]\nprint(\"\\nEstimation Statistics:\")\nprint(tabulate(stats_table, headers=[\"Metric\", \"Value\"], tablefmt=\"simple\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Two-Step CCP Estimator (Hotz-Miller 1993)\n\n**Key insight**: With finite dependence, we can avoid solving the DP.\n\n### Two-Step Procedure\n\n**Step 1: Estimate CCPs via Reduced-Form Logit**\n\nEstimate a flexible logit model for replacement probabilities:\n$$\\hat{p}_0(x, z, s) = P(d=0|x, z, s) = \\Lambda(\\mathbf{x}'\\hat{\\gamma})$$\n\nwhere $d=0$ is the **replacement** (base) action with utility normalized to zero.\n\n**Step 2: Hotz-Miller Inversion**\n\nThe ex-ante value function can be recovered from CCPs using Hotz-Miller inversion:\n$$\\bar{V}(x) = \\gamma - \\log(p_0(x))$$\n\nwhere:\n- $\\gamma \\approx 0.5772$ is the Euler-Mascheroni constant\n- $p_0(x) = P(d=0|x)$ is the probability of the **base action** (replacement)\n\n**Why $p_0$ (not $p_1$)?** The Hotz-Miller inversion formula uses the probability of the action whose flow utility is normalized to zero. In the bus engine model, replacement ($d=0$) has $u_0 = 0$, so we use $p_0 = P(\\text{replace})$.\n\n### Future Value Difference\n\nDefine the future value difference:\n$$FV(x) = \\beta \\cdot \\big[E[\\bar{V}(x') | x, d=1] - E[\\bar{V}(x') | x, d=0]\\big]$$\n\nwhere:\n- $d=1$ (continue): next-period mileage evolves from current $x$\n- $d=0$ (replace): next-period mileage starts from $x=0$\n\n### Second Stage GMM/MLE\n\nThe choice probability for continuing ($d=1$) is:\n$$P(d=1|x) = \\Lambda\\big(u_1(x) + FV(x)\\big) = \\Lambda\\big(\\alpha_1 + \\alpha_2 x + \\alpha_3 s + FV(x)\\big)$$\n\nEstimate $\\theta = (\\alpha_1, \\alpha_2, \\alpha_3)$ via MLE with $FV(x)$ treated as a known regressor with coefficient constrained to 1 (or estimated freely as a specification check).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:57.503596Z",
     "iopub.status.busy": "2026-01-30T04:58:57.503496Z",
     "iopub.status.idle": "2026-01-30T04:58:57.537173Z",
     "shell.execute_reply": "2026-01-30T04:58:57.536724Z"
    }
   },
   "source": [
    "class TwoStepCCPEstimator:\n    \"\"\"\n    Two-step CCP estimator (Hotz-Miller 1993).\n\n    Matches MATLAB replication convention (hetero10ns):\n    - First stage: logit for P(replace) with base + type interactions + time dummies\n    - FV = -log(P(replace)), WITHOUT β\n    - Second stage: P(keep) = Λ(α₁ + α₂x + α₃s + β·FV + time_dummies)\n    - β estimated as coefficient on FV\n    - Last period excluded (terminal FV = 0)\n\n    MATLAB CONVENTIONS (shellbusdatacomp420cond.m):\n    - d=0: Replace (base, utility=0); d=1: Keep (flow utility = α₁ + α₂x + α₃s)\n    - First stage models P(replace) via wlogitd with y2==0\n    - FV = -log(P(replace)), no β, no γ (Euler cancels in difference)\n    - Second stage: wlogit with Y=1 is keep, coefficient on FV estimates β\n    \"\"\"\n\n    def __init__(self, data: pd.DataFrame, model: BusEngineModel):\n        self.data = data\n        self.model = model\n        self.params = model.params\n        self._P_continue = model._P_continue\n        self._P_replace = model._P_replace\n\n    def first_stage(self) -> np.ndarray:\n        \"\"\"\n        Estimate CCPs via reduced-form logit.\n\n        Models P(replace|x,z,s,t) with:\n        - Base: [1, x/10, z, xz, x², z²]\n        - Type interactions: [s, sx, sz, sxz, sx², sz²]\n        - Time dummies for periods 1,...,T-1 (period 0 = baseline)\n\n        MATLAB: wlogitd(b1, y2==0, xx, PType)\n        \"\"\"\n        df = self.data\n        p = self.params\n        T = df['period'].max() + 1\n\n        x_norm = df['x'].values / 10\n        z = df['z'].values\n        s = df['type_s'].values\n\n        X_base = np.column_stack([\n            np.ones(len(df)),\n            x_norm, z, x_norm * z, x_norm**2, z**2,\n            s, s * x_norm, s * z, s * x_norm * z, s * x_norm**2, s * z**2,\n        ])\n\n        # Time dummies (period 0 is baseline)\n        td = np.zeros((len(df), T - 1))\n        for t in range(1, T):\n            td[:, t - 1] = (df['period'].values == t).astype(float)\n\n        X = np.column_stack([X_base, td])\n\n        # Model P(replace): y=1 when replace (MATLAB: wlogitd with y2==0)\n        y = df['replace'].values.astype(float)\n\n        def neg_ll(beta):\n            prob = logit_prob(X @ beta)\n            prob = np.clip(prob, 1e-10, 1 - 1e-10)\n            return -np.sum(y * np.log(prob) + (1 - y) * np.log(1 - prob))\n\n        beta0 = np.zeros(X.shape[1])\n        result = minimize(neg_ll, beta0, method='BFGS', options={'disp': False})\n        self.first_stage_beta = result.x\n        self._T = T\n\n        return self.first_stage_beta\n\n    def compute_fv_terms(self, beta: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute FV terms WITHOUT β (MATLAB fvdata.m).\n\n        MATLAB convention:\n            FV1(:,s,t) = -log(P(replace|state, s, t))\n            FV_obs(n,t) = (P_keep_trans - P_repl_trans) @ FV1(:, s, t+1)\n\n        No β, no Euler constant (γ cancels in transition difference).\n        β is estimated as a free coefficient in the second stage.\n        \"\"\"\n        p = self.params\n        n_x, n_z = p.n_x, p.n_z\n        T = self._T\n        tbin = n_x * n_z\n\n        # Grid values: states ordered z-major (MATLAB: adj = x + (z-1)*xbin)\n        xvalr = np.tile(p.x_grid / 10, n_z)\n        zvalr = np.repeat(p.z_grid, n_x)\n\n        # FV1[state, type, period] = -log(P(replace))\n        FV1 = np.zeros((tbin, 2, T + 1))\n\n        for t_pred in range(1, T):  # periods 1,...,T-1; FV at 0 and T stay 0\n            td_grid = np.zeros((tbin, T - 1))\n            td_grid[:, t_pred - 1] = 1.0\n\n            for s in [0, 1]:\n                X_grid = np.column_stack([\n                    np.ones(tbin),\n                    xvalr, zvalr, xvalr * zvalr, xvalr**2, zvalr**2,\n                    np.full(tbin, s), s * xvalr, s * zvalr,\n                    s * xvalr * zvalr, s * xvalr**2, s * zvalr**2,\n                    td_grid\n                ])\n\n                p_replace = logit_prob(X_grid @ beta)\n                p_replace = np.clip(p_replace, 1e-10, 1 - 1e-10)\n                FV1[:, s, t_pred] = -np.log(p_replace)\n\n        # Observation-level FV: (P_keep - P_replace) @ FV1[z_slice, s, t+1]\n        df = self.data\n        fv = np.zeros(len(df))\n\n        for idx in df.index:\n            z_idx = int(df.loc[idx, 'z_idx'])\n            x_idx = int(df.loc[idx, 'x_idx'])\n            t = int(df.loc[idx, 'period'])\n            s = int(df.loc[idx, 'type_s'])\n\n            z_start = z_idx * n_x\n            z_end = (z_idx + 1) * n_x\n\n            fv[idx] = (self._P_continue[z_idx][x_idx, :] - self._P_replace[z_idx]) \\\n                       @ FV1[z_start:z_end, s, t + 1]\n\n        return fv\n\n    def second_stage(self) -> dict:\n        \"\"\"\n        Second stage: structural logit with β as free parameter.\n\n        v₁ - v₀ = α₁ + α₂·x + α₃·s + β·FV + γ·td\n\n        MATLAB: wlogit(bccp, y2, [xccp fvt1 td], PType)\n        - xccp = [1, x*10, s]  (raw x, not x/10)\n        - bccp = [α₁, α₂, α₃, β, time_dummies]\n        - Excludes last period (FV=0 at terminal)\n        - Time dummies for periods 1,...,T-2 (period 0 baseline, last excluded)\n        \"\"\"\n        beta = self.first_stage()\n        fv = self.compute_fv_terms(beta)\n\n        df = self.data.copy()\n        p = self.params\n        T = self._T\n\n        # Exclude last period (FV=0 at terminal)\n        mask = df['period'].values < (T - 1)\n        df_est = df[mask]\n        fv_est = fv[mask]\n\n        # Time dummies (period 0 baseline, periods 1...T-2)\n        n_td = T - 2\n        td = np.zeros((mask.sum(), n_td))\n        periods_est = df_est['period'].values\n        for t in range(1, T - 1):\n            td[:, t - 1] = (periods_est == t).astype(float)\n\n        # [1, x, s, FV, time_dummies] — x is raw mileage (MATLAB: x2*10)\n        X2 = np.column_stack([\n            np.ones(mask.sum()),\n            df_est['x'].values,\n            df_est['type_s'].values,\n            fv_est,\n            td\n        ])\n\n        # y=1 when keep (MATLAB: wlogit with Y=y2, Y=1 is keep)\n        y2 = (df_est['replace'].values == 0).astype(float)\n\n        def neg_ll(alpha):\n            v = X2 @ alpha\n            prob = logit_prob(v)\n            prob = np.clip(prob, 1e-10, 1 - 1e-10)\n            return -np.sum(y2 * np.log(prob) + (1 - y2) * np.log(1 - prob))\n\n        # Initial: [α₁, α₂, α₃, β, time_dummies]\n        alpha0 = np.concatenate([\n            np.array([2.0, -0.1, 0.5, p.discount]),\n            np.zeros(n_td)\n        ])\n        result = minimize(neg_ll, alpha0, method='BFGS')\n\n        theta_hat = result.x[:4]  # [α₁, α₂, α₃, β]\n\n        return {\n            'theta': theta_hat,\n            'time_dummy_coefs': result.x[4:],\n            'log_likelihood': -result.fun,\n            'first_stage_beta': beta\n        }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:57.538312Z",
     "iopub.status.busy": "2026-01-30T04:58:57.538245Z",
     "iopub.status.idle": "2026-01-30T04:58:58.360375Z",
     "shell.execute_reply": "2026-01-30T04:58:58.360014Z"
    }
   },
   "source": [
    "# Run Two-Step CCP estimation\nprint(\"=\"*70)\nprint(\"Two-Step CCP Estimation (types observed)\")\nprint(\"=\"*70)\n\nstart = time.time()\ntwostep = TwoStepCCPEstimator(data, model)\nresult_twostep = twostep.second_stage()\nelapsed = time.time() - start\n\n# Display results using tabulate\ntrue_params = model.params.theta_vector()\nest_params = result_twostep['theta']\nparam_names = ['alpha_1 (intercept)', 'alpha_2 (mileage)', 'alpha_3 (type)', 'beta (discount)']\n\nresults_table = []\nfor i, name in enumerate(param_names):\n    results_table.append([name, f\"{true_params[i]:.4f}\", f\"{est_params[i]:.4f}\"])\n\nprint(\"\\nParameter Estimates:\")\nprint(tabulate(results_table, headers=[\"Parameter\", \"True\", \"Estimated\"], tablefmt=\"simple\"))\n\nstats_table = [\n    [\"Log-likelihood\", f\"{result_twostep['log_likelihood']:.2f}\"],\n    [\"Computation time (s)\", f\"{elapsed:.2f}\"],\n]\nprint(\"\\nEstimation Statistics:\")\nprint(tabulate(stats_table, headers=[\"Metric\", \"Value\"], tablefmt=\"simple\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Comparison: NFXP vs Two-Stage CCP\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | NFXP (Rust 1987) | Two-Stage CCP (Hotz-Miller 1993) |\n",
    "|--------|------------------|----------------------------------|\n",
    "| **Approach** | Nested optimization | Sequential estimation |\n",
    "| **Inner loop** | Solve DP for each θ guess | None (CCPs estimated once from data) |\n",
    "| **Outer loop** | MLE over θ | Regression of Hotz-Miller equation |\n",
    "| **Computation** | $O(K \\times T_{inner})$ where K=outer iterations | $O(N)$ where N=sample size |\n",
    "| **Efficiency** | MLE efficient (achieves Cramér-Rao bound) | Less efficient (first-stage error propagates) |\n",
    "\n",
    "### Algorithm Comparison\n",
    "\n",
    "**NFXP (Nested Fixed Point)**:\n",
    "```\n",
    "For each θ candidate in optimization:\n",
    "    1. Solve value function: V(x;θ) via contraction iteration\n",
    "       - Initialize V⁰\n",
    "       - Iterate: V^{k+1} = Γ(V^k; θ) until convergence\n",
    "    2. Compute CCPs from V: p(x;θ) = Λ(v₁(x;θ) - v₀(x;θ))\n",
    "    3. Evaluate likelihood: L(θ) = Π P(d_i|x_i; θ)\n",
    "```\n",
    "\n",
    "**Two-Stage CCP**:\n",
    "```\n",
    "Stage 1: Estimate CCPs non-parametrically\n",
    "    - Flexible logit: p̂(x) = Λ(X'γ) with polynomials/interactions\n",
    "    - No structural model needed\n",
    "\n",
    "Stage 2: Use Hotz-Miller inversion\n",
    "    - Compute ex-ante values: V̄(x) = γ - log(p̂(x))\n",
    "    - Identify θ from: log(p̂/(1-p̂)) = u(x;θ) + β·FV(x)\n",
    "    - Solve linear regression (fast!)\n",
    "```\n",
    "\n",
    "### Why Two-Stage Works (Finite Dependence Property)\n",
    "\n",
    "The bus engine model has **finite dependence** because replacement ($d=0$) is a *renewal action*:\n",
    "- After replacement, mileage resets to $x=0$\n",
    "- The continuation value $E[V(x')|d=0]$ is the same for all starting states\n",
    "- This enables the Hotz-Miller inversion to identify θ without solving the full DP\n",
    "\n",
    "$$\\bar{V}(x) - \\bar{V}(0) = -\\ln p_0(x) + \\ln p_0(0)$$\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "| Scenario | Recommended Method |\n",
    "|----------|-------------------|\n",
    "| Small state space, need efficiency | NFXP |\n",
    "| Large state space | Two-Stage CCP |\n",
    "| Multiple specifications to test | Two-Stage CCP |\n",
    "| Final estimates for publication | NFXP (for efficiency) |\n",
    "| Robustness checks | Two-Stage CCP |\n",
    "\n",
    "### Trade-off Summary\n",
    "\n",
    "- **NFXP**: Slower but statistically efficient (MLE). Solves the full model, ensuring internal consistency.\n",
    "- **Two-Stage**: Much faster but less efficient. First-stage estimation error propagates to second stage.\n",
    "\n",
    "The efficiency loss from Two-Stage is typically small when:\n",
    "1. Sample size is large (first-stage CCPs are precisely estimated)\n",
    "2. State space is well-covered by data\n",
    "3. The finite dependence property holds\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:58.361708Z",
     "iopub.status.busy": "2026-01-30T04:58:58.361622Z",
     "iopub.status.idle": "2026-01-30T04:58:58.363858Z",
     "shell.execute_reply": "2026-01-30T04:58:58.363448Z"
    }
   },
   "source": [
    "# Summary comparison\nprint(\"=\"*70)\nprint(\"Comparison: Estimation Without Heterogeneity\")\nprint(\"=\"*70)\n\ntrue_params = model.params.theta_vector()\nnfxp_params = result_nfxp['theta']\ntwostep_params = result_twostep['theta']\nparam_names = ['alpha_1 (intercept)', 'alpha_2 (mileage)', 'alpha_3 (type)', 'beta (discount)']\n\ncomparison_table = []\nfor i, name in enumerate(param_names):\n    comparison_table.append([\n        name,\n        f\"{true_params[i]:.4f}\",\n        f\"{nfxp_params[i]:.4f}\",\n        f\"{twostep_params[i]:.4f}\"\n    ])\n\nprint(\"\\nParameter Estimates:\")\nprint(tabulate(comparison_table, headers=[\"Parameter\", \"True\", \"NFXP\", \"Two-Step\"], tablefmt=\"simple\"))\n\ntiming_table = [\n    [\"NFXP\", f\"{result_nfxp['computation_time']:.2f}\", f\"{result_nfxp['log_likelihood']:.2f}\"],\n    [\"Two-Step\", f\"{elapsed:.2f}\", f\"{result_twostep['log_likelihood']:.2f}\"],\n]\nprint(\"\\nPerformance Comparison:\")\nprint(tabulate(timing_table, headers=[\"Method\", \"Time (s)\", \"Log-Likelihood\"], tablefmt=\"simple\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Estimation With Unobserved Heterogeneity\n",
    "\n",
    "Now suppose type $s_i$ is **unobserved**. We only see:\n",
    "- Mileage $x_{it}$\n",
    "- Z parameter $z_i$ (bus-specific)\n",
    "- Replacement decisions $d_{it}$\n",
    "\n",
    "**Challenge**: We need to integrate over the unknown type distribution.\n",
    "\n",
    "## The Likelihood\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\sum_i \\log \\left[ \\sum_{s=0}^1 \\pi_s \\prod_t P(d_{it} | x_{it}, s; \\theta) \\right]$$\n",
    "\n",
    "where $\\pi_s = P(s_i = s)$ is the type probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Why Two-Step CCP Fails\n",
    "\n",
    "**The problem**: In the first stage, we estimate\n",
    "$$\\hat{p}(x, z) = \\frac{1}{N_{xz}} \\sum_{i: x_i = x, z_i = z} \\mathbf{1}\\{d_i = 1\\}$$\n",
    "\n",
    "But this pools across types:\n",
    "$$\\hat{p}(x, z) \\approx \\pi_0 \\cdot p(x, z | s=0) + \\pi_1 \\cdot p(x, z | s=1)$$\n",
    "\n",
    "**We need type-specific CCPs** $p(x, z | s)$, but we don't know $s$!\n",
    "\n",
    "To get type-specific CCPs, we need posterior type probabilities:\n",
    "$$P(s_i = s | \\text{data}_i; \\theta) \\propto \\pi_s \\prod_t P(d_{it} | x_{it}, s; \\theta)$$\n",
    "\n",
    "But to compute this, we need $\\theta$... **Chicken and egg problem!**\n",
    "\n",
    "**Solution**: Iterate via EM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:58.364960Z",
     "iopub.status.busy": "2026-01-30T04:58:58.364893Z",
     "iopub.status.idle": "2026-01-30T04:58:58.367871Z",
     "shell.execute_reply": "2026-01-30T04:58:58.367495Z"
    }
   },
   "source": [
    "# Create data without type labels (simulating unobserved heterogeneity)\n",
    "data_unobs = data.drop(columns=['type_s']).copy()\n",
    "print(\"Data without type labels:\")\n",
    "print(data_unobs.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 NFXP + EM\n",
    "\n",
    "The EM algorithm iterates between:\n",
    "\n",
    "**E-step**: Given current $\\theta^k$, compute posterior type probabilities\n",
    "$$\\tau_{is}^k = P(s_i = s | Y_i, X_i; \\theta^k) = \\frac{\\pi_s \\prod_t P(d_{it} | x_{it}, s; \\theta^k)}{\\sum_{s'} \\pi_{s'} \\prod_t P(d_{it} | x_{it}, s'; \\theta^k)}$$\n",
    "\n",
    "**M-step**: Given $\\tau^k$, maximize expected complete-data likelihood\n",
    "$$\\theta^{k+1} = \\arg\\max_\\theta \\sum_{i,s} \\tau_{is}^k \\sum_t \\log P(d_{it} | x_{it}, s; \\theta)$$\n",
    "\n",
    "In NFXP+EM, the M-step requires solving the DP at each parameter guess (nested within optimization)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:58.368872Z",
     "iopub.status.busy": "2026-01-30T04:58:58.368787Z",
     "iopub.status.idle": "2026-01-30T04:58:58.375420Z",
     "shell.execute_reply": "2026-01-30T04:58:58.375003Z"
    }
   },
   "source": [
    "class NFXPEMEstimator:\n    \"\"\"\n    NFXP with EM for unobserved heterogeneity.\n\n    Inner loop: For each theta guess, solve DP via Bellman value iteration\n    (stationary model), compute likelihood using finite dependence.\n    Outer loop: EM iterations over type probabilities.\n\n    Based on MATLAB nohetero/likebusML4.m structure, adapted to stationary\n    model to match the Python data generating process (BusEngineModel.solve()\n    uses CCP fixed-point iteration, producing stationary choice probabilities).\n\n    MATLAB note: likebusML4.m uses finite-horizon backward induction because\n    genbus4.m generates data from a finite-horizon model. Our Python Simulator\n    generates from the stationary (infinite-horizon) model, so we use Bellman\n    iteration to convergence instead. The likelihood structure is otherwise\n    identical: finite dependence with FV = beta * log(exp(u_keep + E[FV']) +\n    exp(E[FV'|replace])).\n\n    Key design choices:\n      - Bellman value iteration to convergence (stationary FV)\n      - Finite dependence in likelihood (matching likebusML4.m)\n      - beta via logit transform: beta = sigma(alpha_4) for unconstrained opt\n      - Initial conditions correction (intcond / intcondP)\n    \"\"\"\n\n    def __init__(self, data: pd.DataFrame, base_params: BusEngineParams = None):\n        self.data = data.copy()\n        self.base_params = base_params or BusEngineParams()\n        p = self.base_params\n\n        self.bus_ids = sorted(data['bus_id'].unique())\n        self.n_buses = len(self.bus_ids)\n        N = self.n_buses\n        self.T = data['period'].max() + 1\n\n        # Per-bus data\n        self.bus_data = {}\n        for bus_id in self.bus_ids:\n            mask = data['bus_id'] == bus_id\n            self.bus_data[bus_id] = {\n                'x': data.loc[mask, 'x'].values,\n                'x_idx': data.loc[mask, 'x_idx'].values,\n                'z_idx': data.loc[mask, 'z_idx'].values[0],\n                'replace': data.loc[mask, 'replace'].values,\n            }\n\n        # Transition matrices\n        self._P_continue = []\n        self._P_replace = []\n        for z in p.z_grid:\n            P = build_transition_matrix(z, p.x_grid)\n            self._P_continue.append(P)\n            self._P_replace.append(P[0, :])\n\n        # Initial conditions: [1, x_0, z]\n        initial_data = data[data['period'] == 0].set_index('bus_id').loc[self.bus_ids]\n        self.intcondX = np.column_stack([\n            np.ones(N),\n            initial_data['x'].values,\n            initial_data['z'].values,\n        ])\n\n    # =========================================================================\n    # Bellman Value Iteration (stationary model)\n    # =========================================================================\n\n    def _solve_dp(self, alpha_raw: np.ndarray) -> Tuple[np.ndarray, float]:\n        \"\"\"\n        Solve for stationary value function via Bellman iteration.\n\n        Iterates: FV(x,z,s) = beta * log(exp(u_keep + E[FV'|keep]) + exp(E[FV'|replace]))\n        until convergence.\n\n        Matches likebusML4.m backward induction structure but iterated to\n        convergence for consistency with stationary DGP.\n\n        alpha_raw = [alpha_1, alpha_2, alpha_3, alpha_4_logit] where beta = sigma(alpha_4_logit).\n\n        Returns (FV, Beta) where FV has shape (n_z, n_x, 2) — stationary (no time dimension).\n        \"\"\"\n        p = self.base_params\n        n_x, n_z = p.n_x, p.n_z\n\n        Beta = logit_prob(alpha_raw[3])\n\n        FV = np.zeros((n_z, n_x, 2))\n        u_base = alpha_raw[0] + alpha_raw[1] * p.x_grid  # (n_x,)\n\n        for iteration in range(500):\n            FV_new = np.zeros_like(FV)\n\n            for s in range(2):\n                u1_flow = u_base + alpha_raw[2] * s\n                for z_idx in range(n_z):\n                    P_cont = self._P_continue[z_idx]\n                    P_repl = self._P_replace[z_idx]\n                    FV_curr = FV[z_idx, :, s]\n\n                    util_keep = u1_flow + P_cont @ FV_curr\n                    util_replace = P_repl @ FV_curr  # scalar, broadcast\n\n                    max_u = np.maximum(util_keep, util_replace)\n                    FV_new[z_idx, :, s] = Beta * (\n                        max_u + np.log(np.exp(util_keep - max_u) + np.exp(util_replace - max_u))\n                    )\n\n            diff = np.max(np.abs(FV_new - FV))\n            FV = FV_new\n            if diff < 1e-10:\n                break\n\n        return FV, Beta\n\n    # =========================================================================\n    # Type-specific likelihoods\n    # =========================================================================\n\n    def _compute_type_likelihoods(self, alpha_raw: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Bus-level likelihood under each type. Returns base: (N, 2).\n\n        Uses stationary FV (no time subscript). Finite dependence structure\n        matches likebusML4.m: util1 = alpha_1 + alpha_2*x + alpha_3*s +\n        (P_cont - P_repl) @ FV(z, s).\n        \"\"\"\n        N, T = self.n_buses, self.T\n        FV, Beta = self._solve_dp(alpha_raw)\n\n        base = np.zeros((N, 2))\n\n        for i, bus_id in enumerate(self.bus_ids):\n            bd = self.bus_data[bus_id]\n            z_idx = bd['z_idx']\n\n            for s in range(2):\n                log_lik = 0.0\n                # Precompute FV-related terms for this (z, s)\n                FV_zs = FV[z_idx, :, s]  # stationary — same for all t\n                P_repl_row = self._P_replace[z_idx]\n                ev_replace = P_repl_row @ FV_zs\n\n                for t in range(T):\n                    x_idx = bd['x_idx'][t]\n                    P_cont_row = self._P_continue[z_idx][x_idx, :]\n                    ev_cont = P_cont_row @ FV_zs\n                    fd = ev_cont - ev_replace\n\n                    util1 = alpha_raw[0] + alpha_raw[1] * bd['x'][t] + alpha_raw[2] * s + fd\n                    y_keep = 1.0 - bd['replace'][t]\n\n                    log1pexp = util1 if util1 > 20 else np.log1p(np.exp(min(util1, 500)))\n                    log_lik -= (log1pexp - y_keep * util1)\n\n                base[i, s] = np.exp(np.clip(log_lik, -500, 0))\n\n        return base\n\n    def _nfxp_weighted_nll(self, alpha_raw: np.ndarray, PType: np.ndarray) -> float:\n        \"\"\"Weighted NFXP NLL for M-step. Uses stationary FV.\"\"\"\n        N, T = self.n_buses, self.T\n        FV, Beta = self._solve_dp(alpha_raw)\n\n        nll = 0.0\n        for i, bus_id in enumerate(self.bus_ids):\n            bd = self.bus_data[bus_id]\n            z_idx = bd['z_idx']\n\n            for s in range(2):\n                w = PType[i, s]\n                if w < 1e-10:\n                    continue\n\n                FV_zs = FV[z_idx, :, s]\n                P_repl_row = self._P_replace[z_idx]\n                ev_replace = P_repl_row @ FV_zs\n\n                for t in range(T):\n                    x_idx = bd['x_idx'][t]\n                    P_cont_row = self._P_continue[z_idx][x_idx, :]\n                    ev_cont = P_cont_row @ FV_zs\n                    fd = ev_cont - ev_replace\n\n                    util1 = alpha_raw[0] + alpha_raw[1] * bd['x'][t] + alpha_raw[2] * s + fd\n                    y_keep = 1.0 - bd['replace'][t]\n                    log1pexp = util1 if util1 > 20 else np.log1p(np.exp(min(util1, 500)))\n                    nll += w * (log1pexp - y_keep * util1)\n\n        return nll\n\n    # =========================================================================\n    # Initial conditions\n    # =========================================================================\n\n    def _intcond(self, gamma: np.ndarray, base: np.ndarray) -> float:\n        # Note: MATLAB intcond.m divides by 200 (hardcoded); we divide by N.\n        # Same optimum, different scale — does not affect parameter estimates.\n        p0 = logit_prob(self.intcondX @ gamma)\n        p = np.column_stack([p0, 1 - p0])\n        marginal = np.sum(p * base, axis=1)\n        return -np.sum(np.log(np.maximum(marginal, 1e-300))) / self.n_buses\n\n    def _intcondP(self, gamma: np.ndarray, base: np.ndarray) -> np.ndarray:\n        p0 = logit_prob(self.intcondX @ gamma)\n        p = np.column_stack([p0, 1 - p0])\n        joint = base * p\n        marginal = np.sum(joint, axis=1, keepdims=True)\n        return joint / np.maximum(marginal, 1e-300)\n\n    # =========================================================================\n    # EM Algorithm\n    # =========================================================================\n\n    def estimate(self, theta0: np.ndarray = None, max_iter: int = 50,\n                 tol: float = 1e-7, fix_discount: bool = True) -> dict:\n        \"\"\"\n        Run NFXP + EM.\n\n        Parameters use logit transform for beta:\n          alpha_raw = [alpha_1, alpha_2, alpha_3, logit(beta)]\n        Results report actual beta.\n\n        fix_discount : bool\n            If True (default), fix beta at base_params.discount and optimize only (alpha_1, alpha_2, alpha_3).\n            If False, estimate beta jointly.\n        \"\"\"\n        p = self.base_params\n\n        if theta0 is None:\n            theta0 = np.array([p.theta0_intercept, p.theta0_mileage, p.theta0_type, p.discount])\n\n        beta0 = np.clip(theta0[3], 0.01, 0.99)\n        alpha_raw = np.array([theta0[0], theta0[1], theta0[2],\n                              np.log(beta0) - np.log(1 - beta0)])\n\n        gamma = np.zeros(3)\n        gamma_start = np.zeros(3)\n        lp_history = []\n        history = []\n        start_time = time.time()\n\n        for j in range(max_iter):\n            # -- E-step --\n            base = self._compute_type_likelihoods(alpha_raw)\n\n            # Initial conditions\n            if j > 1:\n                ic_start = gamma_start if j < 50 else gamma\n                res_ic = minimize(self._intcond, ic_start, args=(base,),\n                                  method='BFGS', options={'disp': False})\n                gamma = res_ic.x\n\n            ll = self._intcond(gamma, base)\n            lp_history.append(ll)\n\n            PType = self._intcondP(gamma, base)\n            tau = PType[:, 1]\n            pi_new = 1 - tau.mean()\n\n            # -- M-step --\n            if fix_discount:\n                beta_logit_fixed = alpha_raw[3]\n                def m_obj_3(a3):\n                    a_full = np.array([a3[0], a3[1], a3[2], beta_logit_fixed])\n                    return self._nfxp_weighted_nll(a_full, PType)\n\n                res_m = minimize(m_obj_3, alpha_raw[:3], method='L-BFGS-B',\n                                 options={'maxiter': 200, 'disp': False})\n                alpha_raw_new = np.array([res_m.x[0], res_m.x[1], res_m.x[2], beta_logit_fixed])\n            else:\n                def m_obj(a):\n                    return self._nfxp_weighted_nll(a, PType)\n\n                res_m = minimize(m_obj, alpha_raw, method='L-BFGS-B',\n                                 options={'maxiter': 200, 'disp': False})\n                alpha_raw_new = res_m.x\n\n            # Convert back for reporting\n            beta_est = logit_prob(alpha_raw_new[3])\n            theta_report = np.array([alpha_raw_new[0], alpha_raw_new[1],\n                                     alpha_raw_new[2], beta_est])\n\n            history.append({\n                'theta': theta_report.copy(), 'pi': pi_new,\n                'll': ll, 'gamma': gamma.copy()\n            })\n\n            print(f\"Iter {j+1}: theta={theta_report.round(3)}, \"\n                  f\"pi={pi_new:.3f}, gamma={gamma.round(3)}, LL={ll:.4f}\")\n\n            # Convergence\n            diff = np.max(np.abs(alpha_raw_new - alpha_raw))\n            if diff < tol and j > 5:\n                print(f\"Converged in {j+1} iterations\")\n                break\n\n            if len(lp_history) > 26:\n                c1 = abs((lp_history[-1] - lp_history[-26]) / max(abs(lp_history[-1]), 1e-300))\n                c2 = abs((lp_history[-2] - lp_history[-27]) / max(abs(lp_history[-2]), 1e-300))\n                if c1 < tol and c2 < tol:\n                    print(f\"Converged (LL criterion) in {j+1} iterations\")\n                    break\n\n            alpha_raw = alpha_raw_new\n\n        return {\n            'theta': theta_report,\n            'pi': pi_new,\n            'gamma': gamma,\n            'tau': tau,\n            'log_likelihood': ll,\n            'n_iterations': j + 1,\n            'computation_time': time.time() - start_time,\n            'history': history\n        }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:58:58.376333Z",
     "iopub.status.busy": "2026-01-30T04:58:58.376269Z",
     "iopub.status.idle": "2026-01-30T04:59:48.632637Z",
     "shell.execute_reply": "2026-01-30T04:59:48.632211Z"
    }
   },
   "source": [
    "# Run NFXP+EM (full sample, matching replication N=2000)\n",
    "print(\"=\"*70)\n",
    "print(\"NFXP + EM Estimation (types unobserved)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data_unobs = data.drop(columns=['type_s'])\n",
    "\n",
    "config_table = [\n",
    "    [\"Buses used\", data_unobs['bus_id'].nunique()],\n",
    "    [\"Total observations\", len(data_unobs)],\n",
    "    [\"Periods\", data_unobs['period'].nunique()],\n",
    "]\n",
    "print(\"\\nConfiguration:\")\n",
    "print(tabulate(config_table, headers=[\"Parameter\", \"Value\"], tablefmt=\"simple\"))\n",
    "\n",
    "print(\"\\nNote: NFXP+EM is slow — each iteration solves the full DP via backward induction.\")\n",
    "print(\"Expected: ~60x slower than CCP-EM.\\n\")\n",
    "\n",
    "nfxp_em = NFXPEMEstimator(data_unobs, model.params)\n",
    "result_nfxp_em = nfxp_em.estimate(max_iter=30)\n",
    "\n",
    "# Display results\n",
    "true_params = model.params.theta_vector()\n",
    "est_params = result_nfxp_em['theta']\n",
    "param_names = ['alpha_1 (intercept)', 'alpha_2 (mileage)', 'alpha_3 (type)', 'beta (discount)']\n",
    "\n",
    "results_table = []\n",
    "for i, name in enumerate(param_names):\n",
    "    results_table.append([name, f\"{true_params[i]:.4f}\", f\"{est_params[i]:.4f}\"])\n",
    "results_table.append(['pi (type prob)', f\"{model.params.type_prob:.4f}\", f\"{result_nfxp_em['pi']:.4f}\"])\n",
    "\n",
    "print(\"\\nParameter Estimates:\")\n",
    "print(tabulate(results_table, headers=[\"Parameter\", \"True\", \"Estimated\"], tablefmt=\"simple\"))\n",
    "\n",
    "# Estimation statistics\n",
    "stats_table = [\n",
    "    [\"Iterations\", result_nfxp_em['n_iterations']],\n",
    "    [\"Computation time (s)\", f\"{result_nfxp_em['computation_time']:.2f}\"],\n",
    "]\n",
    "print(\"\\nEstimation Statistics:\")\n",
    "print(tabulate(stats_table, headers=[\"Metric\", \"Value\"], tablefmt=\"simple\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 CCP-EM with Data Updating\n",
    "\n",
    "**Key innovation of Arcidiacono & Miller (2011)**: Replace the expensive M-step (solving DP) with CCP-based estimation.\n",
    "\n",
    "### CCP-Data Approach\n",
    "\n",
    "In the M-step, instead of solving the DP:\n",
    "\n",
    "1. **Update CCPs directly from data** using posterior weights:\n",
    "   $$\\hat{p}^{k+1}(x, s) = \\frac{\\sum_{i,t} \\tau_{is}^k \\mathbf{1}\\{x_{it}=x, d_{it}=1\\}}{\\sum_{i,t} \\tau_{is}^k \\mathbf{1}\\{x_{it}=x\\}}$$\n",
    "\n",
    "2. **Estimate structural parameters** via weighted regression with Hotz-Miller inversion.\n",
    "\n",
    "This avoids solving the DP entirely!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:59:48.634029Z",
     "iopub.status.busy": "2026-01-30T04:59:48.633929Z",
     "iopub.status.idle": "2026-01-30T04:59:48.641909Z",
     "shell.execute_reply": "2026-01-30T04:59:48.641464Z"
    }
   },
   "source": [
    "class CCPEMDataEstimator:\n    \"\"\"\n    CCP-EM with Data updating + Initial Conditions.\n\n    Exact Python translation of MATLAB hetero10ns/shellbusdatacomp420cond.m.\n\n    EM loop structure:\n      E-step: likeCCP → intcond → intcondP → PType\n      M-step 1: wlogitd (re-estimate reduced-form with PType weights)\n      M-step 2: fvdata (recompute FV from updated reduced-form)\n      M-step 3: wlogit (re-estimate structural with PType weights)\n\n    Key design choices matching MATLAB:\n      - Single pooled reduced-form logit with type interactions (not separate type logits)\n      - FV from data-based CCPs evaluated on grid (β NOT in FV)\n      - likeCCP (structural model) for type classification (not reduced-form)\n      - Time-varying FV via period-specific dummies in reduced-form\n      - Last period excluded from structural estimation\n      - Initial conditions: P(type=0|x₀,z) = Λ(γ'w)\n    \"\"\"\n\n    def __init__(self, data: pd.DataFrame, base_params: BusEngineParams = None):\n        self.data = data.copy()\n        self.base_params = base_params or BusEngineParams()\n        p = self.base_params\n\n        # ── Bus-level indexing ──\n        self.bus_ids = sorted(data['bus_id'].unique())\n        self.n_buses = len(self.bus_ids)\n        N = self.n_buses\n        self.T = data['period'].max() + 1\n        T = self.T\n\n        # Per-bus data\n        self.bus_data = {}\n        for bus_id in self.bus_ids:\n            mask = data['bus_id'] == bus_id\n            self.bus_data[bus_id] = {\n                'x': data.loc[mask, 'x'].values,\n                'x_idx': data.loc[mask, 'x_idx'].values,\n                'z': data.loc[mask, 'z'].values[0],\n                'z_idx': data.loc[mask, 'z_idx'].values[0],\n                'replace': data.loc[mask, 'replace'].values,\n                'periods': data.loc[mask, 'period'].values,\n            }\n\n        # ── Transition matrices ──\n        self._P_continue = []  # P(x'|x,z, keep)\n        self._P_replace = []   # P(x'|0,z, replace) = first row\n        for z in p.z_grid:\n            P = build_transition_matrix(z, p.x_grid)\n            self._P_continue.append(P)\n            self._P_replace.append(P[0, :])\n\n        # ── Build stacked data arrays ──\n        # MATLAB ordering: y2 = reshape(Y, N*T, 1) then [y2; y2]\n        # Column-major reshape = time-major: (bus1_t1, bus2_t1, ..., busN_t1, bus1_t2, ...)\n        # For simplicity we use bus-major: (bus1_t0..tT, bus2_t0..tT, ...) — same result\n        # since all weighted sums are permutation-invariant.\n\n        y_list, x_list, z_list, xidx_list, zidx_list, t_list = [], [], [], [], [], []\n        for bus_id in self.bus_ids:\n            bd = self.bus_data[bus_id]\n            for t in range(T):\n                y_list.append(bd['replace'][t])\n                x_list.append(bd['x'][t])\n                xidx_list.append(bd['x_idx'][t])\n                z_list.append(bd['z'])\n                zidx_list.append(bd['z_idx'])\n                t_list.append(t)\n\n        y_s = np.array(y_list)            # N*T\n        x_s = np.array(x_list)\n        z_s = np.array(z_list)\n        xidx_s = np.array(xidx_list)\n        zidx_s = np.array(zidx_list)\n        t_s = np.array(t_list)\n\n        # Stack: [type0; type1]\n        self.y2 = np.concatenate([y_s, y_s])           # 2*N*T\n        self.y_replace = (self.y2 == 1).astype(float)   # replace indicator (MATLAB: y2==0 where y2=1=keep)\n        # MATLAB convention: Y=1=keep, y2==0=replace. Our data: replace=1=replace.\n        # So our replace indicator = self.y2 directly. MATLAB's (y2==0) = our replace.\n        # For wlogitd modeling P(replace|X)=σ(Xb): Y_wlogit = replace indicator.\n\n        x2 = np.concatenate([x_s, x_s]) / 10.0   # MATLAB: x2 = X/10\n        z2 = np.concatenate([z_s, z_s])\n        self.s2 = np.concatenate([np.zeros(N*T), np.ones(N*T)])\n        self.t2 = np.concatenate([t_s, t_s])\n        self.xidx2 = np.concatenate([xidx_s, xidx_s])\n        self.zidx2 = np.concatenate([zidx_s, zidx_s])\n\n        # ── Time dummies (T-1 columns) ──\n        # MATLAB: td(:,t) = (t2*10-1 == t) for t=1..T-1\n        # In 0-indexed: td[:,k] = (period == k+1) for k=0..T-2\n        # Period 0 is reference.\n        self.td = np.zeros((2*N*T, T-1))\n        for k in range(T-1):\n            self.td[:, k] = (self.t2 == k+1).astype(float)\n\n        # ── Reduced-form design matrix ──\n        # MATLAB: xx = [1, x2, z2, x2.*z2, x2.^2, z2.^2, s2, s2.*x2, s2.*z2,\n        #               s2.*x2.*z2, s2.*x2.^2, s2.*z2.^2, td]\n        self.xx = np.column_stack([\n            np.ones(2*N*T),\n            x2, z2, x2*z2, x2**2, z2**2,\n            self.s2, self.s2*x2, self.s2*z2,\n            self.s2*x2*z2, self.s2*x2**2, self.s2*z2**2,\n            self.td\n        ])\n\n        # ── Grid design matrix for FV (RX1) ──\n        # MATLAB: xvalr = kron(ones(zbin,1), xval)/10, zvalr = kron(zval, ones(xbin,1))\n        # RX1 = [1, xvalr, zvalr, xvalr.*zvalr, xvalr.^2, zvalr.^2]\n        n_x, n_z = p.n_x, p.n_z\n        xvalr = np.tile(p.x_grid / 10.0, n_z)      # repeat x_grid for each z\n        zvalr = np.repeat(p.z_grid, n_x)             # repeat each z for all x\n        self.RX1 = np.column_stack([\n            np.ones(n_x * n_z),\n            xvalr, zvalr, xvalr*zvalr, xvalr**2, zvalr**2\n        ])\n        self.tbin = n_x * n_z\n\n        # ── Structural regressors ──\n        # MATLAB: xccp = [1, x2*10, s2] = [1, X_original, s]\n        self.xccp = np.column_stack([np.ones(2*N*T), x2*10.0, self.s2])\n\n        # ── Index: exclude last period ──\n        # MATLAB: index = t2 < 1 (since t2 = period/10, this means period < T)\n        self.index = self.t2 < (T - 1)\n\n        # ── Initial conditions ──\n        # MATLAB: intcondX = [1, X(1:N,1), Z(1:N,1)]\n        initial_data = data[data['period'] == 0].set_index('bus_id').loc[self.bus_ids]\n        self.intcondX = np.column_stack([\n            np.ones(N),\n            initial_data['x'].values,\n            initial_data['z'].values,\n        ])\n\n    # =========================================================================\n    # MATLAB function translations\n    # =========================================================================\n\n    @staticmethod\n    def _plogit(b: np.ndarray, X: np.ndarray) -> np.ndarray:\n        \"\"\"plogit.m: P(replace|X) = exp(Xb)/(1+exp(Xb)) = σ(Xb)\"\"\"\n        return logit_prob(X @ b)\n\n    @staticmethod\n    def _wlogitd(b: np.ndarray, Y: np.ndarray, X: np.ndarray, P: np.ndarray):\n        \"\"\"\n        wlogitd.m: Weighted logit NLL with gradient.\n        Models P(Y=1|X) = σ(Xb).\n        NLL = P' @ (log(1+exp(Xb)) - Y*Xb)\n        grad = (P*(σ(Xb)-Y))' @ X\n        \"\"\"\n        U = X @ b\n        # Numerically stable log(1+exp(U))\n        log1pexp = np.where(U > 20, U, np.log1p(np.exp(np.clip(U, -500, 20))))\n        nll = P @ (log1pexp - Y * U)\n        sigma = logit_prob(U)\n        grad = (P * (sigma - Y)) @ X\n        return nll, grad\n\n    @staticmethod\n    def _wlogit(b: np.ndarray, Y: np.ndarray, X: np.ndarray, P: np.ndarray) -> float:\n        \"\"\"wlogit.m: Weighted logit NLL (no gradient). Like = P'*(log(1+exp(Xb))-Y.*Xb)\"\"\"\n        U = X @ b\n        log1pexp = np.where(U > 20, U, np.log1p(np.exp(np.clip(U, -500, 20))))\n        return P @ (log1pexp - Y * U)\n\n    @staticmethod\n    def _likeCCP(b: np.ndarray, Y: np.ndarray, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        likeCCP.m: Per-observation structural likelihood.\n        Like = (Y*exp(Xb) + (1-Y)) / (1+exp(Xb))\n        = Y*σ(Xb) + (1-Y)*(1-σ(Xb))\n        where Y=1 means keep (the action modeled by σ(Xb)).\n        \"\"\"\n        U = X @ b\n        expU = np.exp(np.clip(U, -500, 500))\n        return (Y * expU + (1 - Y)) / (1 + expU)\n\n    def _intcond(self, gamma: np.ndarray, base: np.ndarray) -> float:\n        \"\"\"intcond.m: NLL for initial conditions. Like=-sum(log(sum(p.*like,2)))/N\"\"\"\n        # Note: MATLAB intcond.m divides by 200 (hardcoded); we divide by N.\n        # Same optimum, different scale — does not affect parameter estimates.\n        p0 = logit_prob(self.intcondX @ gamma)\n        p = np.column_stack([p0, 1 - p0])\n        marginal = np.sum(p * base, axis=1)\n        return -np.sum(np.log(np.maximum(marginal, 1e-300))) / self.n_buses\n\n    def _intcondP(self, gamma: np.ndarray, base: np.ndarray) -> np.ndarray:\n        \"\"\"intcondP.m: Posterior type probs. PType=(like.*p)./(sum(like.*p,2))\"\"\"\n        p0 = logit_prob(self.intcondX @ gamma)\n        p = np.column_stack([p0, 1 - p0])\n        joint = base * p\n        marginal = np.sum(joint, axis=1, keepdims=True)\n        return joint / np.maximum(marginal, 1e-300)\n\n    def _fvdata(self, b1: np.ndarray) -> np.ndarray:\n        \"\"\"\n        fvdata.m: Compute FV terms from reduced-form logit b1.\n\n        1. For each period t (0-indexed: 1..T-1) and type s:\n           Build grid regressors [RX1, s*RX1, td_grid]\n           p0 = plogit(b1, X_grid) = P(replace|grid,s,t)\n           FV1[grid,s,t] = -log(p0)\n\n        2. For each (bus, period, type):\n           FVT1[n,t,s] = (P_continue[x_n_t] - P_replace[x=0]) @ FV1[z_grid,s,t+1]\n\n        Returns stacked fvt1 of length 2*N*T.\n        \"\"\"\n        p = self.base_params\n        n_x, n_z = p.n_x, p.n_z\n        N, T = self.n_buses, self.T\n        tbin = self.tbin\n\n        # FV1: (tbin, 2_types, T+1_periods) — 0-indexed periods, period T is terminal (=0)\n        FV1 = np.zeros((tbin, 2, T + 1))\n        td_grid = np.zeros((tbin, T - 1))\n\n        # MATLAB loop: for t=2:T (1-indexed) → 0-indexed: t=1..T-1\n        for t_0idx in range(1, T):\n            # Set one-hot time dummy for this period\n            td_grid[:, :] = 0\n            td_grid[:, t_0idx - 1] = 1.0\n\n            for s in range(2):\n                X_grid = np.column_stack([self.RX1, s * self.RX1, td_grid])\n                p0 = self._plogit(b1, X_grid)\n                p0 = np.clip(p0, 1e-10, 1 - 1e-10)\n                FV1[:, s, t_0idx] = -np.log(p0)\n\n        # Compute FVT1 for each (bus, period, type)\n        FVT1 = np.zeros((N, T, 2))\n\n        for i, bus_id in enumerate(self.bus_ids):\n            bd = self.bus_data[bus_id]\n            z_idx = bd['z_idx']\n            z_start = z_idx * n_x       # grid slice start for this z\n            z_end = z_start + n_x        # grid slice end\n\n            # P_replace row = transition from x=0 for this z\n            P_repl_row = self._P_continue[z_idx][0, :]  # = P(x'|x=0, z)\n\n            for t in range(T):\n                x_idx = bd['x_idx'][t]\n                P_cont_row = self._P_continue[z_idx][x_idx, :]  # P(x'|x,z,keep)\n\n                for s in range(2):\n                    # Finite dependence: (P_cont - P_repl) @ FV1[z_slice, s, t+1]\n                    FVT1[i, t, s] = (P_cont_row - P_repl_row) @ FV1[z_start:z_end, s, t + 1]\n\n        # Stack: [all type0 (N*T); all type1 (N*T)]\n        fvt1 = np.concatenate([FVT1[:, :, 0].flatten(), FVT1[:, :, 1].flatten()])\n        return fvt1\n\n    # =========================================================================\n    # EM Algorithm\n    # =========================================================================\n\n    def estimate(self, theta0: np.ndarray = None, max_iter: int = 200,\n                 tol: float = 1e-7) -> dict:\n        \"\"\"\n        Run CCP-EM with data updating + initial conditions.\n\n        Exact match of MATLAB shellbusdatacomp420cond.m lines 83-168.\n        \"\"\"\n        p = self.base_params\n        N, T = self.n_buses, self.T\n        idx = self.index  # exclude last period\n\n        # Starting values\n        # MATLAB: alphac = [2.233; -.1339; .4; .9115]\n        if theta0 is None:\n            theta0 = np.array([p.theta0_intercept, p.theta0_mileage, p.theta0_type, p.discount])\n\n        # MATLAB: bccp = [alphac(1:4); zeros(T2-2,1)]\n        n_td_struct = max(T - 2, 0)\n        bccp = np.concatenate([theta0[:4], np.zeros(n_td_struct)])\n\n        # Initial PType = 0.5 for all observations\n        PType_obs = 0.5 * np.ones(2 * N * T)\n\n        # ── Pre-loop: initial reduced-form estimate + FV ──\n        b1 = np.zeros(self.xx.shape[1])\n\n        # MATLAB: [b1]=fminunc('wlogitd',b1,o2,y2==0,xx,PType)\n        def wlogitd_obj(b):\n            nll, grad = self._wlogitd(b, self.y_replace, self.xx, PType_obs)\n            return nll, grad\n\n        res = minimize(wlogitd_obj, b1, method='BFGS', jac=True,\n                       options={'disp': False, 'maxiter': 200})\n        b1 = res.x\n\n        # MATLAB: [fvt1]=fvdata(b1,RX1,...)\n        fvt1 = self._fvdata(b1)\n\n        # ── EM loop ──\n        gamma = np.zeros(3)\n        gamma_start = np.zeros(3)\n        lp_history = []\n        history = []\n        start_time = time.time()\n\n        for j in range(max_iter):\n            # ── E-step: type classification via structural likelihood ──\n\n            # MATLAB: Like=likeCCP(bccp,y2(index==1),[xccp(index==1,:) fvt1(index==1) td(index==1,1:T2-2)])\n            X_struct = np.column_stack([\n                self.xccp[idx],\n                fvt1[idx],\n                self.td[idx, :n_td_struct]   # T-2 structural time dummies\n            ])\n            y_keep = (1 - self.y_replace[idx]).astype(float)  # Y=1=keep for likeCCP\n            Like = self._likeCCP(bccp, y_keep, X_struct)\n\n            # MATLAB: Like2=reshape(Like,N,T2-1,2); base=squeeze(prod(Like2,2))\n            half = int(idx.sum()) // 2\n            like0 = Like[:half].reshape(N, T - 1)\n            like1 = Like[half:].reshape(N, T - 1)\n            base0 = np.prod(np.clip(like0, 1e-300, None), axis=1)\n            base1 = np.prod(np.clip(like1, 1e-300, None), axis=1)\n            base = np.column_stack([base0, base1])\n\n            # ── Initial conditions ──\n            # MATLAB: if j>1, estimate gamma\n            if j > 1:\n                ic_start = gamma_start if j < 50 else gamma\n                res_ic = minimize(self._intcond, ic_start, args=(base,),\n                                  method='BFGS', options={'disp': False})\n                gamma = res_ic.x\n\n            ll = self._intcond(gamma, base)\n            lp_history.append(ll)\n\n            # ── Posterior type probabilities ──\n            PType = self._intcondP(gamma, base)\n            tau = PType[:, 1]\n            pi_new = 1 - tau.mean()\n\n            # Expand PType to per-observation weights\n            # MATLAB: PType=kron(ones(T2,1),PType); PType=reshape(PType,N*T2*2,1)\n            PType_obs = np.zeros(2 * N * T)\n            for i in range(N):\n                for t in range(T):\n                    PType_obs[i * T + t] = PType[i, 0]           # type 0 block\n                    PType_obs[N * T + i * T + t] = PType[i, 1]   # type 1 block\n\n            # ── M-step 1: re-estimate reduced-form logit ──\n            # MATLAB: [b1]=fminunc('wlogitd',b1,o2,y2==0,xx,PType)\n            def wlogitd_obj_em(b):\n                nll, grad = self._wlogitd(b, self.y_replace, self.xx, PType_obs)\n                return nll, grad\n\n            res_rf = minimize(wlogitd_obj_em, b1, method='BFGS', jac=True,\n                              options={'disp': False, 'maxiter': 200})\n            b1 = res_rf.x\n\n            # ── M-step 2: recompute FV ──\n            fvt1 = self._fvdata(b1)\n\n            # ── M-step 3: re-estimate structural parameters ──\n            # MATLAB: [bccp]=fminunc('wlogit',bccp,o1,y2(idx),[xccp(idx) fvt1(idx) td(idx,1:T-2)],PType(idx))\n            X_struct_full = np.column_stack([\n                self.xccp[idx],\n                fvt1[idx],\n                self.td[idx, :n_td_struct]\n            ])\n            PType_idx = PType_obs[idx]\n\n            def wlogit_obj(b):\n                return self._wlogit(b, y_keep, X_struct_full, PType_idx)\n\n            res_struct = minimize(wlogit_obj, bccp, method='BFGS',\n                                  options={'disp': False, 'maxiter': 200})\n            bccp = res_struct.x\n            theta_new = bccp[:4]\n\n            history.append({\n                'theta': theta_new.copy(), 'pi': pi_new,\n                'll': ll, 'gamma': gamma.copy()\n            })\n\n            if j % 5 == 0:\n                print(f\"Iter {j+1}: θ={theta_new.round(3)}, \"\n                      f\"π={pi_new:.3f}, γ={gamma.round(3)}, LL={ll:.4f}\")\n\n            # ── Convergence (MATLAB: 26-lag relative change) ──\n            if len(lp_history) > 26:\n                c1 = abs((lp_history[-1] - lp_history[-26]) / max(abs(lp_history[-1]), 1e-300))\n                c2 = abs((lp_history[-2] - lp_history[-27]) / max(abs(lp_history[-2]), 1e-300))\n                if c1 < tol and c2 < tol:\n                    print(f\"Converged in {j+1} iterations\")\n                    break\n\n        return {\n            'theta': theta_new,\n            'pi': pi_new,\n            'gamma': gamma,\n            'tau': tau,\n            'bccp': bccp,\n            'b1': b1,\n            'log_likelihood': ll,\n            'n_iterations': j + 1,\n            'computation_time': time.time() - start_time,\n            'history': history\n        }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:59:48.642927Z",
     "iopub.status.busy": "2026-01-30T04:59:48.642854Z",
     "iopub.status.idle": "2026-01-30T04:59:54.334360Z",
     "shell.execute_reply": "2026-01-30T04:59:54.333872Z"
    }
   },
   "source": [
    "# Run CCP-EM with Data updating\n",
    "print(\"=\"*70)\n",
    "print(\"CCP-EM (Data Updating) — Arcidiacono & Miller (2011)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data_unobs = data.drop(columns=['type_s'])\n",
    "\n",
    "ccpem_data = CCPEMDataEstimator(data_unobs, model.params)\n",
    "result_ccpem_data = ccpem_data.estimate()\n",
    "\n",
    "# Display results\n",
    "true_params = model.params.theta_vector()\n",
    "est_params = result_ccpem_data['theta']\n",
    "param_names = ['alpha_1 (intercept)', 'alpha_2 (mileage)', 'alpha_3 (type)', 'beta (discount)']\n",
    "\n",
    "results_table = []\n",
    "for i, name in enumerate(param_names):\n",
    "    results_table.append([name, f\"{true_params[i]:.4f}\", f\"{est_params[i]:.4f}\"])\n",
    "results_table.append(['pi (type prob)', f\"{model.params.type_prob:.4f}\", f\"{result_ccpem_data['pi']:.4f}\"])\n",
    "\n",
    "print(\"\\nStructural Parameter Estimates:\")\n",
    "print(tabulate(results_table, headers=[\"Parameter\", \"True\", \"Estimated\"], tablefmt=\"simple\"))\n",
    "\n",
    "# Estimation statistics\n",
    "stats_table = [\n",
    "    [\"Iterations\", result_ccpem_data['n_iterations']],\n",
    "    [\"Computation time (s)\", f\"{result_ccpem_data['computation_time']:.2f}\"],\n",
    "]\n",
    "print(\"\\nEstimation Statistics:\")\n",
    "print(tabulate(stats_table, headers=[\"Metric\", \"Value\"], tablefmt=\"simple\"))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 CCP-EM with Model Updating\n",
    "\n",
    "Alternative to data updating: use the **model's CCP operator** to update CCPs.\n",
    "\n",
    "### CCP-Model Approach (NPL-style)\n",
    "\n",
    "In the M-step:\n",
    "\n",
    "1. Given current CCPs $p^k$ and posterior weights $\\tau^k$, estimate $\\theta^{k+1}$\n",
    "\n",
    "2. **Update CCPs using model operator**:\n",
    "   $$p^{k+1} = \\Psi(p^k; \\theta^{k+1})$$\n",
    "   \n",
    "   where $\\Psi$ maps CCPs to best-response CCPs given $\\theta$.\n",
    "\n",
    "This is similar to NPL (Aguirregabiria & Mira 2007) but with EM for unobserved heterogeneity."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:59:54.335619Z",
     "iopub.status.busy": "2026-01-30T04:59:54.335547Z",
     "iopub.status.idle": "2026-01-30T04:59:54.344715Z",
     "shell.execute_reply": "2026-01-30T04:59:54.344322Z"
    }
   },
   "source": [
    "class CCPEMModelEstimator:\n",
    "    \"\"\"\n",
    "    CCP-EM with Model updating + Initial Conditions correction.\n",
    "\n",
    "    Hybrid approach matching MATLAB hetero10ns/shellbusdatacomp420cond.m:\n",
    "    - FV from model CCPs solved to convergence (avoids first-stage logit bias)\n",
    "    - Structural model likelihood (likeCCP) for type classification\n",
    "    - Initial conditions: P(type=0|x₀,z) = Λ(γ'w)\n",
    "    - β estimated as free coefficient + time dummies\n",
    "    - Last period excluded\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, base_params: BusEngineParams = None):\n",
    "        self.data = data.copy()\n",
    "        self.base_params = base_params or BusEngineParams()\n",
    "\n",
    "        self.bus_ids = sorted(data['bus_id'].unique())\n",
    "        self.n_buses = len(self.bus_ids)\n",
    "        self.T = data['period'].max() + 1\n",
    "        self.bus_data = {}\n",
    "        for bus_id in self.bus_ids:\n",
    "            mask = data['bus_id'] == bus_id\n",
    "            self.bus_data[bus_id] = {\n",
    "                'x_idx': data.loc[mask, 'x_idx'].values,\n",
    "                'z_idx': data.loc[mask, 'z_idx'].values[0],\n",
    "                'replace': data.loc[mask, 'replace'].values,\n",
    "                'periods': data.loc[mask, 'period'].values,\n",
    "            }\n",
    "\n",
    "        self._build_transitions()\n",
    "\n",
    "        # Initial conditions data: [1, x_0, z]\n",
    "        initial_data = data[data['period'] == 0].set_index('bus_id').loc[self.bus_ids]\n",
    "        self.intcondX = np.column_stack([\n",
    "            np.ones(self.n_buses),\n",
    "            initial_data['x'].values,\n",
    "            initial_data['z'].values,\n",
    "        ])\n",
    "\n",
    "        # Build stacked data arrays for structural estimation\n",
    "        self._build_stacked_data()\n",
    "\n",
    "    def _build_transitions(self):\n",
    "        p = self.base_params\n",
    "        self._P_continue = []\n",
    "        self._P_replace = []\n",
    "        for z in p.z_grid:\n",
    "            P = build_transition_matrix(z, p.x_grid)\n",
    "            self._P_continue.append(P)\n",
    "            self._P_replace.append(P[0, :])\n",
    "\n",
    "    def _build_stacked_data(self):\n",
    "        \"\"\"Build stacked arrays: data doubled, first half type=0, second half type=1.\"\"\"\n",
    "        p = self.base_params\n",
    "        T, N = self.T, self.n_buses\n",
    "\n",
    "        y_list, x_list, t_list, xidx_list, zidx_list = [], [], [], [], []\n",
    "        for bus_id in self.bus_ids:\n",
    "            bd = self.bus_data[bus_id]\n",
    "            for t in range(T):\n",
    "                y_list.append(bd['replace'][t])\n",
    "                x_list.append(p.x_grid[bd['x_idx'][t]])\n",
    "                t_list.append(t)\n",
    "                xidx_list.append(bd['x_idx'][t])\n",
    "                zidx_list.append(bd['z_idx'])\n",
    "\n",
    "        y_s = np.array(y_list)\n",
    "        self.y2 = np.concatenate([y_s, y_s])  # 2*N*T\n",
    "        self.x_raw = np.concatenate([np.array(x_list)]*2)\n",
    "        self.t2 = np.concatenate([np.array(t_list)]*2)\n",
    "        self.xidx2 = np.concatenate([np.array(xidx_list)]*2)\n",
    "        self.zidx2 = np.concatenate([np.array(zidx_list)]*2)\n",
    "        self.s2 = np.concatenate([np.zeros(N*T), np.ones(N*T)])\n",
    "\n",
    "        # Index: exclude last period\n",
    "        self.index = self.t2 < (T - 1)\n",
    "\n",
    "        # Structural regressors: [1, x_raw, s]\n",
    "        self.xccp = np.column_stack([np.ones(2*N*T), self.x_raw, self.s2])\n",
    "\n",
    "        # Time dummies for structural (T-2 dummies, for periods 1..T-2)\n",
    "        self.td_struct = np.zeros((2*N*T, max(T-2, 0)))\n",
    "        for t in range(1, T-1):\n",
    "            self.td_struct[:, t-1] = (self.t2 == t).astype(float)\n",
    "\n",
    "    # =========================================================================\n",
    "    # CCP solving\n",
    "    # =========================================================================\n",
    "\n",
    "    def ccp_operator(self, ccps: np.ndarray, theta: np.ndarray, type_s: int) -> np.ndarray:\n",
    "        \"\"\"Single Bellman operator step: p' = Ψ(p; θ, s)\"\"\"\n",
    "        params = BusEngineParams.from_vector(theta)\n",
    "        p = self.base_params\n",
    "        n_x, n_z = p.n_x, p.n_z\n",
    "        ccps = ccps.reshape(n_z, n_x)\n",
    "        ccps_new = np.zeros_like(ccps)\n",
    "        u1 = params.intercept + params.mileage_coef * p.x_grid + params.type_coef * type_s\n",
    "        for z_idx in range(n_z):\n",
    "            p_replace = np.clip(1 - ccps[z_idx, :], 1e-6, 1 - 1e-6)\n",
    "            V = EULER_CONSTANT - np.log(p_replace)\n",
    "            v_diff = u1 + params.discount * (self._P_continue[z_idx] @ V - self._P_replace[z_idx] @ V)\n",
    "            ccps_new[z_idx, :] = logit_prob(v_diff)\n",
    "        return ccps_new.flatten()\n",
    "\n",
    "    def solve_ccps(self, theta: np.ndarray, type_s: int,\n",
    "                   initial: np.ndarray = None, tol: float = 1e-8, max_iter: int = 500) -> np.ndarray:\n",
    "        \"\"\"Solve for CCPs via fixed-point iteration.\"\"\"\n",
    "        p = self.base_params\n",
    "        ccps = initial if initial is not None else 0.5 * np.ones(p.n_x * p.n_z)\n",
    "        for it in range(max_iter):\n",
    "            ccps_new = self.ccp_operator(ccps, theta, type_s)\n",
    "            if np.max(np.abs(ccps_new - ccps)) < tol:\n",
    "                return ccps_new\n",
    "            ccps = ccps_new\n",
    "        return ccps\n",
    "\n",
    "    # =========================================================================\n",
    "    # FV computation (model-based, without β)\n",
    "    # =========================================================================\n",
    "\n",
    "    def compute_fv(self, ccps0: np.ndarray, ccps1: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute FV for stacked data using model CCPs.\n",
    "\n",
    "        FV(x,z,s) = [P(x'|x,z,keep) - P(x'|0,z,replace)] · [-log(P(replace|x',z,s))]\n",
    "\n",
    "        Returns: fvt1 of length 2*N*T (stacked: type 0 then type 1)\n",
    "        \"\"\"\n",
    "        p = self.base_params\n",
    "        n_x, n_z = p.n_x, p.n_z\n",
    "        N, T = self.n_buses, self.T\n",
    "        ccps0_2d = ccps0.reshape(n_z, n_x)\n",
    "        ccps1_2d = ccps1.reshape(n_z, n_x)\n",
    "\n",
    "        FV_type = np.zeros((2, n_z, n_x))\n",
    "        for s_idx, ccps_2d in enumerate([ccps0_2d, ccps1_2d]):\n",
    "            for z_idx in range(n_z):\n",
    "                V = -np.log(np.clip(1 - ccps_2d[z_idx, :], 1e-6, 1 - 1e-6))\n",
    "                FV_type[s_idx, z_idx, :] = self._P_continue[z_idx] @ V - self._P_replace[z_idx] @ V\n",
    "\n",
    "        # Build stacked FV array\n",
    "        fvt1 = np.zeros(2 * N * T)\n",
    "        for i, bus_id in enumerate(self.bus_ids):\n",
    "            bd = self.bus_data[bus_id]\n",
    "            z_idx = bd['z_idx']\n",
    "            for t in range(T):\n",
    "                x_idx = bd['x_idx'][t]\n",
    "                fvt1[i * T + t] = FV_type[0, z_idx, x_idx]           # type 0 block\n",
    "                fvt1[N * T + i * T + t] = FV_type[1, z_idx, x_idx]   # type 1 block\n",
    "        return fvt1\n",
    "\n",
    "    # =========================================================================\n",
    "    # likeCCP: structural per-observation likelihood\n",
    "    # =========================================================================\n",
    "\n",
    "    @staticmethod\n",
    "    def likeCCP(bccp: np.ndarray, y_keep: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Per-observation likelihood P(y_obs | X, θ).\n",
    "\n",
    "        P(keep|X) = σ(Xb). Returns P(y_observed | X).\n",
    "        Matches MATLAB likeCCP.m: Like = (Y.*exp(U) + (1-Y)) ./ (1+exp(U))\n",
    "        \"\"\"\n",
    "        sigma = logit_prob(X @ bccp)\n",
    "        return y_keep * sigma + (1 - y_keep) * (1 - sigma)\n",
    "\n",
    "    # =========================================================================\n",
    "    # Initial Conditions\n",
    "    # =========================================================================\n",
    "\n",
    "    def intcond_nll(self, gamma: np.ndarray, base: np.ndarray) -> float:\n",
    "        p0 = logit_prob(self.intcondX @ gamma)\n",
    "        p = np.column_stack([p0, 1 - p0])\n",
    "        marginal = np.sum(p * base, axis=1)\n",
    "        return -np.sum(np.log(np.maximum(marginal, 1e-300))) / self.n_buses\n",
    "\n",
    "    def intcond_posterior(self, gamma: np.ndarray, base: np.ndarray) -> np.ndarray:\n",
    "        p0 = logit_prob(self.intcondX @ gamma)\n",
    "        p = np.column_stack([p0, 1 - p0])\n",
    "        joint = base * p\n",
    "        marginal = np.sum(joint, axis=1, keepdims=True)\n",
    "        return joint / np.maximum(marginal, 1e-300)\n",
    "\n",
    "    # =========================================================================\n",
    "    # EM Algorithm\n",
    "    # =========================================================================\n",
    "\n",
    "    def estimate(self, theta0: np.ndarray = None, max_iter: int = 100,\n",
    "                 tol: float = 1e-7) -> dict:\n",
    "        \"\"\"\n",
    "        Run CCP-EM with model updating + initial conditions.\n",
    "\n",
    "        Algorithm:\n",
    "        1. Solve CCPs to convergence with current θ\n",
    "        2. Compute FV from model CCPs (without β)\n",
    "        3. Compute type likelihoods via likeCCP (structural)\n",
    "        4. Estimate initial conditions γ\n",
    "        5. Compute posterior PType\n",
    "        6. Re-estimate θ via weighted logit with PType weights\n",
    "        \"\"\"\n",
    "        p = self.base_params\n",
    "        N, T = self.n_buses, self.T\n",
    "\n",
    "        if theta0 is None:\n",
    "            theta0 = np.append(self.base_params.theta0_vector(), p.discount)\n",
    "\n",
    "        theta = theta0.copy()\n",
    "        ccps0 = 0.5 * np.ones(p.n_x * p.n_z)\n",
    "        ccps1 = 0.5 * np.ones(p.n_x * p.n_z)\n",
    "        gamma = np.zeros(3)\n",
    "        gamma_start = np.zeros(3)\n",
    "\n",
    "        idx = self.index  # exclude last period\n",
    "        n_td = max(T - 2, 0)\n",
    "        bccp = np.concatenate([theta[:4], np.zeros(n_td)])\n",
    "\n",
    "        start_time = time.time()\n",
    "        lp_history = []\n",
    "        history = []\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            # 1. Solve CCPs to convergence\n",
    "            ccps0 = self.solve_ccps(theta, type_s=0, initial=ccps0)\n",
    "            ccps1 = self.solve_ccps(theta, type_s=1, initial=ccps1)\n",
    "\n",
    "            # 2. Compute FV from model CCPs\n",
    "            fvt1 = self.compute_fv(ccps0, ccps1)\n",
    "\n",
    "            # 3. Type likelihoods via structural model (likeCCP)\n",
    "            X_struct = np.column_stack([self.xccp[idx], fvt1[idx], self.td_struct[idx]])\n",
    "            y_keep = (1 - self.y2[idx]).astype(float)  # 1=keep\n",
    "            Like = self.likeCCP(bccp, y_keep, X_struct)\n",
    "\n",
    "            # Reshape: (N, T-1, 2) → bus-level likelihoods\n",
    "            half = int(idx.sum()) // 2\n",
    "            like0 = Like[:half].reshape(N, T-1)\n",
    "            like1 = Like[half:].reshape(N, T-1)\n",
    "            base0 = np.prod(np.clip(like0, 1e-300, None), axis=1)\n",
    "            base1 = np.prod(np.clip(like1, 1e-300, None), axis=1)\n",
    "            base = np.column_stack([base0, base1])\n",
    "\n",
    "            # 4. Initial conditions (start after iteration 1)\n",
    "            if iteration > 1:\n",
    "                ic_start = gamma_start if iteration < 50 else gamma\n",
    "                res_ic = minimize(self.intcond_nll, ic_start, args=(base,),\n",
    "                                  method='BFGS', options={'disp': False})\n",
    "                gamma = res_ic.x\n",
    "\n",
    "            ll = self.intcond_nll(gamma, base)\n",
    "            lp_history.append(ll)\n",
    "\n",
    "            # 5. Posterior type probabilities\n",
    "            PType = self.intcond_posterior(gamma, base)\n",
    "            tau = PType[:, 1]\n",
    "            pi_new = 1 - tau.mean()\n",
    "\n",
    "            # 6. Expand PType to per-observation weights\n",
    "            PType_obs = np.zeros(2 * N * T)\n",
    "            for i in range(N):\n",
    "                for t in range(T):\n",
    "                    PType_obs[i * T + t] = PType[i, 0]\n",
    "                    PType_obs[N * T + i * T + t] = PType[i, 1]\n",
    "\n",
    "            # 7. Re-estimate structural parameters (weighted logit)\n",
    "            X_struct_full = np.column_stack([self.xccp[idx], fvt1[idx], self.td_struct[idx]])\n",
    "            PType_idx = PType_obs[idx]\n",
    "\n",
    "            def wlogit_nll(b, Y, X, W):\n",
    "                U = X @ b\n",
    "                log1pexp = np.where(U > 20, U, np.log1p(np.exp(np.clip(U, -500, 20))))\n",
    "                return np.sum(W * (log1pexp - Y * U))\n",
    "\n",
    "            res_bccp = minimize(wlogit_nll, bccp, args=(y_keep, X_struct_full, PType_idx),\n",
    "                              method='BFGS', options={'disp': False})\n",
    "            bccp = res_bccp.x\n",
    "            theta_new = bccp[:4]\n",
    "\n",
    "            history.append({'theta': theta_new.copy(), 'pi': pi_new, 'll': ll, 'gamma': gamma.copy()})\n",
    "\n",
    "            if iteration % 5 == 0:\n",
    "                print(f\"Iter {iteration+1}: θ={theta_new.round(3)}, \"\n",
    "                      f\"π={pi_new:.3f}, γ={gamma.round(3)}, LL={ll:.4f}\")\n",
    "\n",
    "            # Convergence check\n",
    "            if len(lp_history) > 26:\n",
    "                c1 = abs((lp_history[-1] - lp_history[-26]) / max(abs(lp_history[-1]), 1e-300))\n",
    "                c2 = abs((lp_history[-2] - lp_history[-27]) / max(abs(lp_history[-2]), 1e-300))\n",
    "                if c1 < tol and c2 < tol:\n",
    "                    print(f\"Converged in {iteration+1} iterations\")\n",
    "                    break\n",
    "\n",
    "            theta = theta_new\n",
    "\n",
    "        return {\n",
    "            'theta': theta_new,\n",
    "            'pi': pi_new,\n",
    "            'gamma': gamma,\n",
    "            'tau': tau,\n",
    "            'ccps0': ccps0,\n",
    "            'ccps1': ccps1,\n",
    "            'log_likelihood': ll,\n",
    "            'n_iterations': iteration + 1,\n",
    "            'computation_time': time.time() - start_time,\n",
    "            'history': history\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T04:59:54.345784Z",
     "iopub.status.busy": "2026-01-30T04:59:54.345712Z",
     "iopub.status.idle": "2026-01-30T05:05:48.190624Z",
     "shell.execute_reply": "2026-01-30T05:05:48.190227Z"
    }
   },
   "source": [
    "# Run CCP-EM with Model updating\n",
    "print(\"=\"*70)\n",
    "print(\"CCP-EM (Model Updating + Initial Conditions)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ccpem_model = CCPEMModelEstimator(data.drop(columns=['type_s']), model.params)\n",
    "result_ccpem_model = ccpem_model.estimate()\n",
    "\n",
    "# Compare posterior types to true types\n",
    "pred_types_model = (result_ccpem_model['tau'] > 0.5).astype(int)\n",
    "accuracy_model = (pred_types_model == true_types).mean()\n",
    "\n",
    "# Display results\n",
    "true_params = model.params.theta_vector()\n",
    "est_params = result_ccpem_model['theta']\n",
    "param_names = ['alpha_1 (intercept)', 'alpha_2 (mileage)', 'alpha_3 (type)', 'beta (discount)']\n",
    "\n",
    "results_table = []\n",
    "for i, name in enumerate(param_names):\n",
    "    results_table.append([name, f\"{true_params[i]:.4f}\", f\"{est_params[i]:.4f}\"])\n",
    "results_table.append(['pi (type prob)', f\"{model.params.type_prob:.4f}\", f\"{result_ccpem_model['pi']:.4f}\"])\n",
    "\n",
    "print(\"\\nStructural Parameter Estimates:\")\n",
    "print(tabulate(results_table, headers=[\"Parameter\", \"True\", \"Estimated\"], tablefmt=\"simple\"))\n",
    "\n",
    "# Estimation statistics\n",
    "stats_table = [\n",
    "    [\"Iterations\", result_ccpem_model['n_iterations']],\n",
    "    [\"Computation time (s)\", f\"{result_ccpem_model['computation_time']:.2f}\"],\n",
    "]\n",
    "print(\"\\nEstimation Statistics:\")\n",
    "print(tabulate(stats_table, headers=[\"Metric\", \"Value\"], tablefmt=\"simple\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Summary: Methods Comparison\n",
    "\n",
    "| Method | Handles Unobs. Het.? | Solves DP? | CCP Updates | Computational Cost |\n",
    "|--------|---------------------|------------|-------------|-------------------|\n",
    "| Two-Step CCP | **No** | No | N/A | Very Low |\n",
    "| NFXP | Yes (with EM) | Yes (every θ) | N/A | Very High |\n",
    "| CCP-EM (Data) | **Yes** | No | Weighted logit | Low |\n",
    "| CCP-EM (Model) | **Yes** | No | Ψ(p; θ) | Low-Medium |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Two-Step fails** with unobserved heterogeneity because it cannot separate type-specific CCPs\n",
    "\n",
    "2. **NFXP + EM** works but is computationally expensive (nested optimization within EM)\n",
    "\n",
    "3. **CCP-EM (Data)**: Updates CCPs via weighted reduced-form estimation. Fast, but may be less efficient.\n",
    "\n",
    "4. **CCP-EM (Model)**: Updates CCPs using structural model. Potentially more efficient, ensures CCPs are model-consistent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T05:05:48.265020Z",
     "iopub.status.busy": "2026-01-30T05:05:48.264925Z",
     "iopub.status.idle": "2026-01-30T05:05:48.267732Z",
     "shell.execute_reply": "2026-01-30T05:05:48.267424Z"
    }
   },
   "source": [
    "# Final comparison\n",
    "print(\"=\"*70)\n",
    "print(\"Summary: Estimation Results (With Observed Types)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# True parameters\n",
    "true_params = model.params.theta_vector()\n",
    "param_names = ['alpha_1 (intercept)', 'alpha_2 (mileage)', 'alpha_3 (type)', 'beta (discount)']\n",
    "\n",
    "# Build comparison table for observed types\n",
    "observed_table = []\n",
    "for i, name in enumerate(param_names):\n",
    "    observed_table.append([\n",
    "        name,\n",
    "        f\"{true_params[i]:.4f}\",\n",
    "        f\"{result_nfxp['theta'][i]:.4f}\",\n",
    "        f\"{result_twostep['theta'][i]:.4f}\"\n",
    "    ])\n",
    "\n",
    "print(tabulate(observed_table, headers=[\"Parameter\", \"True\", \"NFXP\", \"Two-Step\"], tablefmt=\"simple\"))\n",
    "\n",
    "# Build comparison table for unobserved types\n",
    "# Collect all results that exist\n",
    "em_results = {}\n",
    "for label, var in [('NFXP+EM', 'result_nfxp_em'), \n",
    "                    ('CCP-EM (Data)', 'result_ccpem_data'),\n",
    "                    ('CCP-EM (Model)', 'result_ccpem_model')]:\n",
    "    if var in dir():\n",
    "        em_results[label] = eval(var)\n",
    "\n",
    "headers = [\"Parameter\", \"True\"] + list(em_results.keys())\n",
    "unobserved_table = []\n",
    "for i, name in enumerate(param_names):\n",
    "    row = [name, f\"{true_params[i]:.4f}\"]\n",
    "    for res in em_results.values():\n",
    "        row.append(f\"{res['theta'][i]:.4f}\")\n",
    "    unobserved_table.append(row)\n",
    "\n",
    "# Add pi row\n",
    "pi_row = [\"π (type prob)\", f\"{model.params.type_prob:.4f}\"]\n",
    "for res in em_results.values():\n",
    "    pi_row.append(f\"{res['pi']:.4f}\")\n",
    "unobserved_table.append(pi_row)\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\"*70)\n",
    "print(\"Summary: Estimation Results (With Unobserved Types)\")\n",
    "print(\"=\"*70)\n",
    "print(tabulate(unobserved_table, headers=headers, tablefmt=\"simple\"))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
