{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Managerial Compensation and Moral Hazard\n",
    "\n",
    "**Econometrics II - Structural Estimation**\n",
    "\n",
    "*Starter Version - Based on Margiotta & Miller (2000)*\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements the static principal–agent model with moral hazard. We derive the optimal contract, estimate parameters, compute the cost of moral hazard, and conduct counterfactual policy experiments.\n",
    "\n",
    "**Topics:**\n",
    "\n",
    "- Optimal contracting under moral hazard (Kuhn–Tucker conditions)\n",
    "\n",
    "- Identification in principal-agent models\n",
    "\n",
    "- Maximum likelihood estimation\n",
    "\n",
    "- Cost decomposition: risk compensation, effort disutility, output loss\n",
    "\n",
    "- Policy counterfactuals: risk aversion, monitoring, volatility\n",
    "\n",
    "**Data:** Gayle, Golan, and Miller (2015) executive compensation dataset."
   ],
   "id": "4b9073d36231c46c"
  },
  {
   "cell_type": "markdown",
   "id": "fj1du6ictnm",
   "source": [
    "## 0. Quick instructions\n",
    "\n",
    "1. **Data Setup**: Update `DATA_PATH` in the configuration cell to point to `tutorial_data.dta`\n",
    "\n",
    "2. **Performance Variable**: We use `net_excess_ret` (net excess return) as the performance measure $x$\n",
    "\n",
    "3. **Key Parameters**:\n",
    "\n",
    "- $\\gamma$ = risk aversion coefficient (> 0)\n",
    "\n",
    "- $\\alpha, \\beta$ = disutility parameters (with $\\alpha > \\beta > 0$)\n",
    "\n",
    "4. **Output Files**: Results are saved to CSV files in the current directory\n",
    "\n",
    "5. **Runtime**: Full estimation may take several minutes depending on sample size"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ogvhyfb9g7",
   "source": [
    "# Install required packages if needed\n",
    "!pip install numpy pandas scipy matplotlib tabulate statsmodels"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "v2q1t56m8dm",
   "source": "# ============================================================================\n# IMPORTS\n# ============================================================================\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats, optimize\nfrom scipy.stats import gaussian_kde\nfrom scipy.integrate import quad  # Added for numerical integration in g(x) computation\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Dict, Tuple, Callable\nfrom tabulate import tabulate\n\n# Display options\npd.set_option('display.float_format', lambda x: f\"{x:,.6f}\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "zk76ovts11",
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH = './data/final_sample_full06112010.dta'\n",
    "\n",
    "# Variable names from dataset\n",
    "X_VAR = 'net_excess_ret'  # Performance measure (net excess return)\n",
    "W_VAR = 'totalComp'       # Total compensation\n",
    "\n",
    "# Wage scaling factor\n",
    "WAGE_SCALE = 1      # Choose the right scaling for wages (1 = no scaling)\n",
    "\n",
    "# Model hyperparameters\n",
    "ETA_DEFAULT = 1.0  # IC multiplier (eta > 0)\n",
    "N_BOOTSTRAP = 100   # Number of bootstrap samples for SEs\n"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "yd5d91a003",
   "source": [
    "# ============================================================================\n",
    "# RESULT CONTAINER (from Tutorial 2)\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class MLEResult:\n",
    "    \"\"\"\n",
    "    Container for MLE estimation results with enhanced functionality.\n",
    "\n",
    "    Features:\n",
    "    - Basic statistics (z-stat, CI, bias)\n",
    "    - Printing methods (print, summary)\n",
    "    - Data conversion (to_dict, to_dataframe)\n",
    "    - Persistence (save, load)\n",
    "    - Comparison methods\n",
    "    \"\"\"\n",
    "    param_name: str\n",
    "    estimate: float\n",
    "    std_error: float\n",
    "    true_value: Optional[float] = None\n",
    "    sample_size: Optional[int] = None\n",
    "    loglik: Optional[float] = None\n",
    "    convergence: bool = True\n",
    "    method: str = \"Unknown\"\n",
    "\n",
    "    # ========================================================================\n",
    "    # Basic Statistics\n",
    "    # ========================================================================\n",
    "\n",
    "    def z_stat(self) -> float:\n",
    "        \"\"\"Compute z-statistic\"\"\"\n",
    "        return self.estimate / self.std_error if self.std_error > 0 else np.nan\n",
    "\n",
    "    def ci_95(self) -> Tuple[float, float]:\n",
    "        \"\"\"Compute 95% confidence interval\"\"\"\n",
    "        return (self.estimate - 1.96 * self.std_error,\n",
    "                self.estimate + 1.96 * self.std_error)\n",
    "\n",
    "    def ci(self, alpha: float = 0.05) -> Tuple[float, float]:\n",
    "        \"\"\"Compute confidence interval at any level\"\"\"\n",
    "        from scipy.stats import norm\n",
    "        z_crit = norm.ppf(1 - alpha/2)\n",
    "        return (self.estimate - z_crit * self.std_error,\n",
    "                self.estimate + z_crit * self.std_error)\n",
    "\n",
    "    def bias(self) -> float:\n",
    "        \"\"\"Compute bias if true value known\"\"\"\n",
    "        if self.true_value is not None:\n",
    "            return self.estimate - self.true_value\n",
    "        return np.nan\n",
    "\n",
    "    def relative_bias(self) -> float:\n",
    "        \"\"\"Compute relative bias as percentage\"\"\"\n",
    "        if self.true_value is not None and self.true_value != 0:\n",
    "            return 100 * (self.estimate - self.true_value) / self.true_value\n",
    "        return np.nan\n",
    "\n",
    "    # ========================================================================\n",
    "    # Printing Methods\n",
    "    # ========================================================================\n",
    "\n",
    "    def print(self, detailed: bool = False):\n",
    "        \"\"\"Print formatted summary of results\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"MLE Estimation Result: {self.param_name}\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Method:        {self.method}\")\n",
    "        if self.sample_size:\n",
    "            print(f\"Sample size:   N = {self.sample_size}\")\n",
    "\n",
    "        print()\n",
    "        print(\"Point Estimate:\")\n",
    "        print(f\"  {self.param_name} = {self.estimate:.6f}\")\n",
    "        print(f\"  SE({self.param_name}) = {self.std_error:.6f}\")\n",
    "        print(f\"  z-statistic = {self.z_stat():.4f}\")\n",
    "\n",
    "        print()\n",
    "        print(\"Confidence Intervals:\")\n",
    "        ci_95 = self.ci_95()\n",
    "        print(f\"  95% CI: [{ci_95[0]:.6f}, {ci_95[1]:.6f}]\")\n",
    "\n",
    "        if self.true_value is not None:\n",
    "            print()\n",
    "            print(\"Comparison to True Value:\")\n",
    "            print(f\"  True value:     {self.true_value:.6f}\")\n",
    "            print(f\"  Bias:           {self.bias():.6f}\")\n",
    "            print(f\"  |Bias|/SE:      {abs(self.bias())/self.std_error:.4f}\")\n",
    "\n",
    "        if detailed and self.loglik is not None:\n",
    "            print()\n",
    "            print(f\"Log-likelihood: {self.loglik:.4f}\")\n",
    "\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"Alias for print(detailed=True)\"\"\"\n",
    "        self.print(detailed=True)\n",
    "\n",
    "    # ========================================================================\n",
    "    # Data Conversion Methods\n",
    "    # ========================================================================\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert result to dictionary with computed statistics\"\"\"\n",
    "        from dataclasses import asdict\n",
    "        result_dict = asdict(self)\n",
    "        result_dict['z_stat'] = self.z_stat()\n",
    "        ci_lower, ci_upper = self.ci_95()\n",
    "        result_dict['ci_95_lower'] = ci_lower\n",
    "        result_dict['ci_95_upper'] = ci_upper\n",
    "        if self.true_value is not None:\n",
    "            result_dict['bias'] = self.bias()\n",
    "            result_dict['relative_bias'] = self.relative_bias()\n",
    "        return result_dict\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        \"\"\"Convert single result to one-row DataFrame\"\"\"\n",
    "        return pd.DataFrame([self.to_dict()])\n",
    "\n",
    "    @staticmethod\n",
    "    def results_to_dataframe(results: List['MLEResult']):\n",
    "        \"\"\"Convert list of results to DataFrame\"\"\"\n",
    "        if not results:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame([r.to_dict() for r in results])\n",
    "\n",
    "    # ========================================================================\n",
    "    # Persistence Methods\n",
    "    # ========================================================================\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save result to JSON file\"\"\"\n",
    "        import json\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2, default=str)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath: str) -> 'MLEResult':\n",
    "        \"\"\"Load result from JSON file\"\"\"\n",
    "        import json\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        # Remove computed fields\n",
    "        for field in ['z_stat', 'ci_95_lower', 'ci_95_upper', 'bias', 'relative_bias']:\n",
    "            data.pop(field, None)\n",
    "        return cls(**data)\n",
    "\n",
    "    # ========================================================================\n",
    "    # String Representations\n",
    "    # ========================================================================\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = f\"MLE Result for {self.param_name} ({self.method}):\\n\"\n",
    "        s += f\"  Estimate: {self.estimate:.5f} (SE: {self.std_error:.5f})\\n\"\n",
    "        s += f\"  95% CI: [{self.ci_95()[0]:.5f}, {self.ci_95()[1]:.5f}]\\n\"\n",
    "        if self.true_value is not None:\n",
    "            s += f\"  True value: {self.true_value:.5f} (Bias: {self.bias():.5f})\\n\"\n",
    "        return s\n",
    "\n",
    "    def __str__(self):\n",
    "        s = f\"{self.param_name} = {self.estimate:.6f} ({self.std_error:.6f})\"\n",
    "        if self.true_value is not None:\n",
    "            s += f\" [true: {self.true_value:.6f}]\"\n",
    "        return s\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLIFIED MLE ESTIMATOR CLASS (from Tutorial 2)\n",
    "# ============================================================================\n",
    "\n",
    "class MLEEstimator:\n",
    "    \"\"\"\n",
    "    General-purpose Maximum Likelihood Estimator with simplified implementation.\n",
    "\n",
    "    Improvements over original:\n",
    "    - Cleaner standard error computation with single fallback path\n",
    "    - Better error handling\n",
    "    - More consistent interface\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loglik_func: Optional[Callable] = None, param_name: str = \"theta\"):\n",
    "        \"\"\"\n",
    "        Initialize MLE estimator.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        loglik_func : callable, optional\n",
    "            Log-likelihood function with signature: loglik(theta, data, **kwargs)\n",
    "            Should return NEGATIVE log-likelihood (for minimization)\n",
    "        param_name : str\n",
    "            Name of parameter being estimated\n",
    "        \"\"\"\n",
    "        self.loglik_func = loglik_func\n",
    "        self.param_name = param_name\n",
    "\n",
    "    def _compute_standard_error(self, theta_hat: float, objective: Callable,\n",
    "                                 optimizer_result) -> float:\n",
    "        \"\"\"\n",
    "        Compute standard error using numerical Hessian with clean fallback.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        theta_hat : float\n",
    "            Estimated parameter value\n",
    "        objective : callable\n",
    "            Objective function (negative log-likelihood)\n",
    "        optimizer_result : OptimizeResult\n",
    "            Result from scipy.optimize\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        se : float\n",
    "            Standard error (or np.nan if computation fails)\n",
    "        \"\"\"\n",
    "        # Method 1: Numerical Hessian using centered finite differences\n",
    "        try:\n",
    "            # Step 1: Adaptive step size\n",
    "            eps = 1e-5 * max(abs(theta_hat), 1.0)\n",
    "            # Step 2: Three points for evaluation\n",
    "            f_plus = objective([theta_hat + eps])\n",
    "            f_minus = objective([theta_hat - eps])\n",
    "            f_center = objective([theta_hat])\n",
    "            # Step 3: Hessian\n",
    "            hess = (f_plus - 2*f_center + f_minus) / (eps**2)\n",
    "            # Step 4: Check whether positive definite\n",
    "            if hess > 1e-10:\n",
    "                return np.sqrt(1.0 / hess)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Method 2: Fallback to optimizer's inverse Hessian (BFGS only)\n",
    "        if hasattr(optimizer_result, 'hess_inv') and optimizer_result.hess_inv is not None:\n",
    "            try:\n",
    "                if isinstance(optimizer_result.hess_inv, np.ndarray):\n",
    "                    var = (optimizer_result.hess_inv[0, 0] if optimizer_result.hess_inv.ndim > 1\n",
    "                           else optimizer_result.hess_inv[0])\n",
    "                    if var > 0:\n",
    "                        return np.sqrt(var)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    def estimate(self, data: np.ndarray, initial_guess: float,\n",
    "                 bounds: Optional[Tuple[float, float]] = None,\n",
    "                 true_value: Optional[float] = None,\n",
    "                 **kwargs) -> MLEResult:\n",
    "        \"\"\"\n",
    "        Estimate parameter via numerical optimization.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array-like\n",
    "            Observed data\n",
    "        initial_guess : float\n",
    "            Starting value for optimization\n",
    "        bounds : tuple, optional\n",
    "            (min, max) bounds for parameter\n",
    "        true_value : float, optional\n",
    "            True parameter value (for simulation studies)\n",
    "        **kwargs : dict\n",
    "            Additional arguments passed to log-likelihood function\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        result : MLEResult\n",
    "            Estimation results with standard errors\n",
    "        \"\"\"\n",
    "        if self.loglik_func is None:\n",
    "            raise ValueError(\"Must provide log-likelihood function\")\n",
    "\n",
    "        # Define objective\n",
    "        def objective(theta_vec):\n",
    "            return self.loglik_func(theta_vec[0], data, **kwargs)\n",
    "\n",
    "        # Optimize\n",
    "        if bounds is not None:\n",
    "            res = optimize.minimize(objective, [initial_guess], method='L-BFGS-B',\n",
    "                             bounds=[bounds])\n",
    "        else:\n",
    "            res = optimize.minimize(objective, [initial_guess], method='BFGS')\n",
    "\n",
    "        theta_hat = res.x[0]\n",
    "        se = self._compute_standard_error(theta_hat, objective, res)\n",
    "\n",
    "        return MLEResult(\n",
    "            param_name=self.param_name,\n",
    "            estimate=theta_hat,\n",
    "            std_error=se,\n",
    "            true_value=true_value,\n",
    "            sample_size=len(data) if hasattr(data, '__len__') else None,\n",
    "            loglik=-res.fun,\n",
    "            convergence=res.success,\n",
    "            method=\"Numerical (Hessian)\"\n",
    "        )\n",
    "\n",
    "    def estimate_closed_form(self, data: np.ndarray,\n",
    "                            estimator_func: Callable,\n",
    "                            se_func: Callable,\n",
    "                            true_value: Optional[float] = None,\n",
    "                            **kwargs) -> MLEResult:\n",
    "        \"\"\"\n",
    "        Estimate using closed-form formulas.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array-like\n",
    "            Observed data\n",
    "        estimator_func : callable\n",
    "            Function: estimator_func(data, **kwargs) -> float\n",
    "        se_func : callable\n",
    "            Function: se_func(estimate, data, **kwargs) -> float\n",
    "        true_value : float, optional\n",
    "            True parameter value\n",
    "        **kwargs : dict\n",
    "            Additional arguments\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        result : MLEResult\n",
    "        \"\"\"\n",
    "        theta_hat = estimator_func(data, **kwargs)\n",
    "        se = se_func(theta_hat, data, **kwargs)\n",
    "\n",
    "        return MLEResult(\n",
    "            param_name=self.param_name,\n",
    "            estimate=theta_hat,\n",
    "            std_error=se,\n",
    "            true_value=true_value,\n",
    "            sample_size=len(data) if hasattr(data, '__len__') else None,\n",
    "            method=\"Closed-form (Fisher Info)\"\n",
    "        )\n",
    "\n",
    "# ============================================================================\n",
    "# TABLE PRINTING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def print_mle_table(results: List[MLEResult], sample_sizes: List[int],\n",
    "                   title: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Print formatted table of MLE results.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : list of MLEResult\n",
    "        Results for different sample sizes\n",
    "    sample_sizes : list of int\n",
    "        Sample sizes used\n",
    "    title : str, optional\n",
    "        Table title\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "\n",
    "    param_name = results[0].param_name\n",
    "    true_val = results[0].true_value\n",
    "    method = results[0].method\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'N': sample_sizes,\n",
    "        'Estimate': [r.estimate for r in results],\n",
    "        'Std Error': [r.std_error for r in results],\n",
    "        'z-stat': [r.z_stat() for r in results],\n",
    "    })\n",
    "\n",
    "    if true_val is not None:\n",
    "        df['Bias'] = [r.bias() for r in results]\n",
    "        df['|Bias|/SE'] = np.abs(df['Bias']) / df['Std Error']\n",
    "\n",
    "    # Print\n",
    "    print(\"=\" * 85)\n",
    "    if title:\n",
    "        print(title)\n",
    "    else:\n",
    "        print(f\"MLE ESTIMATION RESULTS: {param_name} ({method})\")\n",
    "    print(\"=\" * 85)\n",
    "\n",
    "    if true_val is not None:\n",
    "        print(f\"True value: {param_name} = {true_val:.4f}\")\n",
    "        print(\"-\" * 85)\n",
    "\n",
    "    print(df.to_string(index=False, float_format=lambda x: f'{x:.5f}'))\n",
    "    print(\"=\" * 85)\n",
    "\n",
    "    if true_val is not None:\n",
    "        print(\"\\nNotes:\")\n",
    "        print(\"  - Estimates should converge to true value as N increases\")\n",
    "        print(\"  - Standard errors should decrease proportional to 1/sqrt(N)\")\n",
    "        print(\"  - |Bias|/SE should be small (< 0.1) for unbiased estimation\")\n",
    "        print(\"=\" * 85)"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "### 1.1 Load and prepare data\n",
    "\n",
    "We use the Gayle, Golan, and Miller (2015) executive compensation dataset, which contains:\n",
    "\n",
    "- **Performance measure** ($x$): `net_excess_ret` (net excess return)\n",
    "- **Compensation** ($w$): `totalComp` (total compensation)\n",
    "- **Sample**: Executive-year observations from publicly traded US firms\n",
    "\n",
    "The data is already cleaned and includes various executive and firm characteristics."
   ],
   "id": "7e3dbdcd4fb68828"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.read_stata(DATA_PATH)",
   "id": "e1ab72cafc7e24a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ---------------------------------------------------------------------------\n# 0. Load data and construct x, w\n# ---------------------------------------------------------------------------\n\ntry:\n    df = pd.read_stata(DATA_PATH)\nexcept FileNotFoundError:\n    raise FileNotFoundError(f'Could not find {DATA_PATH}. Update DATA_PATH to the correct location.')\n\n# Drop rows with missing x or w\ndf = df.dropna(subset=[X_VAR, W_VAR])\n\n# Define outcome and wage\ndf['x_outcome'] = df[X_VAR]\ndf['w_obs'] = df[W_VAR]\n\n# Optionally down-sample for speed while testing\n# df = df.sample(5000, random_state=0)\n\nx_data = df['x_outcome'].to_numpy()\nw_data = df['w_obs'].to_numpy() / WAGE_SCALE  # Scale wages to match model\n\n# Check for cluster ID variable (for cluster bootstrap)\n# Common ID columns: execid, exec_id, gvkey, firmid, id\nCLUSTER_VAR = None\nfor col in ['execid', 'exec_id', 'gvkey', 'firmid', 'firm_id', 'id']:\n    if col in df.columns:\n        CLUSTER_VAR = col\n        break\n\nif CLUSTER_VAR is not None:\n    cluster_ids = df[CLUSTER_VAR].to_numpy()\n    print(f'Cluster variable: {CLUSTER_VAR} ({len(np.unique(cluster_ids))} unique clusters)')\nelse:\n    cluster_ids = None\n    print('No cluster variable found - using standard bootstrap')\n\nprint(f'Number of observations: {len(df)}')\nprint(f'Wage scaling factor: {WAGE_SCALE}')\nprint(f'Scaled wages - mean: {w_data.mean():.4f}, std: {w_data.std():.4f}')\ndf[[X_VAR, W_VAR, 'x_outcome', 'w_obs']].head()",
   "id": "6c7c7845954bca0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Framework\n",
    "\n",
    "### 1.1. Setting\n",
    "\n",
    "We work with the static principal–agent model from Margiotta & Miller (2000):\n",
    "\n",
    "- **Agent** chooses unobservable effort $\\ell\\in\\{0,1\\}$ (working vs shirking).\n",
    "\n",
    "- **Output** $X \\in \\mathbb{R}_+$ is observable and stochastic, distributed as:\n",
    "$$\n",
    "X \\sim\n",
    "\\begin{cases}\n",
    "f(x) & \\text{if }\\ell=1\\text{ (working)},\\\\\n",
    "f(x)g(x) & \\text{if }\\ell=0\\text{ (shirking)}\n",
    "\\end{cases}\n",
    "$$\n",
    "where $f(x)$ is a density on $\\mathbb{R}_+$ and $g(x) \\geq 0$ is a likelihood ratio satisfying:\n",
    "$$\n",
    "\\int g(x)f(x)\\,dx = 1, \\qquad \\mathbb{E}[xg(x)] < \\mathbb{E}[x]\n",
    "$$\n",
    "so expected output is lower when shirking.\n",
    "\n",
    "- **Agent preferences** (CARA with risk aversion $\\gamma > 0$):\n",
    "$$\n",
    "U(w,\\ell) = -\\exp(-\\gamma w) \\times\n",
    "\\begin{cases}\n",
    "\\alpha & \\text{if }\\ell=1\\text{ (working)},\\\\\n",
    "\\beta & \\text{if }\\ell=0\\text{ (shirking)},\n",
    "\\end{cases}\n",
    "$$\n",
    "where $\\alpha > \\beta > 0$ captures that shirking has lower disutility than working.\n",
    "\n",
    "- **Outside option** normalized to utility $-1$ (rejecting the contract).\n",
    "\n",
    "- **Principal** is risk-neutral and chooses wage schedule $w(x)$ to maximize expected profit:\n",
    "$$\n",
    "\\max_{w(\\cdot)} \\mathbb{E}[x - w(x)]\n",
    "$$\n",
    "subject to two constraints:\n",
    "\n",
    "**Individual Rationality (IR)**: Agent prefers working to outside option\n",
    "$$\n",
    "\\alpha \\, \\mathbb{E}\\big[\\exp(-\\gamma w(x))\\big] \\; \\leq \\; 1\n",
    "$$\n",
    "\n",
    "**Incentive Compatibility (IC)**: Working dominates shirking\n",
    "$$\n",
    "\\alpha \\, \\mathbb{E}\\big[\\exp(-\\gamma w(x))\\big]\n",
    "\\; \\leq \\;\n",
    "\\beta \\, \\mathbb{E}\\big[\\exp(-\\gamma w(x)) g(x)\\big]\n",
    "$$\n",
    "\n",
    "where expectations are taken with respect to $f(x)$ (the working distribution).\n",
    "\n",
    "**Note on constraint direction**: The negative exponential utility means *lower* values are better. Hence IR requires $\\alpha \\mathbb{E}[\\exp(-\\gamma w)] \\leq 1$ (working utility $\\geq$ outside utility of $-1$), and IC requires working utility $\\geq$ shirking utility.\n"
   ],
   "id": "a913b0a7cdf167"
  },
  {
   "cell_type": "markdown",
   "id": "a3le83habq",
   "source": "### 1.2. Optimal Contract (Question 1)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n**YOUR ANSWER HERE**\n\n---",
   "id": "9245fb830a303747"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Identification (Question 2)\n",
    "\n",
    "From data, we can observe performance $x$ and wages $w(x)$ from various US companies. Our goal is to find a set of parameters $(\\alpha, \\beta, \\gamma, f(x), g(x))$ that supports the observed wage schedule to be an optimal. In this section, we discuss the identification of parameters from observed wages $w(x)$ and the role of knowing $\\gamma$.\n",
    "\n",
    "### 2.1. Non-identification Without Known $\\gamma$\n",
    "\n",
    "**Observables**: From data we observe pairs $(x_i, w_i)$ representing performance and wages.\n",
    "\n",
    "**Unknowns**: We want to identify $(\\gamma, \\alpha, \\beta, f(x), g(x))$.\n",
    "\n",
    "**The problem**: The model is *underidentified* in the static cross-section because:\n"
   ],
   "id": "1bdea2c8060bc48c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n**YOUR ANSWER HERE**\n\n---",
   "id": "af264631f5751d0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2. Identification with Known $\\gamma$\n",
    "\n",
    "**Assumption**: $\\gamma$ is known (e.g., from experimental evidence or calibration)."
   ],
   "id": "fe765f35b53844c8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n**YOUR ANSWER HERE**\n\n---",
   "id": "77a18bcda742fcd5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimation\n",
    "\n",
    "This section implements the estimation of the moral hazard model using a unified `MoralHazardModel` class. The class supports both parametric and non-parametric approaches for estimating key functions:\n",
    "\n",
    "- $f(x)$: density of output under working technology\n",
    "\n",
    "- $g(x)$: likelihood ratio (shirking density / working density)\n",
    "\n",
    "- $v^*(x)$: optimal transformed contract\n",
    "\n",
    "- $w^*(x)$: optimal wage schedule\n",
    "\n",
    "We may impose parametric structure on $f(x)$ and/or $g(x)$, and depending on its specification we estimate the following parameters:\n",
    "| Config | f(x) Estimation | g(x) Estimation | Parameters Estimated |\n",
    "|--------|-----------------|-----------------|---------------------|\n",
    "| **1** | Parametric (Normal) | Parametric (exp) | μ_w, μ_s, σ, γ, α, β, ψ |\n",
    "| **2** | Non-parametric (KDE) | Non-parametric (KDE ratio) | γ, α, β |\n",
    "\n",
    "where:\n",
    "- $\\gamma$: risk aversion parameter\n",
    "\n",
    "- $\\alpha, \\beta$: disutility parameters ($\\alpha < \\beta < 0$)\n",
    "\n",
    "- $\\mu_w, \\mu_s, \\sigma$: parameters of working distribution $N(\\mu_w, \\sigma^2)$ under parametric $f(x)$ and $N(\\mu_s, \\sigma^2)$ under shirking $f(x)g(x)$\n",
    "\n",
    "- $\\psi$: truncation point of output distribution under parametric $f(x)$\n",
    "\n",
    "- $\\sigma_\\varepsilon$: measurement error in wages\n",
    "\n",
    "The estimation proceeds through the following steps:\n",
    "\n",
    "| Stage | Method | Input                   | Output                      |\n",
    "|-------|--------|-------------------------|-----------------------------|\n",
    "| **Data Setup** | `set_data(x, w)` | Raw performance & wages | Cleaned arrays              |\n",
    "| **Step 1** | `step1_estimate_threshold()` | x data                  | Truncation point ψ̂         |\n",
    "| **Step 2** | `step2_estimate_f()` | x data, x̲              | f(x) density or (μ̂_w, σ̂)  |\n",
    "| **Step 3** | `step3_estimate_contract()` | x, w, g(x)              | (γ̂, α̂, β̂, μ̂_w, σ̂\\_ε) |\n",
    "| **Step 4** | `step4_correct_se()` | All estimates           | Bootstrap standard errors   |\n",
    "\n",
    "---\n",
    "#### Output Methods\n",
    "\n",
    "| Method | Returns |\n",
    "|--------|---------|\n",
    "| `v_star(x)` | Optimal transformed contract |\n",
    "| `w_star(x)` | Optimal wage schedule |\n",
    "| `check_constraints()` | IR/IC verification dict |\n",
    "| `compute_moral_hazard_costs()` | Δ₁, Δ₂, Δ₃ decomposition |\n",
    "| `counterfactual(scenarios)` | Policy experiment DataFrame |\n",
    "| `summary_table()` | Parameter estimates DataFrame |\n",
    "---"
   ],
   "id": "b8e235b45549c9dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Parametric Estimation\n",
    "\n",
    "Following the assignment, we impose parametric structure on the densities:\n",
    "\n",
    "**Working distribution**: Truncated normal with lower truncation at $\\psi \\geq 0$:\n",
    "$$\n",
    "X \\mid \\ell=1 \\sim f_w(x) = \\text{TN}(\\mu_w, \\sigma^2; \\psi)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f_w(x) = \\frac{1}{\\sigma}\\frac{\\phi\\left(\\frac{x-\\mu_w}{\\sigma}\\right)}{1 - \\Phi\\left(\\frac{\\psi-\\mu_w}{\\sigma}\\right)}, \\quad x \\geq \\psi\n",
    "$$\n",
    "\n",
    "**Shirking distribution**: Truncated normal with *different mean* but same variance and truncation:\n",
    "$$\n",
    "X \\mid \\ell=0 \\sim f_s(x) = \\text{TN}(\\mu_s, \\sigma^2; \\psi)\n",
    "$$\n",
    "where $\\mu_s < \\mu_w$ (shirking reduces expected output).\n",
    "\n",
    "**Likelihood ratio**:\n",
    "$$\n",
    "g(x) = \\frac{f_s(x)}{f_w(x)} = \\frac{\\phi\\left(\\frac{x-\\mu_s}{\\sigma}\\right)}{\\phi\\left(\\frac{x-\\mu_w}{\\sigma}\\right)} \\times \\frac{1 - \\Phi\\left(\\frac{\\psi-\\mu_w}{\\sigma}\\right)}{1 - \\Phi\\left(\\frac{\\psi-\\mu_s}{\\sigma}\\right)}\n",
    "$$\n",
    "\n",
    "**Parameter vector**:\n",
    "$$\n",
    "\\theta = (\\mu_w, \\sigma, \\mu_s, \\gamma, \\alpha, \\beta, \\sigma_w)\n",
    "$$\n",
    "where $\\sigma_w$ is measurement error in observed wages:\n",
    "$$\n",
    "w_i = w^*(x_i; \\theta, \\eta) + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma_w^2)\n",
    "$$\n",
    "\n",
    "**Step 1 - Estimate Truncation Point**:\n",
    "$$\n",
    "\\hat{\\psi} = \\min_i x_i\n",
    "$$\n",
    "\n",
    "**Step 2 - Estimate Working Distribution** $(\\mu_w, \\sigma)$:\n",
    "\n",
    "Maximum likelihood for truncated normal:\n",
    "$$\n",
    "\\max_{\\mu_w, \\sigma} \\sum_{i=1}^N \\log f_w(x_i; \\mu_w, \\sigma, \\hat{\\psi})\n",
    "$$\n",
    "\n",
    "**Step 3 - Estimate Contract Parameters** $(\\gamma, \\alpha, \\beta, \\mu_s, \\sigma_w)$:\n",
    "\n",
    "Nonlinear least squares on wage equation:\n",
    "$$\n",
    "\\min_{\\gamma, \\alpha, \\beta, \\mu_s, \\sigma_w} \\sum_{i=1}^N \\left(w_i - w^*(x_i; \\theta, \\eta(\\theta))\\right)^2\n",
    "$$\n",
    "\n",
    "where $\\eta(\\theta)$ is solved numerically from the binding IC constraint:\n",
    "$$\n",
    "\\alpha \\mathbb{E}[v^*(x)] = \\beta \\mathbb{E}[v^*(x)g(x)]\n",
    "$$\n",
    "\n",
    "**Step 4 - Standard Errors**:\n",
    "\n",
    "Two approaches:\n",
    "1. **Bootstrap**: Resample observations (or clusters) with replacement, re-estimate\n",
    "2. **Murphy-Topel correction**: Analytical correction for two-step estimation\n",
    "---"
   ],
   "id": "b4f0cc42ef1d1822"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Non-Parametric Alternative\n",
    "\n",
    "For robustness, we can estimate $f(x)$ and/or $g(x)$ non-parametrically:\n",
    "\n",
    "**Non-parametric $f(x)$**: Use kernel density estimation on observed $\\{x_i\\}$\n",
    "$$\n",
    "\\hat{f}(x) = \\frac{1}{Nh}\\sum_{i=1}^N K\\left(\\frac{x - x_i}{h}\\right)\n",
    "$$\n",
    "\n",
    "**Non-parametric $g(x)$**: Under current parameter guess $\\gamma$, compute weights $v_i = \\exp(-\\gamma w_i)$ and estimate $g(x)$ via weighted KDE\n",
    "\n",
    "**Trade-offs**:\n",
    "- **Parametric**: Efficient if specification correct, but subject to misspecification\n",
    "- **Non-parametric**: Robust to functional form, but requires larger sample and careful bandwidth selection\n",
    "\n",
    "---"
   ],
   "id": "68ed093f876a0080"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 The MoralHazardModel Class\n",
    "\n",
    "Below is the complete implementation of the model class, which provides:- Flexible estimation methods (parametric/non-parametric)\n",
    "\n",
    "- Automatic constraint checking\n",
    "\n",
    "- Cost decomposition\n",
    "\n",
    "- Counterfactual analysis\n"
   ],
   "id": "9d05b4f4c6ee8c15"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "\"\"\"\nMoral Hazard Model - Student Implementation\n\nTODO: Implement the MoralHazardModel class following the lecture notes.\n\nEstimation procedure:\n1. Estimate truncation threshold ψ (minimum observed x)\n2. Estimate (μ, σ) via LIML on truncated normal\n3. Estimate (γ, α, β, μ_s) via NLS with inner loop for η\n\nKey formulas:\n- f(x|work): TN(μ, σ²; ψ) - truncated normal\n- f(x|shirk): TN(μ_s, σ²; ψ) - truncated normal  \n- g(x) = f(x|shirk) / f(x|work) - likelihood ratio\n- v*(x) = 1 / [α(1 + η(α/β - g(x)))] - optimal contract\n- w*(x) = (1/γ) ln[α(1 + η(α/β - g(x)))] - optimal wage\n- IR: α E[v*] = 1 (binding)\n- IC: α E[v*] = β E[v*g] (binding)\n\"\"\"\n\nfrom typing import Dict, Optional, Tuple\nimport numpy as np\nfrom scipy import stats, optimize\nfrom scipy.stats import gaussian_kde\nimport pandas as pd\n\nclass MoralHazardModel:\n    \"\"\"\n    Sequential Moral Hazard Estimator\n    \n    Implements the three-step estimation procedure from lecture notes.\n    Students should implement all methods marked with TODO.\n    \"\"\"\n    \n    def __init__(self, config: Optional[Dict] = None):\n        \"\"\"\n        Initialize the model with optional configuration.\n        \n        Parameters:\n        -----------\n        config : dict, optional\n            Configuration dictionary. Keys may include:\n            - 'use_nonparam_f': Use KDE for f(x) (default: False)\n            - 'use_nonparam_g': Use KDE for g(x) (default: False)\n            - 'kde_bandwidth': Bandwidth for KDE (default: 'scott')\n        \"\"\"\n        self.config = {\n            'use_nonparam_f': False,\n            'use_nonparam_g': False,\n            'kde_bandwidth': 'scott',\n        }\n        if config is not None:\n            self.config.update(config)\n        \n        # Data\n        self.x = None  # Performance data\n        self.w = None  # Wage data\n        self.n = None  # Sample size\n        \n        # Step 1 results\n        self.psi = None  # Truncation threshold\n        \n        # Step 2 results\n        self.mu_hat = None    # Mean of f(x)\n        self.sigma_hat = None # Std of f(x)\n        \n        # Step 3 results\n        self.gamma_hat = None   # Risk aversion\n        self.alpha_hat = None   # Utility parameter (work)\n        self.beta_hat = None    # Utility parameter (shirk)\n        self.mu_s_hat = None    # Mean under shirking\n        self.eta_hat = None     # Lagrange multiplier (IC)\n        self.sigma_eps_hat = None  # Residual std\n    \n    def set_data(self, x: np.ndarray, w: np.ndarray):\n        \"\"\"\n        Load performance and wage data.\n        \n        Parameters:\n        -----------\n        x : np.ndarray\n            Performance/output data (net excess returns)\n        w : np.ndarray\n            Wage/compensation data (total compensation)\n        \"\"\"\n        self.x = np.asarray(x).flatten()\n        self.w = np.asarray(w).flatten()\n        self.n = len(self.x)\n        \n        print(f\"Data loaded: n = {self.n}\")\n        print(f\"  x: mean={np.mean(self.x):.4f}, std={np.std(self.x):.4f}\")\n        print(f\"  w: mean={np.mean(self.w):.4f}, std={np.std(self.w):.4f}\")\n    \n    # =========================================================================\n    # STEP 1: Estimate truncation threshold\n    # =========================================================================\n    \n    def step1_estimate_threshold(self) -> float:\n        \"\"\"\n        Step 1: Estimate truncation threshold ψ.\n        \n        The truncation point is the minimum observed performance.\n        \n        Returns:\n        --------\n        psi : float\n            Estimated truncation threshold\n        \"\"\"\n        # TODO: Implement threshold estimation\n        # Hint: Use minimum of observed x\n        raise NotImplementedError(\"TODO: Implement step1_estimate_threshold\")\n    \n    # =========================================================================\n    # STEP 2: Estimate f(x) parameters\n    # =========================================================================\n    \n    def step2_estimate_f(self) -> Tuple[float, float]:\n        \"\"\"\n        Step 2: Estimate parameters of f(x).\n        \n        Parametric: Estimate (μ, σ) via LIML on truncated normal\n        Non-parametric: Estimate f(x) via KDE\n        \n        Returns:\n        --------\n        mu_hat : float\n            Estimated mean (None if non-parametric)\n        sigma_hat : float\n            Estimated std (None if non-parametric)\n        \"\"\"\n        if self.config['use_nonparam_f']:\n            return self._step2_nonparametric()\n        else:\n            return self._step2_parametric()\n    \n    def _step2_parametric(self) -> Tuple[float, float]:\n        \"\"\"\n        Estimate (μ, σ) via Limited Information Maximum Likelihood (LIML)\n        on truncated normal distribution.\n        \n        The log-likelihood for truncated normal TN(μ, σ²; ψ) is:\n        ℓ(μ,σ) = Σ log φ((x_i - μ)/σ) - n log σ - n log(1 - Φ((ψ-μ)/σ))\n        \n        where φ is standard normal PDF and Φ is standard normal CDF.\n        \n        Returns:\n        --------\n        mu_hat, sigma_hat : floats\n        \"\"\"\n        # TODO: Implement truncated normal MLE\n        # Hint: Use scipy.optimize.minimize with negative log-likelihood\n        raise NotImplementedError(\"TODO: Implement _step2_parametric\")\n    \n    def _step2_nonparametric(self) -> Tuple[None, None]:\n        \"\"\"\n        Estimate f(x) non-parametrically via KDE.\n        \n        Returns:\n        --------\n        None, None (KDE stored in self.f_kde)\n        \"\"\"\n        # TODO: Implement KDE estimation\n        # Hint: Use scipy.stats.gaussian_kde\n        raise NotImplementedError(\"TODO: Implement _step2_nonparametric\")\n    \n    # =========================================================================\n    # STEP 3: Estimate contract parameters\n    # =========================================================================\n    \n    def step3_estimate_contract(self,\n                                gamma_init: float = 0.5,\n                                alpha_init: float = 2.0,\n                                beta_init: float = 1.0,\n                                mu_s_init: Optional[float] = None) -> Dict:\n        \"\"\"\n        Step 3: Estimate contract parameters via NLS.\n        \n        Estimate (γ, α, β) [and μ_s if parametric g] by minimizing:\n        Σ (w_i - w*(x_i))²\n        \n        where w*(x) = (1/γ) ln[α(1 + η(α/β - g(x)))]\n        \n        η is solved from the binding IR constraint: α E[v*] = 1\n        \n        Parameters:\n        -----------\n        gamma_init : float\n            Initial value for risk aversion\n        alpha_init : float\n            Initial value for α (utility when working)\n        beta_init : float\n            Initial value for β (utility when shirking)\n        mu_s_init : float, optional\n            Initial value for μ_s (only for parametric g)\n        \n        Returns:\n        --------\n        results : dict\n            Dictionary containing estimated parameters\n        \"\"\"\n        if self.config['use_nonparam_g']:\n            return self._step3_nonparametric_g(gamma_init, alpha_init, beta_init)\n        else:\n            return self._step3_parametric_g(gamma_init, alpha_init, beta_init, mu_s_init)\n    \n    def _compute_g_parametric(self, x: np.ndarray, mu_s: float) -> np.ndarray:\n        \"\"\"\n        Compute likelihood ratio g(x) = f_s(x)/f_w(x) parametrically.\n        \n        Both f_s and f_w are truncated normals with same σ but different means.\n        \n        Parameters:\n        -----------\n        x : np.ndarray\n            Performance values\n        mu_s : float\n            Mean under shirking (μ_s < μ for effort to increase output)\n        \n        Returns:\n        --------\n        g : np.ndarray\n            Likelihood ratio values\n        \"\"\"\n        # TODO: Implement parametric g(x) computation\n        # Hint: g(x) = f_s(x)/f_w(x) where both are truncated normals\n        raise NotImplementedError(\"TODO: Implement _compute_g_parametric\")\n    \n    def _solve_eta(self, alpha: float, beta: float, g_vals: np.ndarray) -> float:\n        \"\"\"\n        Solve for η from binding IR constraint: α E[v*] = 1\n        \n        v*(x) = 1 / [α(1 + η(α/β - g(x)))]\n        \n        Parameters:\n        -----------\n        alpha, beta : floats\n            Utility parameters\n        g_vals : np.ndarray\n            Likelihood ratio values\n        \n        Returns:\n        --------\n        eta : float\n            Lagrange multiplier on IC constraint\n        \"\"\"\n        # TODO: Implement eta solver\n        # Hint: Find η such that α * mean(v*) = 1\n        raise NotImplementedError(\"TODO: Implement _solve_eta\")\n    \n    def _compute_wage_residuals(self, gamma: float, alpha: float, beta: float,\n                                 eta: float, g_vals: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute wage residuals: w_i - w*(x_i)\n        \n        w*(x) = (1/γ) ln[α(1 + η(α/β - g(x)))]\n        \n        Returns:\n        --------\n        residuals : np.ndarray\n        \"\"\"\n        # TODO: Implement wage residual computation\n        raise NotImplementedError(\"TODO: Implement _compute_wage_residuals\")\n    \n    def _step3_parametric_g(self, gamma_init: float, alpha_init: float,\n                            beta_init: float, mu_s_init: Optional[float]) -> Dict:\n        \"\"\"\n        Estimate (γ, α, β, μ_s) via NLS with parametric g(x).\n        \n        Minimizes Σ(w_i - w*(x_i))² over (γ, α, β, μ_s)\n        with η solved from IR constraint in inner loop.\n        \n        Returns:\n        --------\n        results : dict\n        \"\"\"\n        # TODO: Implement NLS estimation with parametric g\n        raise NotImplementedError(\"TODO: Implement _step3_parametric_g\")\n    \n    def _step3_nonparametric_g(self, gamma_init: float, alpha_init: float,\n                                beta_init: float) -> Dict:\n        \"\"\"\n        Estimate (γ, α, β) via NLS with non-parametric g(x).\n        \n        g(x) is estimated via KDE-based importance sampling.\n        \n        Returns:\n        --------\n        results : dict\n        \"\"\"\n        # TODO: Implement NLS estimation with non-parametric g\n        raise NotImplementedError(\"TODO: Implement _step3_nonparametric_g\")\n    \n    # =========================================================================\n    # Helper methods for g(x) and wage computation\n    # =========================================================================\n    \n    def g(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Evaluate g(x) at given points using estimated parameters.\n        \"\"\"\n        # TODO: Implement g(x) evaluation\n        raise NotImplementedError(\"TODO: Implement g\")\n    \n    def optimal_wage(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute optimal wage w*(x) at given points.\n        \"\"\"\n        # TODO: Implement optimal wage computation\n        raise NotImplementedError(\"TODO: Implement optimal_wage\")\n    \n    # =========================================================================\n    # Analysis methods\n    # =========================================================================\n    \n    def compute_cost_decomposition(self) -> Dict:\n        \"\"\"\n        Decompose the cost of moral hazard into three components:\n        \n        Δ₁: Risk premium (cost due to agent's risk aversion)\n        Δ₂: Effort disutility compensation  \n        Δ₃: Output loss from moral hazard\n        \n        Returns:\n        --------\n        costs : dict\n            Dictionary with Δ₁, Δ₂, Δ₃, and total cost\n        \"\"\"\n        # TODO: Implement cost decomposition\n        raise NotImplementedError(\"TODO: Implement compute_cost_decomposition\")\n    \n    def run_counterfactual(self, gamma_new: Optional[float] = None,\n                           alpha_new: Optional[float] = None,\n                           beta_new: Optional[float] = None) -> Dict:\n        \"\"\"\n        Run counterfactual experiment with modified parameters.\n        \n        Parameters:\n        -----------\n        gamma_new : float, optional\n            New risk aversion (default: use estimated)\n        alpha_new : float, optional\n            New α parameter (default: use estimated)\n        beta_new : float, optional\n            New β parameter (default: use estimated)\n        \n        Returns:\n        --------\n        results : dict\n            Counterfactual analysis results\n        \"\"\"\n        # TODO: Implement counterfactual analysis\n        raise NotImplementedError(\"TODO: Implement run_counterfactual\")\n    \n    def verify_constraints(self) -> Dict:\n        \"\"\"\n        Verify that IR and IC constraints are satisfied.\n        \n        IR: α E[v*] = 1\n        IC: α E[v*] = β E[v*g]\n        \n        Returns:\n        --------\n        verification : dict\n            IR and IC constraint values and errors\n        \"\"\"\n        # TODO: Implement constraint verification\n        raise NotImplementedError(\"TODO: Implement verify_constraints\")\n    \n    def summary(self):\n        \"\"\"Print estimation summary.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"MORAL HAZARD MODEL - ESTIMATION SUMMARY\")\n        print(\"=\"*60)\n        \n        print(\"\\nStep 1: Truncation threshold\")\n        print(f\"  ψ = {self.psi:.6f}\" if self.psi else \"  Not estimated\")\n        \n        print(\"\\nStep 2: Distribution parameters\")\n        if self.mu_hat is not None:\n            print(f\"  μ = {self.mu_hat:.6f}\")\n            print(f\"  σ = {self.sigma_hat:.6f}\")\n        else:\n            print(\"  Non-parametric (KDE)\")\n        \n        print(\"\\nStep 3: Contract parameters\")\n        if self.gamma_hat is not None:\n            print(f\"  γ (risk aversion) = {self.gamma_hat:.6f}\")\n            print(f\"  α (work utility)  = {self.alpha_hat:.6f}\")\n            print(f\"  β (shirk utility) = {self.beta_hat:.6f}\")\n            print(f\"  η (IC multiplier) = {self.eta_hat:.6f}\")\n            if self.mu_s_hat is not None:\n                print(f\"  μ_s (shirk mean)  = {self.mu_s_hat:.6f}\")\n        else:\n            print(\"  Not estimated\")\n",
   "id": "9918d8738a82e0f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.5 Test Your Implementation\n\nAfter implementing the `MoralHazardModel` class, test it with the cells below.\n\n**Tip:** Start by running on simulated data where you know the true parameters, then apply to real data.\n",
   "id": "8745100d8a9d3071"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test your implementation\n# Uncomment and run after implementing the class\n\n# model = MoralHazardModel()\n# model.set_data(x_data, w_data)\n\n# # Step 1\n# psi = model.step1_estimate_threshold()\n# print(f\"Threshold: {psi:.4f}\")\n\n# # Step 2\n# mu, sigma = model.step2_estimate_f()\n# print(f\"mu={mu:.4f}, sigma={sigma:.4f}\")\n\n# # Step 3\n# results = model.step3_estimate_contract()\n# model.summary()\n",
   "id": "d69a1ca3e918eb7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Results\n",
    "### 4.1 Parametric Estimation"
   ],
   "id": "91a747dcdbf484ff"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Run your analysis here\n",
   "id": "f5beea6070079c9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Non-Parametric Estimation\n",
    "We now estimate the model using non-parametric methods for both the working density $f(x)$"
   ],
   "id": "e2564fa202553325"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Run your analysis here\n",
   "id": "55de8e492e58570c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model Verification (IR and IC Constraints)\n",
    "\n",
    "Verify that the estimated model satisfies the theoretical constraints from the principal-agent problem:\n",
    "\n",
    "- **Individual Rationality (IR) Constraint:**$$\\alpha E[v(X)] \\leq 1$$At the optimum, this constraint binds (holds with equality).\n",
    "- **Incentive Compatibility (IC) Constraint:**$$\\alpha E[v(X)] \\leq \\beta E[v(X)g(X)]$$At the optimum, this constraint also binds.\n",
    "\n",
    "We check these constraints using the observed contract $(x, w)$ data and estimated parameters.\n",
    "\n",
    "---"
   ],
   "id": "1c9b064f04f65a9e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Run your analysis here\n",
   "id": "6da28a46567197e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Cost of Moral Hazard Decomposition\n",
    "\n",
    "Decompose the total cost of moral hazard into three components:\n",
    "- **Δ₁: Risk Premium**: The extra expected wage cost from tying pay to noisy output (risk compensation). $$\\Delta_1 = E[w^*(x)] - \\frac{1}{\\gamma}\\ln(\\alpha)$$\n",
    "- **Δ₂: Effort Disutility**: The monetized disutility of inducing effort (compensation for working vs shirking). $$\\Delta_2 = \\frac{1}{\\gamma}\\ln\\left(\\frac{\\beta}{\\alpha}\\right)$$\n",
    "- **Δ₃: Output Loss**: The expected output loss if the agent were to shirk. $$\\Delta_3 = E[x] - E[x \\cdot g(x)]$$\n",
    "\n",
    "**Total Cost:**\n",
    "$$\n",
    "\\text{Total Cost} = \\Delta_1 + \\Delta_2 + \\Delta_3\n",
    "$$\n",
    "\n",
    "This equals the profit loss compared to the first-best (full information) benchmark.\n",
    "\n",
    "---"
   ],
   "id": "8ec48dcfb660d438"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Run your analysis here\n",
   "id": "15bc3485426b095c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Counterfactual Experiments\n",
    "\n",
    "This section conducts policy experiments to understand how changes in the environment affect moral hazard costs.\n",
    "\n",
    "We analyze how modifications to key parameters impact the three cost components (Δ₁, Δ₂, Δ₃) and total cost.\n",
    "\n",
    "---"
   ],
   "id": "dfcd6914b308206d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Policy Experiments\n",
    "\n",
    "We evaluate four counterfactual scenarios to understand how environmental changes affect moral hazard costs:\n",
    "\n",
    "**1. Increased Risk Aversion** ($\\gamma \\to 2\\gamma$)\n",
    "   - Agents become twice as risk-averse\n",
    "   - Expected impact: Higher risk premium (Δ₁↑) as agents demand greater compensation for income volatility\n",
    "\n",
    "**2. Lower Truncation Point** ($\\psi \\to \\psi/2$)\n",
    "   - Lower threshold in the truncated normal distribution\n",
    "   - Expected impact: Changes the support and shape of f(x), affecting both the working and shirking distributions. May alter the likelihood ratio g(x) and increase total cost.\n",
    "\n",
    "**3. Increased Output Volatility** ($\\sigma \\to 2\\sigma$)\n",
    "   - Doubled standard deviation introduces more noise into the performance measure\n",
    "   - Expected impact: Higher risk premium (Δ₁↑) as agents bear greater income uncertainty\n",
    "\n",
    "**4. Reduced Effort Cost Differential** ($\\beta \\to 1.5\\beta$)\n",
    "   - Shirking disutility moves closer to working disutility ($\\beta \\to \\alpha$)\n",
    "   - Expected impact: Lower incentive cost (Δ₂↓) as the utility gap between effort levels narrows, making effort easier to induce\n",
    "\n",
    "---"
   ],
   "id": "419dd943b1bde6a8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# TODO: Run your analysis here\n",
   "id": "6db8ae9a1508d61e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
