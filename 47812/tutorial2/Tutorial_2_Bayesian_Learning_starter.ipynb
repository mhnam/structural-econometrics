{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Dynamic Career Choice with Bayesian Learning\n",
    "\n",
    "**Econometrics II - Structural Estimation**  \n",
    "*Solution Version - Last Updated: 2024-10-29*\n",
    "\n",
    "---\n",
    "\n",
    "### Navigation\n",
    "- [Model Setting](#setting)\n",
    "- [Example: Two-period Model](#t2)\n",
    "- [General T Periods](#general)\n",
    "- [Estimation and Inference](#estimation)"
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T04:52:56.405376Z",
     "iopub.status.busy": "2025-10-29T04:52:56.405292Z",
     "iopub.status.idle": "2025-10-29T04:52:57.221411Z",
     "shell.execute_reply": "2025-10-29T04:52:57.220952Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:46.964321Z",
     "start_time": "2025-11-02T04:22:46.959200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "from scipy.stats import beta, binom, norm, chi2\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, List\n",
    "from math import log, sqrt\n",
    "\n",
    "# Display settings\n",
    "np.set_printoptions(suppress=True, linewidth=120, precision=4)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T04:52:57.236096Z",
     "iopub.status.busy": "2025-10-29T04:52:57.235958Z",
     "iopub.status.idle": "2025-10-29T04:52:57.238092Z",
     "shell.execute_reply": "2025-10-29T04:52:57.237697Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:46.970739Z",
     "start_time": "2025-11-02T04:22:46.968633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Default Parameters\n",
    "gamma_0 = 3.0   # Beta prior shape parameter (\"prior successes\")\n",
    "delta_0 = 2.0   # Beta prior shape parameter (\"prior failures\")\n",
    "w_true = 0.55   # Outside wage (true value for data generation)\n",
    "beta = 0.96    # Discount factor\n",
    "\n",
    "# Simulation settings\n",
    "N_agents_default = 20000\n",
    "seed_default = 123\n",
    "rng = np.random.default_rng(seed_default)\n",
    "\n",
    "print(f\"Default Parameters:\")\n",
    "print(f\"  Prior: Beta({gamma_0}, {delta_0})\")\n",
    "print(f\"  Mean ability E[\\\\xi] = {gamma_0/(gamma_0+delta_0):.4f}\")\n",
    "print(f\"  Outside wage w = {w_true}\")\n",
    "print(f\"  Discount factor beta = {beta}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parameters:\n",
      "  Prior: Beta(3.0, 2.0)\n",
      "  Mean ability E[\\xi] = 0.6000\n",
      "  Outside wage w = 0.55\n",
      "  Discount factor beta = 0.96\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. [Model Setting](#setting)<a name=\"setting\"></a>\n",
    "\n",
    "We study a **finite-horizon dynamic programming model** where an inventor faces an unknown success probability $\\xi$ and must choose between:\n",
    "- **Inventing** ($d=1$): Explore/exploit technology with Bernoulli($\\xi$) success, earning 1 if successful\n",
    "- **Outside option** ($d=0$): Take a safe wage $w$\n",
    "\n",
    "### 1.1. Model Components\n",
    "\n",
    "**Time and Decisions:**\n",
    "- Time horizon: $t=1,\\ldots,T$ where $T < \\infty$\n",
    "- Choice variable each period: $d_t \\in \\{0, 1\\}$\n",
    "  - $d_t = 0$: Take outside wage $w$ (certain payoff)\n",
    "  - $d_t = 1$: Attempt invention\n",
    "\n",
    "**Invention Outcomes:**\n",
    "- If inventing: $x_t \\sim \\text{Bernoulli}(\\xi)$\n",
    "  - Success ($x_t = 1$): Earn payoff of 1 in period $t$\n",
    "  - Failure ($x_t = 0$): Earn 0 in period $t$\n",
    "\n",
    "**Ability and Learning:**\n",
    "- Unknown ability: $\\xi \\sim \\beta(\\gamma_0, \\delta_0)$ drawn once at beginning\n",
    "- Agent doesn't know $xi$ but knows the prior distribution\n",
    "- **Bayesian Learning:** Updates beliefs about $xi$ using Beta-Bernoulli conjugacy\n",
    "  - Prior: $\\xi \\sim \\beta(\\gamma_0, \\delta_0)$\n",
    "  - After $s$ successes and $f$ failures: $\\xi \\sim \\beta(\\gamma_0+s, \\delta_0+f)$\n",
    "  - Success: $(\\gamma, \\delta) \\rightarrow (\\gamma+1, \\delta)$\n",
    "  - Failure: $(\\gamma, \\delta) \\rightarrow (\\gamma, \\delta+1)$\n",
    "\n",
    "**Preferences:**\n",
    "- Lifetime utility: $\\sum_{t=1}^{T} \\beta^{t-1} [d_t\\cdot x_t + (1-d_t)\\cdot w]$\n",
    "- Discount factor: $\\beta \\in (0,1)$\n",
    "- **Implied assumption:** Risk-neutral agent (expected payoff maximizer)\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2. Key Insight: Beta-Bernoulli Conjugacy\n",
    "\n",
    "The Beta distribution is a **conjugate prior** for the Bernoulli likelihood:\n",
    "- Prior: $g(\\xi; \\gamma, \\delta) = \\frac{\\xi^{\\gamma-1}(1-\\xi)^{\\delta-1}}{B(\\gamma,\\delta)}$ where $B(\\gamma, \\delta) = \\int_0^1 u^{\\gamma-1}(1-u)^{\\delta-1}du$\n",
    "- Likelihood: $f(x|\\xi) = \\xi^x(1-\\xi)^{1-x}$\n",
    "- Posterior: $f(\\xi|x) = \\frac{\\xi^{\\gamma+x-1}(1-\\xi)^{\\delta+(1-x)-1}}{B(\\gamma+x, \\delta+(1-x))}$\n",
    "\n",
    "This means the posterior is also Beta distributed with updated parameters:\n",
    "$$\\xi | x \\sim \\beta(\\gamma + x, \\delta + (1-x))$$\n",
    "\n",
    "**Intuition:** Think of $\\gamma$ as \"prior successes\" and $\\delta$ as \"prior failures\". Each new observation updates the count.\n",
    "\n",
    "**Expected success probability:**\n",
    "$$\\mathbb{E}[\\xi | \\gamma, \\delta] = \\frac{\\gamma}{\\gamma + \\delta}$$\n",
    "\n",
    "After observing outcome $x$:\n",
    "$$\\mathbb{E}[\\xi | \\gamma', \\delta'] = \\begin{cases}\n",
    "\\frac{\\gamma}{\\gamma + (\\delta+1)} & \\text{if } x=0 \\text{ (failure)} \\\\\n",
    "\\frac{\\gamma+1}{(\\gamma+1) + \\delta} & \\text{if } x=1 \\text{ (success)}\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. [Example: Two-period Model](#t2)<a name=\"t2\"></a>\n",
    "\n",
    "### 2.1. Benchmark: Static Problem\n",
    "Let's start with a simple setting. Let suppose that we are solving a static model. In this case the lifetime utility is\n",
    "\n",
    "$$d_t\\mathbf{1} \\{x_t=1\\} + (1-d_t)w$$\n",
    "\n",
    "If we knew the **exact value** of $\\xi$, then the decision rule is simple: we choose to invent if the expected payoff of invention is larger than the payoff of the outside option given $\\xi$:\n",
    "$$\n",
    "\\mathbb{E}_{x}\\big[1\\cdot\\mathbf{1}\\{x=1\\}+0\\cdot\\mathbf{1}\\{x=0\\}|\\xi]=1\\cdot\\mathbb{P}(x=1|\\xi) + 0\\cdot\\mathbb{P}(x=0|\\xi) = \\xi.\n",
    "$$\n",
    "In this case, the decision rule would be:\n",
    "$$d_t=\\begin{cases} 1 &\\text{ if } \\xi>w \\\\\n",
    "0  &\\text{ otherwise } \\end{cases}$$\n",
    "\n",
    "However, the agent only knows that her ability $\\xi$ is a random draw from a beta distribution $\\beta(\\gamma,\\delta)$, not its exact value. As a risk-neutral agent, the agent would compute her expected return on inventing and decide whether it is worth trying or not.\n",
    "$$\n",
    "\\mathbb{E}_{\\xi} \\Big[\\mathbb{E}_{x} \\big[ \\mathbf{1}\\{x=1\\} \\big| \\xi \\big] \\Big|\\gamma,\\delta\\Big] =\\mathbb{E}_{\\xi} \\big[1 \\cdot \\mathbb{P}(x=1|\\xi) + 0 \\cdot \\mathbb{P}(x=0|\\xi) \\big| \\gamma,\\delta \\big] =\\mathbb{E}_{\\xi} [1 \\cdot \\xi + 0 \\cdot (1-\\xi) | \\gamma,\\delta] = \\mathbb{E}_{\\xi}[{\\xi}|\\gamma,\\delta]=\\frac{\\gamma}{\\gamma+\\delta}.\n",
    "$$\n",
    "\n",
    "The decision rule is then:\n",
    "$$\n",
    "d_t=\\begin{cases} 1 &\\text{ if } \\frac{\\gamma}{\\gamma+\\delta}>w \\\\\n",
    "0  &\\text{ otherwise } \\end{cases}\n",
    "$$\n",
    "\n",
    "By now, you might have noticed that in the static case, everyone makes the same choice! This is because in the first period, everyone has the same belief about their ability. As agents try out inventing and finds more about their ability, we will see that people making different choices.\n",
    "\n",
    "**Remark**:\n",
    "Given the data where everyone making the same choice (i.e., all zeros or all ones), we cannot point-identify **any** parameter of the set of parameters $(\\gamma,\\delta,w)$. Let's take a step back and look at the decision rule:\n",
    "$$\n",
    "d_t=\\begin{cases} 1 &\\text{ if } \\frac{\\gamma}{\\gamma+\\delta}>w \\\\\n",
    "0  &\\text{ otherwise } \\end{cases}\n",
    "$$\n",
    "\n",
    "- If we are given the true value of one of $(\\gamma,\\delta,w)$, can you find the exact values for the rest of parameters? No.\n",
    "  - For example, let's say we are given $w=42$. What are the values of $\\gamma$ and $\\delta$ that can generate the data we have? All $(\\gamma,\\delta)$ that satisfy $\\frac{\\gamma}{\\gamma+\\delta}>42$.\n",
    "  - Since we have only one inequality and two parameters, we cannot point-identify $\\gamma$ and $\\delta$.\n",
    "- What if we are given two parameter values from $(\\gamma,\\delta,w)$? Can we point-identify the only left parameter?\n",
    "  - Again, as we are given an inequality, we can only find some bounds on the parameter we wish to identify.\n",
    "  - For example, if we are given $\\gamma=3$ and $\\delta=1$, then any $w$ that satisfies $\\frac{\\gamma}{\\gamma+\\delta}=\\frac{3}{4}>w$ can generate the same data.\n",
    "\n",
    "**Key Takeaway**: We need more than two periods to see any variation in our data to pin down our unknown parameters ($\\gamma$, $\\delta$, $w$) from data on agent's decisions."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 2.2. Optimal Decision\n",
    "Let's thoroughly analyze the **two-period model** to build intuition before tackling the general case.\n",
    "\n",
    "**Lifetime utility with T=2:**\n",
    "$$U = d_1\\cdot x_1 + (1-d_1)\\cdot w + \\beta[d_2\\cdot x_2 + (1-d_2)\\cdot w]$$\n",
    "\n",
    "State variables in each period: $(\\gamma_t, \\delta_t, w, t, \\beta)$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.2.1. Solving by Backward Induction -- Period 2: Terminal Decision\n",
    "\n",
    "At $t=2$ (the final period), the agent observes state $(\\gamma_2, \\delta_2)$ and solves:\n",
    "$$\n",
    "\\max_{d_2 \\in \\{0,1\\}} \\mathbb{E}_{\\xi_2}\\left[\\mathbb{E}_{x_2}\\left[d_2\\cdot\\mathbf{1}\\{x_2=1\\} + (1-d_2)\\cdot w \\mid \\xi_2\\right] \\mid \\gamma_2, \\delta_2\\right] =\n",
    "\\begin{cases}\n",
    "\\frac{\\gamma_2}{\\gamma_2+\\delta_2} & \\text{ if }d_2 = 1 \\\\\n",
    "w & \\text{ otherwise}\n",
    "\\end{cases} \\\\\n",
    "$$\n",
    "\n",
    "Since this is the **terminal period** (no future to consider), the decision is straightforward:\n",
    "- **Expected payoff from inventing:** $\\mathbb{E}[\\mathbf{1}\\{x_2=1\\} | \\gamma_2, \\delta_2] = \\frac{\\gamma_2}{\\gamma_2 + \\delta_2}$\n",
    "- **Payoff from outside option:** $w$\n",
    "\n",
    "**Decision rule at t=2:**\n",
    "$$d_2(\\gamma_2, \\delta_2, w) = \\mathbf{1}\\left\\{\\frac{\\gamma_2}{\\gamma_2 + \\delta_2} > w\\right\\}$$\n",
    "\n",
    "**Value function at t=2:**\n",
    "$$V(\\gamma_2, \\delta_2, w, 2, \\beta) = \\max\\left\\{\\frac{\\gamma_2}{\\gamma_2 + \\delta_2}, w\\right\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Solving by Backward Induction -- Period 1: Forward-Looking Decision\n",
    "\n",
    "At $t=1$, the agent must consider:\n",
    "1. **Immediate payoff** (just like period 2)\n",
    "2. **Information value** of learning about $\\xi$ for use in period 2\n",
    "\n",
    "**Value function at t=1:**\n",
    "$$\n",
    "\\begin{align*}\n",
    "    V(\\gamma_1,\\delta_1,w,1,\\beta) = \\underset{d_{1}\\in\\{0,1\\}}{\\max}\\; \\mathbb{E}_{\\xi_1}\\Big[&\\mathbb{E}_{x_1}[d_1\\mathbf{1}\\{x_{1}=1\\}+(1-d_{1})w|\\xi_1\\big]\\Big|\\gamma_1,\\delta_1\\Big] \\\\\n",
    "    +\\beta \\Bigg(&d_1\\mathbb{E}_{\\xi_1}\\Big[\\mathbb{E}_{x_1}\\Big[\\mathbf{1}\\{x_1=0|\\gamma_1,\\delta_1\\}V(\\gamma_2,\\delta_2,w,2,\\beta)\\Big|\\xi_1\\Big]\\Big|\\gamma_1,\\delta_1\\Big]+\\\\\n",
    "                 &d_1\\mathbb{E}_{\\xi_1}\\Big[\\mathbb{E}_{x_1}\\Big[\\mathbf{1}\\{x_1=1|\\gamma_1,\\delta_1\\}V(\\gamma_2,\\delta_2,w,2,\\beta)\\Big|\\xi_1\\Big]\\Big|\\gamma_1,\\delta_1\\Big]+\\\\\n",
    "                 &(1-d_1)V(\\gamma_2,\\delta_2,w,2,\\beta)\\Bigg)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $(\\gamma_2, \\delta_2)$ are updated based on the outcome at $t=1$:\n",
    "$$\n",
    "(\\gamma_2, \\delta_2) = \\begin{cases}\n",
    "(\\gamma_1 + 1, \\delta_1) & \\text{if } x_1 = 1 \\text{ (success)} \\\\\n",
    "(\\gamma_1, \\delta_1 + 1) & \\text{if } x_1 = 0 \\text{ (failure)} \\\\\n",
    "(\\gamma_1, \\delta_1) & \\text{if } d_1 = 0 \\text{ (didn't invent)}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Mort Explicit Representation -- Comparing the Two Options at t=1\n",
    "\n",
    "Let's denote:\n",
    "- $p_1 = \\frac{\\gamma_1}{\\gamma_1 + \\delta_1}$ = current expected success probability\n",
    "- $p_1^s = \\frac{\\gamma_1 + 1}{\\gamma_1 + \\delta_1 + 1}$ = posterior mean after success\n",
    "- $p_1^f = \\frac{\\gamma_1}{\\gamma_1 + \\delta_1 + 1}$ = posterior mean after failure\n",
    "\n",
    "Note that $p_1^f < p_1 < p_1^s$ (failure lowers belief, success raises it).\n",
    "\n",
    "**Option A: Take outside option at t=1**\n",
    "$$V^0_1 = w + \\beta \\cdot V_2(\\gamma_1, \\delta_1)$$\n",
    "- Get wage $w$ now\n",
    "- At $t=2$, beliefs are unchanged $(\\gamma_1, \\delta_1)$\n",
    "\n",
    "**Option B: Invent at t=1**\n",
    "$$V^1_1 = p_1 + \\beta \\cdot [p_1 \\cdot V_2(\\gamma_1+1, \\delta_1) + (1-p_1) \\cdot V_2(\\gamma_1, \\delta_1+1)]$$\n",
    "- Get expected payoff $p_1$ now\n",
    "- With probability $p_1$: succeed, update to $(\\gamma_1+1, \\delta_1)$\n",
    "- With probability $1-p_1$: fail, update to $(\\gamma_1, \\delta_1+1)$\n",
    "- At $t=2$, make decision with updated beliefs\n",
    "\n",
    "**Key insight:** The invention option has an **information option value**. Even if $p_1 \\approx w$ (so immediate payoffs are similar), inventing might be better because it generates information that improves period 2 decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Closed-Form Analysis for T=2\n",
    "\n",
    "With $T=2$, we can derive explicit expressions. Let's assume everyone invents at $t=1$ (we'll verify this holds for our parameters).\n",
    "\n",
    "**The Quit Hazard at t=2:**\n",
    "\n",
    "The hazard $h_2$ is the fraction of agents (who invented at $t=1$) that quit at $t=2$, $Pr[d_2=0|d_1=1]$.\n",
    "\n",
    "Starting from $(\\gamma_1, \\delta_1)$ at $t=1$:\n",
    "- With probability $p_1 = \\frac{\\gamma_1}{\\gamma_1+\\delta_1}$: Success → update to $(\\gamma_1+1, \\delta_1)$\n",
    "  - At $t=2$: Quit if $p_1^s \\equiv \\frac{\\gamma_1+1}{\\gamma_1+\\delta_1+1} < w$\n",
    "- With probability $1-p_1 = \\frac{\\delta_1}{\\gamma_1+\\delta_1}$: Failure → update to $(\\gamma_1, \\delta_1+1)$\n",
    "  - At $t=2$: Quit if $p_1^f \\equiv \\frac{\\gamma_1}{\\gamma_1+\\delta_1+1} < w$\n",
    "\n",
    "**Hazard formula:**\n",
    "$$h_2(w) = p_1 \\cdot \\mathbf{1}\\left\\{p_1^s < w\\right\\} + (1-p_1) \\cdot \\mathbf{1}\\left\\{p_1^f < w\\right\\}$$\n",
    "\n",
    "This is a **step function** with three regions:\n",
    "\n",
    "1. **Region 1:** $w \\leq p_1^f$ → Both success and failure types continue → $h_2 = 0$\n",
    "2. **Region 2:** $p_1^f < w < p_1^s$ → Only failures quit → $h_2 = 1-p_1 = \\frac{\\delta_1}{\\gamma_1+\\delta_1}$\n",
    "3. **Region 3:** $w \\geq p_1^s$ → Everyone quits → $h_2 = 1$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T04:52:57.255806Z",
     "iopub.status.busy": "2025-10-29T04:52:57.255739Z",
     "iopub.status.idle": "2025-10-29T04:52:57.258424Z",
     "shell.execute_reply": "2025-10-29T04:52:57.258004Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:46.980039Z",
     "start_time": "2025-11-02T04:22:46.977138Z"
    }
   },
   "source": [
    "# Compute T=2 threshold values\n",
    "def compute_t2_thresholds(gamma, delta):\n",
    "    \"\"\"Compute the threshold wages for T=2 model\"\"\"\n",
    "    p1 = gamma / (gamma + delta)\n",
    "    p1_fail = gamma / (gamma + delta + 1)\n",
    "    p1_succ = (gamma + 1) / (gamma + delta + 1)\n",
    "    h2_middle = delta / (gamma + delta)\n",
    "    \n",
    "    return {\n",
    "        'p1': p1,\n",
    "        'p1_fail': p1_fail,\n",
    "        'p1_succ': p1_succ,\n",
    "        'h2_middle': h2_middle\n",
    "    }\n",
    "\n",
    "# Compute for default parameters\n",
    "thresholds = compute_t2_thresholds(gamma_0, delta_0)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"T=2 THEORETICAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPrior: Beta({gamma_0}, {delta_0})\")\n",
    "print(f\"  Prior mean: $p_1$ = {thresholds['p1']:.4f}\")\n",
    "print(f\"\\nPosterior means at t=2:\")\n",
    "print(f\"  After failure: $p_1^f$ = {thresholds['p1_fail']:.4f}\")\n",
    "print(f\"  After success: $p_1^s$ = {thresholds['p1_succ']:.4f}\")\n",
    "print(f\"\\nHazard function $h_2(w)$:\")\n",
    "print(f\"  Region 1: w ≤ {thresholds['p1_fail']:.4f}  →  $h_2$ = 0.0000 (no one quits)\")\n",
    "print(f\"  Region 2: {thresholds['p1_fail']:.4f} < w < {thresholds['p1_succ']:.4f}  →  $h_2$ = {thresholds['h2_middle']:.4f} (failures quit)\")\n",
    "print(f\"  Region 3: w ≥ {thresholds['p1_succ']:.4f}  →  $h_2$ = 1.0000 (everyone quits)\")\n",
    "print(f\"\\nWith w_true = {w_true}:\")\n",
    "print(f\"  w is in Region 2: {thresholds['p1_fail']:.4f} < {w_true} < {thresholds['p1_succ']:.4f}\")\n",
    "print(f\"  Expected $h_2$ ≈ {thresholds['h2_middle']:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "T=2 THEORETICAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Prior: Beta(3.0, 2.0)\n",
      "  Prior mean: $p_1$ = 0.6000\n",
      "\n",
      "Posterior means at t=2:\n",
      "  After failure: $p_1^f$ = 0.5000\n",
      "  After success: $p_1^s$ = 0.6667\n",
      "\n",
      "Hazard function $h_2(w)$:\n",
      "  Region 1: w ≤ 0.5000  →  $h_2$ = 0.0000 (no one quits)\n",
      "  Region 2: 0.5000 < w < 0.6667  →  $h_2$ = 0.4000 (failures quit)\n",
      "  Region 3: w ≥ 0.6667  →  $h_2$ = 1.0000 (everyone quits)\n",
      "\n",
      "With w_true = 0.55:\n",
      "  w is in Region 2: 0.5000 < 0.55 < 0.6667\n",
      "  Expected $h_2$ ≈ 0.4000\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T04:52:57.259338Z",
     "iopub.status.busy": "2025-10-29T04:52:57.259280Z",
     "iopub.status.idle": "2025-10-29T04:52:57.322195Z",
     "shell.execute_reply": "2025-10-29T04:52:57.321819Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:47.071902Z",
     "start_time": "2025-11-02T04:22:46.984092Z"
    }
   },
   "source": [
    "# Visualize the step function\n",
    "def plot_h2_step_function(gamma, delta, w_true_val=None):\n",
    "    \"\"\"Plot the theoretical step function h2(w)\"\"\"\n",
    "    th = compute_t2_thresholds(gamma, delta)\n",
    "    \n",
    "    w_grid = np.linspace(0, 1, 1000)\n",
    "    h2_theory = np.zeros_like(w_grid)\n",
    "    \n",
    "    # Step function\n",
    "    h2_theory[w_grid <= th['p1_fail']] = 0.0\n",
    "    h2_theory[(w_grid > th['p1_fail']) & (w_grid < th['p1_succ'])] = th['h2_middle']\n",
    "    h2_theory[w_grid >= th['p1_succ']] = 1.0\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(w_grid, h2_theory, 'b-', linewidth=2.5, label='$h_2(w)$')\n",
    "    ax.axvline(th['p1_fail'], color='red', linestyle='--', alpha=0.7, \n",
    "               label=f\"$p_1^f$ = {th['p1_fail']:.3f}\")\n",
    "    ax.axvline(th['p1_succ'], color='green', linestyle='--', alpha=0.7, \n",
    "               label=f\"$p_1^s$ = {th['p1_succ']:.3f}\")\n",
    "    \n",
    "    if w_true_val:\n",
    "        ax.axvline(w_true_val, color='orange', linestyle=':', linewidth=2, \n",
    "                   label=f'w = {w_true_val}')\n",
    "    \n",
    "    ax.set_xlabel('Outside Wage (w)', fontsize=12)\n",
    "    ax.set_ylabel('Quit Hazard $h_2$', fontsize=12)\n",
    "    ax.set_title(f'T=2: Step Function $h_2(w)$ [Beta({gamma},{delta})]',\n",
    "                 fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_h2_step_function(gamma_0, delta_0, w_true)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.3. Identification\n",
    "\n",
    "**The Fundamental Problem:**\n",
    "\n",
    "With $T=2$, we observe a single hazard $h_2$. From the step function above:\n",
    "- If $h_2 \\approx 0$: We know $w \\leq p_1^f$, but not the exact value\n",
    "- If $h_2 \\approx \\frac{\\delta_1}{\\gamma_1+\\delta_1}$: We know $p_1^f < w < p_1^s$ (an interval!)\n",
    "- If $h_2 \\approx 1$: We know $w \\geq p_1^s$, but not the exact value\n",
    "\n",
    "This is **set identification**, not point identification.\n",
    "\n",
    "**Why does this happen?**\n",
    "- The hazard $h_2(w)$ is constant within each region\n",
    "- Fisher information: $I(w) \\propto [h_2'(w)]^2 = 0$ almost everywhere\n",
    "- Standard MLE asymptotics break down\n",
    "\n",
    "**What helps identification:**\n",
    "1. Longer time horizons ($T > 2$) give more variation\n",
    "2. Additional data on outcomes (not just quit decisions)\n",
    "3. Known values of some parameters $(\\gamma_1, \\delta_1, \\beta)$\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.3.1. Question 1\n",
    "\n",
    "Prove that it is never optimal to do something else in any period $t$ and invent in period $t + 1$. Thus, the optimal career strategy is to spend no periods inventing, or spend one or more periods inventing at the beginning of your career."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "WRITE YOUR ANSWER HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.3.2. Question 2\n",
    "\n",
    "Let's consider $T = 2$ case. Can you explain why not all the parameters in your model are identified from the solution to the quit rates without imposing restrictions (such as knowing the values of some of the parameters)?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "WRITE YOUR ANSWER HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.4. Data / Simulation\n",
    "\n",
    "Let's verify our theoretical predictions by simulation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T04:52:57.323152Z",
     "iopub.status.busy": "2025-10-29T04:52:57.323088Z",
     "iopub.status.idle": "2025-10-29T04:52:57.327849Z",
     "shell.execute_reply": "2025-10-29T04:52:57.327526Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:47.082905Z",
     "start_time": "2025-11-02T04:22:47.079097Z"
    }
   },
   "source": [
    "# Simple T=2 simulation\n",
    "def simulate_t2_simple(gamma, delta, w, N, seed=0):\n",
    "    \"\"\"\n",
    "    Simulate T=2 model and compute h2.\n",
    "    Assumes everyone invents at t=1.\n",
    "    \"\"\"\n",
    "    rng_local = np.random.default_rng(seed)\n",
    "    \n",
    "    # Draw abilities (fixed for each agent)\n",
    "    θ = rng_local.beta(gamma, delta, size=N)\n",
    "    \n",
    "    # Period 1: Everyone invents, observe outcome\n",
    "    x1 = rng_local.binomial(1, θ)\n",
    "    \n",
    "    # Update beliefs based on x1\n",
    "    gamma_2 = gamma + x1\n",
    "    delta_2 = delta + (1 - x1)\n",
    "    \n",
    "    # Period 2: Decide based on updated beliefs\n",
    "    p2 = gamma_2 / (gamma_2 + delta_2)\n",
    "    d2 = (p2 >= w).astype(int)\n",
    "    \n",
    "    # Hazard = fraction who quit (d2=0)\n",
    "    h2_simulated = 1 - d2.mean()\n",
    "    \n",
    "    return h2_simulated, d2\n",
    "\n",
    "# Simulate\n",
    "h2_sim, d2_data = simulate_t2_simple(gamma_0, delta_0, w_true, N_agents_default, seed=123)\n",
    "\n",
    "print(\"\\nT=2 Simulation Results:\")\n",
    "print(f\"  Theoretical h₂ = {thresholds['h2_middle']:.4f}\")\n",
    "print(f\"  Simulated h₂   = {h2_sim:.4f}\")\n",
    "print(f\"  Difference     = {abs(h2_sim - thresholds['h2_middle']):.4f}\")\n",
    "print(f\"  N = {N_agents_default:,} agents\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T=2 Simulation Results:\n",
      "  Theoretical h₂ = 0.4000\n",
      "  Simulated h₂   = 0.4015\n",
      "  Difference     = 0.0015\n",
      "  N = 20,000 agents\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.5. Economic Interpretation\n",
    "\n",
    "**What have we learned?**\n",
    "\n",
    "1. **Learning creates heterogeneity:** Even though all agents start with the same beliefs $(\\gamma_0, \\delta_0)$, after one period they have different beliefs depending on their experience.\n",
    "\n",
    "2. **Selection matters:** At $t=2$, we observe only those with favorable beliefs continuing to invent. Those who failed at $t=1$ have lower beliefs and are more likely to quit.\n",
    "\n",
    "3. **Limited information from short panels:** With just $T=2$ periods, we can't precisely pin down parameters. The hazard is too coarse.\n",
    "\n",
    "4. **Option value of experimentation:** The decision to invent at $t=1$ isn't just about immediate payoff $p_1$ vs $w$. It's also about learning $\\xi$ to make a better decision at $t=2$.\n",
    "\n",
    "**Extension to longer horizons:**\n",
    "With $T > 2$, we get:\n",
    "- More periods of learning and selection\n",
    "- Richer variation in hazards $h_2, h_3, \\ldots, h_T$\n",
    "- Better parameter identification\n",
    "- More complex dynamics (beliefs evolve along different paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. [General T Periods](#general)<a name=\"general\"></a>\n",
    "\n",
    "Now that we understand the T=2 case thoroughly, let's implement the general solution for any $T$.\n",
    "\n",
    "### 3.1. Backward Induction Algorithm with Forward Time Indexing\n",
    "\n",
    "The key insight from T=2 generalizes: we solve **backward** from $t=T$ to $t=1$.\n",
    "- $t=1, 2, \\ldots, T$ represents chronological time (later periods have higher indices)\n",
    "- $V_t(\\gamma, \\delta)$ = value function at period $t$ (with $T-t+1$ periods remaining)\n",
    "- $V_{t+1}(\\gamma, \\delta)$ = continuation value in the **next period** chronologically\n",
    "\n",
    "**(1) State space:** $(\\gamma, \\delta, t)$ where\n",
    "- $\\gamma, \\delta$: Current Beta parameters (belief state)\n",
    "- $t$: Current period (1, 2, ..., T)\n",
    "\n",
    "**(2) Bellman equation for $t = 1, \\ldots, T$:**\n",
    "$$V_t(\\gamma, \\delta) = \\max\\{V_t^0(\\gamma, \\delta), V_t^1(\\gamma, \\delta)\\}$$\n",
    "\n",
    "where:\n",
    "- **Outside option:** $V_t^0(\\gamma, \\delta) = w + \\beta \\cdot V_{t+1}(\\gamma, \\delta)$\n",
    "- **Invention:** $V_t^1(\\gamma, \\delta) = p + \\beta \\cdot [p \\cdot V_{t+1}(\\gamma+1, \\delta) + (1-p) \\cdot V_{t+1}(\\gamma, \\delta+1)]$\n",
    "\n",
    "with $p = \\frac{\\gamma}{\\gamma+\\delta}$.\n",
    "\n",
    "**Terminal condition:** $V_{T}(\\gamma, \\delta) = 0$ for all $(\\gamma, \\delta)$.\n",
    "\n",
    "**(3) Policy function:**\n",
    "$$d_t^*(\\gamma, \\delta) = \\mathbf{1}\\{V_t^1(\\gamma, \\delta) \\geq V_t^0(\\gamma, \\delta)\\}$$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2. Unified Simulator Class\n\nWe now create a **unified class** that encapsulates all model components:\n- Prior beliefs and utility parameters\n- Utility functions (immediate payoffs)\n- Value functions (from backward induction)\n- Decision rules (policies)\n- Agent simulation and hazard computation\n\nThis makes the relationships between components explicit and easier to understand.\n\n#### Function-Component Mapping\n\nThe `BayesianLearningModel` class is organized into 6 logical sections:\n\n**1. UTILITY FUNCTIONS (Section 1)**\n- `expected_success_prob(gamma, delta)`: Computes $\\mathbb{E}[\\xi|\\gamma,\\delta] = \\frac{\\gamma}{\\gamma+\\delta}$\n- `immediate_payoff_invent(gamma, delta)`: Returns expected payoff from inventing this period\n- `immediate_payoff_outside()`: Returns certain payoff $w$ from outside option\n\n**2. BELIEF UPDATING (Section 2)**\n- `update_beliefs(gamma, delta, outcome)`: Implements Beta-Bernoulli conjugacy\n  - Success (outcome=1): $(\\gamma, \\delta) \\rightarrow (\\gamma+1, \\delta)$\n  - Failure (outcome=0): $(\\gamma, \\delta) \\rightarrow (\\gamma, \\delta+1)$\n\n**3. VALUE FUNCTIONS (Section 3)**\n- `solve()`: Main solver using backward induction to fill `self.V` and `self.D` dictionaries\n  - Creates value function grid `V[t][ia, ib]` for each period $t$\n  - Creates decision rule grid `D[t][ia, ib]` for each period $t$\n- `value_outside_option(gamma, delta, t)`: Computes $V^0_t = w + \\beta V_{t+1}(\\gamma, \\delta)$\n- `value_invent(gamma, delta, t)`: Computes $V^1_t = p + \\beta[pV_{t+1}(\\gamma+1,\\delta) + (1-p)V_{t+1}(\\gamma,\\delta+1)]$\n\n**4. DECISION RULES (Section 4)**\n- `get_decision(gamma, delta, t)`: Returns optimal choice (0 or 1) from solved policy\n- `get_value(gamma, delta, t)`: Returns value function at given state\n\n**5. AGENT SIMULATION (Section 5)**\n- `simulate(N, seed, return_paths)`: Forward simulates N agents through T periods\n  - Draws abilities $\\xi_i \\sim Beta(\\gamma_0, \\delta_0)$ for each agent\n  - Follows optimal policies `D[t]` at each period\n  - Computes hazard rates $h_t$ = fraction quitting at period $t$\n  - Optionally returns full decision/outcome paths\n\n**6. SUMMARY AND DIAGNOSTICS (Section 6)**\n- `summary()`: Prints model parameters and solution status\n- `get_hazard_at_t(t)`, `get_value_at_t(t)`, `get_decision_at_t(t)`: Accessor methods\n\n**Usage Flow:**\n1. Create `ModelParameters` with $(\\gamma_0, \\delta_0, w, \\beta, T)$\n2. Instantiate `BayesianLearningModel(params)`\n3. Call `.solve()` to run backward induction (fills V and D)\n4. Call `.simulate(N)` to generate data and compute hazards\n5. Use `.summary()` and accessor methods to inspect results"
  },
  {
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T04:52:57.239236Z",
     "iopub.status.busy": "2025-10-29T04:52:57.239157Z",
     "iopub.status.idle": "2025-10-29T04:52:57.249066Z",
     "shell.execute_reply": "2025-10-29T04:52:57.248727Z"
    },
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:47.100601Z",
     "start_time": "2025-11-02T04:22:47.086754Z"
    }
   },
   "cell_type": "code",
   "source": "# ============================================================================\n# Unified Bayesian Learning Model\n# ============================================================================\n\n@dataclass\nclass ModelParameters:\n    \"\"\"Container for all model parameters\"\"\"\n    gamma_0: float  # Prior successes\n    delta_0: float  # Prior failures\n    w: float        # Outside wage\n    beta: float     # Discount factor\n    T: int          # Time horizon\n\n    def prior_mean(self) -> float:\n        \"\"\"Expected ability under prior\"\"\"\n        return self.gamma_0 / (self.gamma_0 + self.delta_0)\n\n    def __repr__(self):\n        return (f\"ModelParameters(gamma_0={self.gamma_0}, delta_0={self.delta_0}, \"\n                f\"w={self.w}, beta={self.beta}, T={self.T})\")\n\n\nclass BayesianLearningModel:\n    \"\"\"\n    Unified class for Bayesian learning career choice model.\n    \n    USES FORWARD TIME INDEXING: t=1,2,...,T where higher t = later chronological period\n    V[t] = value function at period t\n    V[t+1] = continuation value in next period\n\n    Components:\n    -----------\n    1. Parameters: gamma_0, delta_0, w, beta, T\n    2. Utility functions: immediate payoffs from each choice\n    3. Belief updating: Beta-Bernoulli conjugacy\n    4. Value functions: Solved via backward induction\n    5. Decision rules: Optimal policies at each state\n    6. Simulation: Generate panel data and compute hazards\n    7. Visualization: Plot value/policy functions and hazard rates\n\n    Usage:\n    ------\n    >>> params = ModelParameters(gamma_0=3, delta_0=2, w=0.55, beta=0.96, T=5)\n    >>> model = BayesianLearningModel(params)\n    >>> model.solve()\n    >>> hazards = model.simulate(N=10000, seed=42)\n    >>> model.plot_value_and_policy(t=1)\n    >>> model.plot_hazards()\n    \"\"\"\n\n    def __init__(self, params: ModelParameters):\n        self.params = params\n        self.V = None  # Value functions\n        self.D = None  # Decision rules\n        self.hazards = None     # Hazard rates\n        self.is_solved = False\n\n        # Grid setup\n        self.gamma_base = int(np.floor(params.gamma_0))\n        self.delta_base = int(np.floor(params.delta_0))\n        self.A = self.gamma_base + params.T + 1  # Grid size gamma dimension\n        self.B = self.delta_base + params.T + 1  # Grid size delta dimension\n\n    # ========================================================================\n    # 1. UTILITY FUNCTIONS\n    # ========================================================================\n\n    def expected_success_prob(self, gamma: float, delta: float) -> float:\n        \"\"\"\n        Expected success probability given beliefs.\n        E[xi | gamma, delta] = gamma / (gamma + delta)\n        \"\"\"\n        return gamma / (gamma + delta)\n\n    def immediate_payoff_invent(self, gamma: float, delta: float) -> float:\n        \"\"\"Immediate expected payoff from inventing\"\"\"\n        return self.expected_success_prob(gamma, delta)\n\n    def immediate_payoff_outside(self) -> float:\n        \"\"\"Immediate payoff from outside option\"\"\"\n        return self.params.w\n\n    # ========================================================================\n    # 2. BELIEF UPDATING\n    # ========================================================================\n\n    def update_beliefs(self, gamma: float, delta: float,\n                       outcome: int) -> Tuple[float, float]:\n        \"\"\"\n        Update Beta beliefs based on outcome.\n\n        Parameters:\n        -----------\n        gamma, delta : current belief parameters\n        outcome : 1 if success, 0 if failure\n\n        Returns:\n        --------\n        (gamma_new, delta_new) : updated beliefs\n        \"\"\"\n        if outcome == 1:  # Success\n            return gamma + 1, delta\n        else:  # Failure\n            return gamma, delta + 1\n\n    # ========================================================================\n    # 3. VALUE FUNCTIONS\n    # ========================================================================\n\n    def value_outside_option(self, gamma: float, delta: float,\n                            t: int) -> float:\n        \"\"\"\n        Value of choosing outside option at state (gamma, delta, t).\n        V^0_t = w + beta * V_{t+1}(gamma, delta)\n        \"\"\"\n        if not self.is_solved:\n            raise ValueError(\"Model not solved. Call solve() first.\")\n\n        ia = int(gamma) - self.gamma_base\n        ib = int(delta) - self.delta_base\n\n        # Immediate payoff + continuation value (no belief update)\n        immediate = self.immediate_payoff_outside()\n        continuation = self.params.beta * self.V[t+1][ia, ib] if t <= self.params.T else 0\n\n        return immediate + continuation\n\n    def value_invent(self, gamma: float, delta: float, t: int) -> float:\n        \"\"\"\n        Value of inventing at state (gamma, delta, t).\n        V^1_t = p + beta * [p*V_{t+1}(gamma+1,delta) + (1-p)*V_{t+1}(gamma,delta+1)]\n        \"\"\"\n        if not self.is_solved:\n            raise ValueError(\"Model not solved. Call solve() first.\")\n\n        ia = int(gamma) - self.gamma_base\n        ib = int(delta) - self.delta_base\n        p = self.expected_success_prob(gamma, delta)\n\n        # Immediate expected payoff\n        immediate = self.immediate_payoff_invent(gamma, delta)\n\n        # Continuation value (expected over outcomes)\n        if t <= self.params.T:\n            ia_succ = min(ia + 1, self.A - 1)\n            ib_fail = min(ib + 1, self.B - 1)\n            continuation = self.params.beta * (\n                p * self.V[t+1][ia_succ, ib] +      # Success path\n                (1-p) * self.V[t+1][ia, ib_fail]    # Failure path\n            )\n        else:  # t > T case\n            continuation = 0\n\n        return immediate + continuation\n\n    def solve(self, verbose: bool = False) -> 'BayesianLearningModel':\n        \"\"\"\n        Solve the dynamic programming problem via backward induction.\n        Uses FORWARD TIME INDEXING: t=1,2,...,T\n\n        Fills in:\n        - self.V: Dict[int, np.ndarray] - value functions V[t][gamma_idx, delta_idx]\n        - self.D: Dict[int, np.ndarray] - decision rules D[t][gamma_idx, delta_idx]\n\n        Returns self for method chaining.\n        \"\"\"\n        T = self.params.T\n        self.V = {T+1: np.zeros((self.A, self.B))}  # Terminal condition at T+1\n        self.D = {}\n\n        if verbose:\n            print(f\"Solving DP with T={T}, grid size ({self.A}, {self.B})...\")\n\n        # Backward induction from t=T down to t=1\n        for t in range(T, 0, -1):\n            V_t = np.zeros((self.A, self.B))\n            D_t = np.zeros((self.A, self.B), dtype=int)\n\n            for ia in range(self.A):\n                for ib in range(self.B):\n                    gamma = self.gamma_base + ia\n                    delta = self.delta_base + ib\n                    p = self.expected_success_prob(gamma, delta)\n\n                    # Compute value of each option\n                    V0 = self.params.w + self.params.beta * self.V[t+1][ia, ib]\n\n                    ia_plus = min(ia + 1, self.A - 1)\n                    ib_plus = min(ib + 1, self.B - 1)\n                    V1 = p + self.params.beta * (\n                        p * self.V[t+1][ia_plus, ib] +\n                        (1-p) * self.V[t+1][ia, ib_plus]\n                    )\n\n                    # Optimal decision\n                    if V1 >= V0:\n                        V_t[ia, ib] = V1\n                        D_t[ia, ib] = 1  # Invent\n                    else:\n                        V_t[ia, ib] = V0\n                        D_t[ia, ib] = 0  # Outside option\n\n            self.V[t] = V_t\n            self.D[t] = D_t\n\n            if verbose and t in [1, T//2, T]:\n                ia0 = int(np.floor(self.params.gamma_0)) - self.gamma_base\n                ib0 = int(np.floor(self.params.delta_0)) - self.delta_base\n                print(f\"  t={t}: V={V_t[ia0,ib0]:.4f}, D={D_t[ia0,ib0]}\")\n\n        self.is_solved = True\n        return self\n\n    # ========================================================================\n    # 4. DECISION RULES\n    # ========================================================================\n\n    def get_decision(self, gamma: float, delta: float, t: int) -> int:\n        \"\"\"\n        Get optimal decision at state (gamma, delta, t).\n\n        Returns:\n        --------\n        0 for outside option, 1 for invent\n        \"\"\"\n        if not self.is_solved:\n            raise ValueError(\"Model not solved. Call solve() first.\")\n\n        ia = int(gamma) - self.gamma_base\n        ib = int(delta) - self.delta_base\n        return self.D[t][ia, ib]\n\n    def get_value(self, gamma: float, delta: float, t: int) -> float:\n        \"\"\"Get value function at state (gamma, delta, t)\"\"\"\n        if not self.is_solved:\n            raise ValueError(\"Model not solved. Call solve() first.\")\n\n        ia = int(gamma) - self.gamma_base\n        ib = int(delta) - self.delta_base\n        return self.V[t][ia, ib]\n\n    # ========================================================================\n    # 5. AGENT SIMULATION\n    # ========================================================================\n\n    def simulate(self, N: int, seed: int = 0,\n                return_paths: bool = False) -> Dict[int, float]:\n        \"\"\"\n        Simulate N agents through the model.\n        Uses FORWARD TIME: t=1,2,...,T\n\n        Parameters:\n        -----------\n        N : int\n            Number of agents to simulate\n        seed : int\n            Random seed\n        return_paths : bool\n            If True, return full paths in addition to hazards\n\n        Returns:\n        --------\n        hazards : Dict[int, float]\n            Quit hazards by period h_t = P(quit at t | active at t)\n\n        If return_paths=True, also returns:\n        paths : Dict with arrays of agent histories\n        \"\"\"\n        if not self.is_solved:\n            raise ValueError(\"Model not solved. Call solve() first.\")\n\n        rng_local = np.random.default_rng(seed)\n        T = self.params.T\n\n        # Draw abilities (fixed for each agent)\n        xi = rng_local.beta(self.params.gamma_0, self.params.delta_0, size=N)\n\n        # Initialize belief states (index form)\n        ia = np.full(N, int(np.floor(self.params.gamma_0)) - self.gamma_base, dtype=int)\n        ib = np.full(N, int(np.floor(self.params.delta_0)) - self.delta_base, dtype=int)\n\n        # Track active agents\n        active = np.ones(N, dtype=bool)\n\n        # Storage\n        self.hazards = {}\n        paths = {'decisions': [], 'outcomes': [], 'beliefs': []} if return_paths else None\n\n        # Forward simulation: t=1,2,...,T\n        for t in range(1, T + 1):\n            # Get decisions from policy\n            D_t = self.D[t]\n            decisions = D_t[ia, ib]\n\n            # Compute hazard\n            at_risk = active.copy()\n            if at_risk.sum() > 0:\n                quits = ((decisions == 0) & at_risk)\n                self.hazards[t] = quits.sum() / at_risk.sum()\n            else:\n                self.hazards[t] = np.nan\n\n            # Update active status\n            inventors = (decisions == 1) & at_risk\n            active = inventors.copy()\n\n            # Draw outcomes for inventors and update beliefs\n            if inventors.any():\n                rng_period = np.random.default_rng(seed + t)\n                outcomes = rng_period.binomial(1, xi[inventors])\n\n                # Update belief indices\n                ia_inv = ia[inventors]\n                ib_inv = ib[inventors]\n                ia_new = np.minimum(ia_inv + outcomes, self.A - 1)\n                ib_new = np.minimum(ib_inv + (1 - outcomes), self.B - 1)\n                ia[inventors] = ia_new\n                ib[inventors] = ib_new\n\n                if return_paths:\n                    paths['decisions'].append(decisions.copy())\n                    paths['outcomes'].append(outcomes.copy())\n                    paths['beliefs'].append((ia.copy(), ib.copy()))\n\n        if return_paths:\n            return self.hazards, paths\n        return self.hazards\n\n    # ========================================================================\n    # 6. VISUALIZATION\n    # ========================================================================\n    \n    def plot_value_and_policy(self, t: int = 1, figsize: Tuple[int, int] = (14, 5)):\n        \"\"\"\n        Visualize policy function and value function at period t.\n        \n        Parameters:\n        -----------\n        t : int\n            Period to visualize (default: t=1, initial period)\n        figsize : tuple\n            Figure size\n        \"\"\"\n        if not self.is_solved:\n            raise ValueError(\"Model must be solved first. Call solve().\")\n        \n        # Get policy and value functions\n        D_t = self.D[t]\n        V_t = self.V[t]\n        \n        # Create meshgrid for plotting\n        gamma_vec = np.arange(self.gamma_base, self.gamma_base + self.A)\n        delta_vec = np.arange(self.delta_base, self.delta_base + self.B)\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n        \n        # Policy function heatmap\n        im1 = ax1.imshow(D_t, aspect='auto', origin='lower', cmap='RdYlGn', \n                          extent=[delta_vec[0]-0.5, delta_vec[-1]+0.5, \n                                 gamma_vec[0]-0.5, gamma_vec[-1]+0.5])\n        ax1.set_xlabel(r'$\\delta$ (Prior Failures)', fontsize=12)\n        ax1.set_ylabel(r'$\\gamma$ (Prior Successes)', fontsize=12)\n        ax1.set_title(f'Policy Function at t={t}\\n(1=Invent, 0=Outside)', \n                      fontsize=13, fontweight='bold')\n        cbar1 = plt.colorbar(im1, ax=ax1)\n        cbar1.set_label('Decision', fontsize=10)\n        \n        # Mark initial state\n        ia0 = int(np.floor(self.params.gamma_0)) - self.gamma_base\n        ib0 = int(np.floor(self.params.delta_0)) - self.delta_base\n        ax1.plot(self.params.delta_0, self.params.gamma_0, 'r*', \n                 markersize=15, label=f'Initial state ({self.params.gamma_0},{self.params.delta_0})')\n        ax1.legend(fontsize=9)\n        ax1.grid(True, alpha=0.3, linestyle='--')\n        \n        # Value function heatmap\n        im2 = ax2.imshow(V_t, aspect='auto', origin='lower', cmap='viridis',\n                          extent=[delta_vec[0]-0.5, delta_vec[-1]+0.5, \n                                 gamma_vec[0]-0.5, gamma_vec[-1]+0.5])\n        ax2.set_xlabel(r'$\\delta$ (Prior Failures)', fontsize=12)\n        ax2.set_ylabel(r'$\\gamma$ (Prior Successes)', fontsize=12)\n        ax2.set_title(f'Value Function at t={t}', fontsize=13, fontweight='bold')\n        cbar2 = plt.colorbar(im2, ax=ax2)\n        cbar2.set_label('Value', fontsize=10)\n        \n        # Mark initial state\n        ax2.plot(self.params.delta_0, self.params.gamma_0, 'r*', \n                 markersize=15, label=f'V={V_t[ia0, ib0]:.3f}')\n        ax2.legend(fontsize=9)\n        ax2.grid(True, alpha=0.3, linestyle='--')\n        \n        plt.tight_layout()\n        plt.show()\n\n    def plot_hazards(self, figsize: Tuple[int, int] = (14, 5)):\n        \"\"\"\n        Plot survival rates and hazard rates over time.\n        \n        Parameters:\n        -----------\n        figsize : tuple\n            Figure size\n        \"\"\"\n        if self.hazards is None:\n            raise ValueError(\"No hazards to plot. Call simulate() first.\")\n        \n        T = self.params.T\n        t_range = np.arange(1, T + 1)\n        h_vals = np.array([self.hazards[t] for t in t_range])\n        \n        # Compute survival rates: S_t = (1-h_1)(1-h_2)...(1-h_t)\n        survival = np.cumprod(1 - h_vals)\n        \n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n        \n        # Survival rate plot\n        ax1.plot(t_range, survival, 'b-o', linewidth=2, markersize=6)\n        ax1.set_xlabel('Period t', fontsize=12)\n        ax1.set_ylabel('Survival Rate', fontsize=12)\n        ax1.set_title('Fraction of Agents Still Inventing at t', \n                      fontsize=13, fontweight='bold')\n        ax1.grid(True, alpha=0.3)\n        ax1.set_ylim([-0.05, 1.05])\n        \n        # Hazard rate plot\n        ax2.plot(t_range, h_vals, 'r-s', linewidth=2, markersize=6)\n        ax2.set_xlabel('Period t', fontsize=12)\n        ax2.set_ylabel('Hazard Rate $h_t$', fontsize=12)\n        ax2.set_title(r'Quit Rate: $h_t = \\mathbb{P}(d_t=0 \\mid d_{t-1}=1)$', \n                      fontsize=13, fontweight='bold')\n        ax2.grid(True, alpha=0.3)\n        ax2.set_ylim([-0.05, 1.05])\n        \n        plt.tight_layout()\n        plt.show()\n\n    # ========================================================================\n    # 7. SUMMARY AND DIAGNOSTICS\n    # ========================================================================\n\n    def summary(self) -> str:\n        \"\"\"Return string summary of model\"\"\"\n        s = \"=\"*70 + \"\\n\"\n        s += \"BAYESIAN LEARNING MODEL SUMMARY\\n\"\n        s += \"=\"*70 + \"\\n\"\n        s += f\"\\nParameters:\\n\"\n        s += f\"  Prior: Beta({self.params.gamma_0}, {self.params.delta_0})\\n\"\n        s += f\"  Prior mean ability: {self.params.prior_mean():.4f}\\n\"\n        s += f\"  Outside wage w: {self.params.w}\\n\"\n        s += f\"  Discount factor beta: {self.params.beta}\\n\"\n        s += f\"  Time horizon T: {self.params.T}\\n\"\n        s += f\"\\nSolution status: {'SOLVED' if self.is_solved else 'NOT SOLVED'}\\n\"\n\n        if self.is_solved:\n            ia0 = int(np.floor(self.params.gamma_0)) - self.gamma_base\n            ib0 = int(np.floor(self.params.delta_0)) - self.delta_base\n            s += f\"\\nInitial state value V_1(gamma_0, delta_0) = {self.V[1][ia0, ib0]:.4f}\\n\"\n            s += f\"Initial decision D_1(gamma_0, delta_0) = {self.D[1][ia0, ib0]} \"\n            s += f\"({'INVENT' if self.D[1][ia0, ib0] == 1 else 'OUTSIDE'})\\n\"\n\n        s += \"=\"*70\n        return s",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3. Example Usage for T=2 Model\n\n**Connection to Section 2: Verifying Equivalence with Closed-Form Solution**\n\nIn Section 2, we derived the **closed-form solution** for the T=2 model analytically. Now we verify that our general `BayesianLearningModel` class (which works for any T) reproduces the same results when T=2.\n\n**Key equivalences to verify:**\n- Decision rules: Does everyone invent at t=1?\n- Hazard rate: Does $h_2$ match the theoretical formula $h_2 = \\frac{\\delta_0}{\\gamma_0+\\delta_0}$?\n- Value function: Does $V_1(\\gamma_0, \\delta_0)$ match the value we'd compute by hand?\n\nThis verification ensures our general algorithm is implemented correctly."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:47.107573Z",
     "start_time": "2025-11-02T04:22:47.103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# DEMONSTRATION: T=2 Model - Equivalence with Closed-Form Solution\n",
    "# ============================================================================\n",
    "\n",
    "# Create model with T=2 using parameters defined at the top\n",
    "params_t2 = ModelParameters(\n",
    "    gamma_0=gamma_0,\n",
    "    delta_0=delta_0,\n",
    "    w=w_true,\n",
    "    beta=beta,\n",
    "    T=2\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"T=2 MODEL: VERIFYING EQUIVALENCE WITH SECTION 2\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nParameters: γ={gamma_0}, δ={delta_0}, w={w_true}, β={beta}\")\n",
    "\n",
    "# Solve using general backward induction algorithm\n",
    "model_t2 = BayesianLearningModel(params_t2)\n",
    "model_t2.solve(verbose=False)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + model_t2.summary())\n",
    "\n",
    "# Compare with closed-form solution from Section 2\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: General Algorithm vs. Closed-Form\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Closed-form theoretical hazard\n",
    "h2_theory = delta_0 / (gamma_0 + delta_0)\n",
    "print(f\"\\nTheoretical h₂ (from Section 2.2.4): {h2_theory:.4f}\")\n",
    "\n",
    "# Simulate using general algorithm\n",
    "hazards_t2 = model_t2.simulate(N=N_agents_default, seed=seed_default)\n",
    "print(f\"Simulated h₂ (general algorithm):   {hazards_t2[2]:.4f}\")\n",
    "print(f\"Difference:                          {abs(hazards_t2[2] - h2_theory):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ The general algorithm reproduces the closed-form solution!\")\n",
    "print(\"=\"*80)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "T=2 MODEL: VERIFYING EQUIVALENCE WITH SECTION 2\n",
      "================================================================================\n",
      "\n",
      "Parameters: γ=3.0, δ=2.0, w=0.55, β=0.96\n",
      "\n",
      "======================================================================\n",
      "BAYESIAN LEARNING MODEL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Parameters:\n",
      "  Prior: Beta(3.0, 2.0)\n",
      "  Prior mean ability: 0.6000\n",
      "  Outside wage w: 0.55\n",
      "  Discount factor beta: 0.96\n",
      "  Time horizon T: 2\n",
      "\n",
      "Solution status: SOLVED\n",
      "\n",
      "Initial state value V_1(gamma_0, delta_0) = 1.1952\n",
      "Initial decision D_1(gamma_0, delta_0) = 1 (INVENT)\n",
      "======================================================================\n",
      "\n",
      "================================================================================\n",
      "COMPARISON: General Algorithm vs. Closed-Form\n",
      "================================================================================\n",
      "\n",
      "Theoretical h₂ (from Section 2.2.4): 0.4000\n",
      "Simulated h₂ (general algorithm):   0.4025\n",
      "Difference:                          0.0025\n",
      "\n",
      "================================================================================\n",
      "✓ The general algorithm reproduces the closed-form solution!\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 4. [Estimation and Inference](#estimation)<a name=\"estimation\"></a>\n",
    "\n",
    "Now we estimate structural parameters from simulated data using **maximum likelihood estimation**.\n",
    "\n",
    "For each candidate $w$:\n",
    "1. Solve DP with $(\\gamma_0, \\delta_0, w, \\beta, T)$\n",
    "2. Simulate $N_{sim}$ agents to compute model hazards $\\{\\hat{h}_2(w), \\ldots, \\hat{h}_T(w)\\}$\n",
    "3. Evaluate log-likelihood:\n",
    "$$\\ell(w) = \\sum_{t=2}^{T} \\left[k_t \\log(\\hat{h}_t(w)) + (n_t - k_t) \\log(1 - \\hat{h}_t(w))\\right]$$\n",
    "\n",
    "**MLE:** $\\hat{w} = \\arg\\max_w \\ell(w)$"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:47.119457Z",
     "start_time": "2025-11-02T04:22:47.112115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# GENERAL MLE ESTIMATOR CLASS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class MLEResult:\n",
    "    \"\"\"Container for MLE estimation results\"\"\"\n",
    "    param_name: str\n",
    "    estimate: float\n",
    "    std_error: float\n",
    "    true_value: float = None\n",
    "    sample_size: int = None\n",
    "    loglik: float = None\n",
    "    convergence: bool = True\n",
    "    method: str = \"Unknown\"  # \"Closed-form\" or \"Numerical\"\n",
    "\n",
    "    def z_stat(self) -> float:\n",
    "        \"\"\"Compute z-statistic\"\"\"\n",
    "        return self.estimate / self.std_error if self.std_error > 0 else np.nan\n",
    "\n",
    "    def ci_95(self) -> Tuple[float, float]:\n",
    "        \"\"\"Compute 95% confidence interval\"\"\"\n",
    "        return (self.estimate - 1.96*self.std_error,\n",
    "                self.estimate + 1.96*self.std_error)\n",
    "\n",
    "    def bias(self) -> float:\n",
    "        \"\"\"Compute bias if true value known\"\"\"\n",
    "        if self.true_value is not None:\n",
    "            return self.estimate - self.true_value\n",
    "        return np.nan\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = f\"MLE Result for {self.param_name} ({self.method}):\\n\"\n",
    "        s += f\"  Estimate: {self.estimate:.5f} (SE: {self.std_error:.5f})\\n\"\n",
    "        s += f\"  95% CI: [{self.ci_95()[0]:.5f}, {self.ci_95()[1]:.5f}]\\n\"\n",
    "        if self.true_value is not None:\n",
    "            s += f\"  True value: {self.true_value:.5f}\\n\"\n",
    "            s += f\"  Bias: {self.bias():.5f}\\n\"\n",
    "        if self.sample_size is not None:\n",
    "            s += f\"  Sample size: {self.sample_size}\\n\"\n",
    "        return s\n",
    "\n",
    "class MLEEstimator:\n",
    "    \"\"\"\n",
    "    General-purpose Maximum Likelihood Estimator.\n",
    "\n",
    "    Usage:\n",
    "    ------\n",
    "    1. Define a log-likelihood function: loglik(theta, data, **kwargs)\n",
    "    2. Instantiate: estimator = MLEEstimator(loglik_func, param_name=\"gamma\")\n",
    "    3. Estimate: result = estimator.estimate(data, initial_guess, kwargs)\n",
    "    4. (Optional) Estimate manually: result = estimator.estimate_closed_form(data, closed_form_func, se_func)\n",
    "\n",
    "    The estimator supports both numerical optimization and closed-form solutions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loglik_func=None, param_name: str = \"theta\"):\n",
    "        \"\"\"\n",
    "        Initialize MLE estimator.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        loglik_func : callable\n",
    "            Log-likelihood function with signature: loglik(theta, data, **kwargs)\n",
    "            Should return NEGATIVE log-likelihood (for minimization)\n",
    "        param_name : str\n",
    "            Name of parameter being estimated\n",
    "        \"\"\"\n",
    "        self.loglik_func = loglik_func\n",
    "        self.param_name = param_name\n",
    "\n",
    "    def estimate(self, data, initial_guess: float,\n",
    "                 bounds: Tuple[float, float] = None,\n",
    "                 true_value: float = None,\n",
    "                 **kwargs) -> MLEResult:\n",
    "        \"\"\"\n",
    "        Estimate parameter via numerical optimization.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array-like\n",
    "            Observed data\n",
    "        initial_guess : float\n",
    "            Starting value for optimization\n",
    "        bounds : tuple\n",
    "            Optional (min, max) bounds for parameter\n",
    "        true_value : float\n",
    "            True parameter value (for simulation studies)\n",
    "        **kwargs : dict\n",
    "            Additional arguments passed to log-likelihood function\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        result : MLEResult\n",
    "            Estimation results with standard errors\n",
    "        \"\"\"\n",
    "        if self.loglik_func is None:\n",
    "            raise ValueError(\"Must provide log-likelihood function\")\n",
    "\n",
    "        # Define objective (negative log-likelihood)\n",
    "        def objective(theta_vec):\n",
    "            return self.loglik_func(theta_vec[0], data, **kwargs)\n",
    "\n",
    "        # Optimize\n",
    "        if bounds is not None:\n",
    "            res = opt.minimize(objective, [initial_guess], method='L-BFGS-B',\n",
    "                              bounds=[bounds])\n",
    "        else:\n",
    "            res = opt.minimize(objective, [initial_guess], method='BFGS')\n",
    "\n",
    "        theta_hat = res.x[0]\n",
    "        convergence = res.success\n",
    "\n",
    "        # Compute standard error via Hessian (Information Matrix)\n",
    "        # SE = sqrt(diag(inv(Hessian))) where Hessian = d²(-loglik)/dθ²\n",
    "        try:\n",
    "            # Numerical Hessian approximation using centered finite differences\n",
    "            eps = 1e-5 * max(abs(theta_hat), 1.0)  # Adaptive step size\n",
    "\n",
    "            # Compute f''(x) ≈ [f(x+h) - 2f(x) + f(x-h)] / h²\n",
    "            f_plus = objective([theta_hat + eps])\n",
    "            f_minus = objective([theta_hat - eps])\n",
    "            f_center = objective([theta_hat])\n",
    "\n",
    "            hess = (f_plus - 2*f_center + f_minus) / (eps**2)\n",
    "\n",
    "            # Standard error from inverse of Hessian\n",
    "            if hess > 1e-10:  # Positive definite check\n",
    "                se = np.sqrt(1.0 / hess)\n",
    "            else:\n",
    "                # Fallback: try using the Hessian from optimizer if available\n",
    "                if hasattr(res, 'hess_inv') and res.hess_inv is not None:\n",
    "                    # BFGS provides inverse Hessian approximation\n",
    "                    if isinstance(res.hess_inv, np.ndarray):\n",
    "                        var = res.hess_inv[0, 0] if res.hess_inv.ndim > 1 else res.hess_inv[0]\n",
    "                        se = np.sqrt(var) if var > 0 else np.nan\n",
    "                    else:\n",
    "                        se = np.nan\n",
    "                else:\n",
    "                    se = np.nan\n",
    "        except Exception as e:\n",
    "            # Last resort: try to extract from optimizer\n",
    "            if hasattr(res, 'hess_inv') and res.hess_inv is not None:\n",
    "                try:\n",
    "                    if isinstance(res.hess_inv, np.ndarray):\n",
    "                        var = res.hess_inv[0, 0] if res.hess_inv.ndim > 1 else res.hess_inv[0]\n",
    "                        se = np.sqrt(var) if var > 0 else np.nan\n",
    "                    else:\n",
    "                        se = np.nan\n",
    "                except:\n",
    "                    se = np.nan\n",
    "            else:\n",
    "                se = np.nan\n",
    "\n",
    "        return MLEResult(\n",
    "            param_name=self.param_name,\n",
    "            estimate=theta_hat,\n",
    "            std_error=se,\n",
    "            true_value=true_value,\n",
    "            sample_size=len(data) if hasattr(data, '__len__') else None,\n",
    "            loglik=-res.fun,\n",
    "            convergence=convergence,\n",
    "            method=\"Numerical (Hessian)\"\n",
    "        )\n",
    "\n",
    "    def estimate_closed_form(self, data,\n",
    "                            estimator_func,\n",
    "                            se_func,\n",
    "                            true_value: float = None,\n",
    "                            **kwargs) -> MLEResult:\n",
    "        \"\"\"\n",
    "        Estimate using closed-form formulas.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : array-like\n",
    "            Observed data\n",
    "        estimator_func : callable\n",
    "            Function that computes estimate: estimator_func(data, **kwargs) -> float\n",
    "        se_func : callable\n",
    "            Function that computes standard error: se_func(estimate, data, **kwargs) -> float\n",
    "        true_value : float\n",
    "            True parameter value (for simulation studies)\n",
    "        **kwargs : dict\n",
    "            Additional arguments\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        result : MLEResult\n",
    "            Estimation results\n",
    "        \"\"\"\n",
    "        theta_hat = estimator_func(data, **kwargs)\n",
    "        se = se_func(theta_hat, data, **kwargs)\n",
    "\n",
    "        return MLEResult(\n",
    "            param_name=self.param_name,\n",
    "            estimate=theta_hat,\n",
    "            std_error=se,\n",
    "            true_value=true_value,\n",
    "            sample_size=len(data) if hasattr(data, '__len__') else None,\n",
    "            method=\"Closed-form (Fisher Info)\"\n",
    "        )\n",
    "\n",
    "def print_mle_table(results: List[MLEResult], sample_sizes: List[int],\n",
    "                   title: str = None):\n",
    "    \"\"\"\n",
    "    Print formatted table of MLE results using pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : list of MLEResult\n",
    "        Results for different sample sizes\n",
    "    sample_sizes : list of int\n",
    "        Sample sizes used\n",
    "    title : str\n",
    "        Optional title for the table\n",
    "    \"\"\"\n",
    "    param_name = results[0].param_name\n",
    "    true_val = results[0].true_value\n",
    "    method = results[0].method\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'N': sample_sizes,\n",
    "        'Estimate': [r.estimate for r in results],\n",
    "        'Std Error': [r.std_error for r in results],\n",
    "        'z-stat': [r.z_stat() for r in results],\n",
    "    })\n",
    "\n",
    "    if true_val is not None:\n",
    "        df['Bias'] = [r.bias() for r in results]\n",
    "        df['|Bias|/SE'] = np.abs(df['Bias']) / df['Std Error']\n",
    "\n",
    "    # Print header\n",
    "    print(\"=\"*85)\n",
    "    if title:\n",
    "        print(title)\n",
    "    else:\n",
    "        print(f\"MLE ESTIMATION RESULTS: {param_name} ({method})\")\n",
    "    print(\"=\"*85)\n",
    "\n",
    "    if true_val is not None:\n",
    "        print(f\"True value: {param_name} = {true_val:.4f}\")\n",
    "        print(\"-\"*85)\n",
    "\n",
    "    # Print DataFrame with nice formatting\n",
    "    print(df.to_string(index=False, float_format=lambda x: f'{x:.5f}'))\n",
    "\n",
    "    print(\"=\"*85)\n",
    "\n",
    "    # Print interpretation notes\n",
    "    if true_val is not None:\n",
    "        print(\"\\nNotes:\")\n",
    "        print(\"  - Estimates should converge to true value as N increases\")\n",
    "        print(\"  - Standard errors should decrease proportional to 1/sqrt(N)\")\n",
    "        print(\"  - |Bias|/SE should be small (< 0.1) for unbiased estimation\")\n",
    "        print(\"=\"*85)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1. Example: Two-periods Model\n",
    "\n",
    "The discussion above motivates to construct the likelihood of $\\{d_{i2}\\}_{i=1}^{N}$ by:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}(\\gamma;\\{d_{i2}\\}_{i=1}^{N},\\delta)=\\prod_{i=1}^{N}\\left(\\frac{\\gamma}{\\gamma+\\delta}\\right)^{d_{i2}}\\left(\\frac{\\delta}{\\gamma+\\delta}\\right)^{1-d_{i2}}\n",
    "\\end{align*}\n",
    "$$\n",
    "The log-likelihood is\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\log\\mathcal{L}(\\gamma;\\{d_{i2}\\}_{i=1}^{N},\\delta)&=\\sum_{i=1}^{N}\\left(d_{i2}\\log\\left(\\frac{\\gamma}{\\gamma+\\delta}\\right)+(1-d_{i2})\\log\\left(\\frac{\\delta}{\\gamma+\\delta}\\right)\\right)\\\\\n",
    "    &=\\sum_{i=1}^{N}\\left(d_{i2}\\left(\\log\\gamma-\\log(\\gamma+\\delta)\\right)+(1-d_{i2})\\left(\\log\\delta-\\log(\\gamma+\\delta)\\right)\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's exploit that $\\{d_{i2}\\}_{i=1}^{N}$ are independent. The score function of a single observation $d_{i2}$ is\n",
    "$$\n",
    "\\begin{align*}\n",
    "    s(\\gamma;d_{i2},\\delta)=\\frac{\\partial}{\\partial\\gamma}\\log f(\\gamma;d_{i2},\\delta)=d_{i2}\\left(\\frac{1}{\\gamma}-\\frac{1}{\\gamma+\\delta}\\right)+(1-d_{i2})\\left(-\\frac{1}{\\gamma+\\delta}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "This gives us the derivative of the log-likelihood with respect to $\\gamma$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial\\gamma}\\log \\mathcal{L}(\\gamma;\\{d_{i2}\\}_{i=1}^{N},\\delta)=\\sum_{i=1}^{N}\\left(d_{i2}\\left(\\frac{1}{\\gamma}-\\frac{1}{\\gamma+\\delta}\\right)+(1-d_{i2})\\left(-\\frac{1}{\\gamma+\\delta}\\right)\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Setting above to zero and solve for $\\gamma$ will give us the estimate of $\\gamma$ via MLE:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\sum_{i=1}^{N}\\left(d_{i2}\\left(\\frac{1}{\\gamma}-\\frac{1}{\\gamma+\\delta}\\right)+(1-d_{i2})\\left(-\\frac{1}{\\gamma+\\delta}\\right)\\right) &= 0 \\\\\n",
    "    \\left(\\frac{1}{\\gamma}-\\frac{1}{\\gamma+\\delta}\\right)\\sum_{i=1}^{N}d_{i2} -\\frac{N}{\\gamma+\\delta} + \\frac{1}{\\gamma+\\delta}\\sum_{i=1}^{N}d_{i2}&= 0 \\\\\n",
    "    \\frac{1}{\\gamma}\\sum_{i=1}^{N}d_{i2} = \\frac{N}{\\gamma+\\delta}&\\\\\n",
    "    \\therefore \\hat{\\gamma} = \\frac{\\delta\\sum_{i=1}^{N}d_{i2}}{N-\\sum_{i=1}^{N}d_{i2}}&\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$\\{d_{i2}\\}_{i=1}^{N}$ are i.i.d. so I use the following theorem to compute the Fisher information:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    I_{N}(\\gamma)&=N\\cdot I(\\gamma)=-N\\cdot \\mathbb{E}_{\\gamma}\\left(\\frac{\\partial^2\\log f(\\gamma;d_{i2},\\delta)}{\\partial \\gamma^2}\\right)\\\\\n",
    "    &=-N\\cdot\\mathbb{E}_{\\gamma}\\left( d_{i2}\\left(-\\frac{1}{\\gamma^2}+\\frac{1}{(\\gamma+\\delta)^2}\\right)+(1-d_{i2})\\left(\\frac{1}{(\\gamma+\\delta)^2}\\right)\\right)\\\\\n",
    "    &=-N\\cdot\\left(\\left(-\\frac{1}{\\gamma^2}+\\frac{1}{(\\gamma+\\delta)^2}\\right)\\frac{\\gamma}{\\gamma+\\delta}+\\left(\\frac{1}{(\\gamma+\\delta)^2}\\right)\\frac{\\delta}{\\gamma+\\delta}\\right) \\\\\n",
    "    &=\\frac{N\\delta}{\\gamma(\\gamma+\\delta)^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "Substituting $\\gamma$ with $\\hat{\\gamma}$, we have asymptotic normality of the MLE as follows:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    &\\hat{se} = \\sqrt{1/I_N(\\hat{\\gamma})}\\\\\n",
    "    &\\frac{\\hat{\\gamma}-\\gamma}{\\hat{se}} \\xrightarrow{d} N(0,1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Following code shows the likelihood (`loglik_gamma_t2`) and the estimation in a closed form solution (`gamma_estimator_closed_form`)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:47.133121Z",
     "start_time": "2025-11-02T04:22:47.122054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# T=2 MODEL: LOG-LIKELIHOOD AND ESTIMATION\n",
    "# ============================================================================\n",
    "\n",
    "def loglik_gamma_t2(gamma: float, d_data: np.ndarray, delta: float) -> float:\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for gamma in T=2 model (delta known).\n",
    "    \n",
    "    From Section 4.1, the log-likelihood is:\n",
    "    log L(gamma) = sum_i [d_i2 * log(gamma/(gamma+delta)) + (1-d_i2) * log(delta/(gamma+delta))]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gamma : float\n",
    "        Parameter to estimate (prior successes)\n",
    "    d_data : np.ndarray\n",
    "        Decision data (N x 2), column 1 = t=2 decisions\n",
    "    delta : float\n",
    "        Known prior failures parameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    neg_loglik : float\n",
    "        Negative log-likelihood (for minimization)\n",
    "    \"\"\"\n",
    "    d2 = d_data[:, 1]  # Decisions at t=2\n",
    "    N = len(d2)\n",
    "    \n",
    "    # Avoid log(0) and division by zero\n",
    "    gamma = max(gamma, 1e-10)\n",
    "    p = gamma / (gamma + delta)\n",
    "    p = np.clip(p, 1e-10, 1 - 1e-10)\n",
    "    \n",
    "    # Log-likelihood\n",
    "    loglik = np.sum(d2 * np.log(p) + (1 - d2) * np.log(1 - p))\n",
    "    \n",
    "    return -loglik  # Return negative for minimization\n",
    "\n",
    "def gamma_estimator_closed_form(d_data: np.ndarray, delta: float) -> float:\n",
    "    \"\"\"\n",
    "    Closed-form MLE for gamma in T=2 model.\n",
    "    \n",
    "    From Section 4.1:\n",
    "    gamma_hat = delta * sum(d_i2) / (N - sum(d_i2))\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d_data : np.ndarray\n",
    "        Decision data (N x 2)\n",
    "    delta : float\n",
    "        Known prior failures parameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    gamma_hat : float\n",
    "        MLE estimate of gamma\n",
    "    \"\"\"\n",
    "    d2 = d_data[:, 1]\n",
    "    N = len(d2)\n",
    "    sum_d2 = np.sum(d2)\n",
    "    \n",
    "    if sum_d2 == N:  # All invent -> gamma -> infinity\n",
    "        return np.inf\n",
    "    if sum_d2 == 0:  # None invent -> gamma -> 0\n",
    "        return 1e-10\n",
    "    \n",
    "    gamma_hat = delta * sum_d2 / (N - sum_d2)\n",
    "    return gamma_hat\n",
    "\n",
    "def gamma_std_error(gamma_hat: float, d_data: np.ndarray, delta: float) -> float:\n",
    "    \"\"\"\n",
    "    Standard error for gamma MLE in T=2 model using Fisher Information.\n",
    "    \n",
    "    From Section 4.1, Fisher information:\n",
    "    I_N(gamma) = N * delta / [gamma * (gamma + delta)^2]\n",
    "    SE(gamma_hat) = sqrt(1 / I_N(gamma_hat))\n",
    "    \n",
    "    IMPORTANT: This is the ANALYTIC standard error derived from Fisher Information.\n",
    "    It is equivalent to the Hessian-based SE from numerical optimization (they both\n",
    "    estimate the same asymptotic variance), but computed differently:\n",
    "    \n",
    "    1. Fisher Information Method (this function):\n",
    "       - Uses the closed-form formula for I_N(gamma)\n",
    "       - Derived from E[d²log L / dgamma²]\n",
    "       - Requires knowing the functional form of the likelihood\n",
    "    \n",
    "    2. Hessian Method (in MLEEstimator.estimate):\n",
    "       - Numerically approximates d²log L / dgamma² at gamma_hat\n",
    "       - Works for any likelihood (no closed form needed)\n",
    "       - May have small numerical errors\n",
    "    \n",
    "    Both methods should give nearly identical results. The Fisher Information method\n",
    "    is preferred when available because it's exact (no numerical approximation).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gamma_hat : float\n",
    "        Estimated gamma\n",
    "    d_data : np.ndarray\n",
    "        Decision data\n",
    "    delta : float\n",
    "        Known delta\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    se : float\n",
    "        Standard error\n",
    "    \"\"\"\n",
    "    N = len(d_data)\n",
    "    I_N = N * delta / (gamma_hat * (gamma_hat + delta)**2)\n",
    "    se = np.sqrt(1 / I_N) if I_N > 0 else np.nan\n",
    "    return se\n",
    "\n",
    "# ============================================================================\n",
    "# ESTIMATION EXAMPLE: Estimating gamma with varying sample sizes\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*85)\n",
    "print(\"T=2 MODEL: ESTIMATING γ (δ KNOWN)\")\n",
    "print(\"=\"*85)\n",
    "\n",
    "gamma_true, delta_true = gamma_0, delta_0\n",
    "\n",
    "# Generate large dataset using parameters from top\n",
    "np.random.seed(seed_default)\n",
    "xi_full = np.random.beta(gamma_0, delta_0, size=N_agents_default)\n",
    "\n",
    "# Simulate T=2 decisions for all agents\n",
    "x1_full = np.random.binomial(1, xi_full)  # Period 1 outcomes\n",
    "gamma_2 = gamma_0 + x1_full\n",
    "delta_2 = delta_true + (1 - x1_full)\n",
    "p2_full = gamma_2 / (gamma_2 + delta_2)\n",
    "d2_full = (p2_full >= w_true).astype(int)  # Period 2 decisions\n",
    "\n",
    "# Create full decision matrix (assume all invent at t=1)\n",
    "d_data_full = np.column_stack([np.ones(N_agents_default), d2_full])\n",
    "\n",
    "# Sample sizes to try\n",
    "N_vec = [50, 500, 10000, 100000, 500000]\n",
    "\n",
    "# Storage for results\n",
    "results_closed_form = []\n",
    "results_optimizer = []\n",
    "\n",
    "print(f\"\\nTrue parameters: γ={gamma_true}, δ={delta_true}, w={w_true}\")\n",
    "print(f\"Theoretical h₂ = {delta_true/(gamma_true+delta_true):.4f}\")\n",
    "print(\"\\nEstimating γ for different sample sizes...\")\n",
    "print(\"-\"*85)\n",
    "\n",
    "# Estimate for each sample size\n",
    "for N in N_vec:\n",
    "    d_data_N = d_data_full[:N, :]\n",
    "    h2_empirical = 1 - d_data_N[:, 1].mean()\n",
    "    \n",
    "    # Method 1: Closed-form (Fisher Information for SE)\n",
    "    estimator_cf = MLEEstimator(param_name=\"γ\")\n",
    "    result_cf = estimator_cf.estimate_closed_form(\n",
    "        data=d_data_N,\n",
    "        estimator_func=lambda data, **kw: gamma_estimator_closed_form(data, kw['delta']),\n",
    "        se_func=lambda est, data, **kw: gamma_std_error(est, data, kw['delta']),\n",
    "        true_value=gamma_true,\n",
    "        delta=delta_true\n",
    "    )\n",
    "    results_closed_form.append(result_cf)\n",
    "    \n",
    "    # Method 2: Numerical optimization (Hessian for SE)\n",
    "    estimator_opt = MLEEstimator(\n",
    "        loglik_func=lambda g, data, **kw: loglik_gamma_t2(g, data, kw['delta']),\n",
    "        param_name=\"γ\"\n",
    "    )\n",
    "    result_opt = estimator_opt.estimate(\n",
    "        data=d_data_N,\n",
    "        initial_guess=1.0,\n",
    "        bounds=(0.1, 20.0),\n",
    "        true_value=gamma_true,\n",
    "        delta=delta_true\n",
    "    )\n",
    "    results_optimizer.append(result_opt)\n",
    "    \n",
    "    print(f\"N={N:>7}: h₂={h2_empirical:.5f} | γ̂(CF)={result_cf.estimate:.5f} ({result_cf.std_error:.5f}) | \"\n",
    "          f\"γ̂(Opt)={result_opt.estimate:.5f} ({result_opt.std_error:.5f})\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "T=2 MODEL: ESTIMATING γ (δ KNOWN)\n",
      "=====================================================================================\n",
      "\n",
      "True parameters: γ=3.0, δ=2.0, w=0.55\n",
      "Theoretical h₂ = 0.4000\n",
      "\n",
      "Estimating γ for different sample sizes...\n",
      "-------------------------------------------------------------------------------------\n",
      "N=     50: h₂=0.40000 | γ̂(CF)=3.00000 (0.86603) | γ̂(Opt)=3.00001 (0.86602)\n",
      "N=    500: h₂=0.38800 | γ̂(CF)=3.15464 (0.28952) | γ̂(Opt)=3.15464 (0.28952)\n",
      "N=  10000: h₂=0.39920 | γ̂(CF)=3.01002 (0.06146) | γ̂(Opt)=3.01002 (0.06146)\n",
      "N= 100000: h₂=0.40115 | γ̂(CF)=2.98567 (0.04307) | γ̂(Opt)=2.98567 (0.04307)\n",
      "N= 500000: h₂=0.40115 | γ̂(CF)=2.98567 (0.04307) | γ̂(Opt)=2.98567 (0.04307)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2. Two-periods Model (Estimating `delta`)\n",
    "\n",
    "The tutorial file illustrates how to estimate when the other parameters are given to the econometrician in $T=2$ case. Repeat the exercise where we instead estimate with the same parameter\n",
    "values. That is, set $(\\gamma, \\delta, w, \\beta, T) = (3.0, 2.0, 0.55, 0.96, 2)$ in simulation and i) provide a closed-form MLE estimator and Fisher information for $\\delta$, ii) generate a table of estimation result for  $\\delta$ similar to the one presented in the tutorial file, and iii) verify that your estimates match with any optimization package in your programming language of choice. You are welcome to try out other number of agents or\n",
    "use other parameter values."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:47.140863Z",
     "start_time": "2025-11-02T04:22:47.138223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# QUESTION 3: Estimating delta (gamma known) in T=2 Model\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "TASK: Derive and implement MLE estimator for delta when gamma is known.\n",
    "\n",
    "Following the same approach as Section 4.1 (estimating gamma), you need to:\n",
    "\n",
    "1. DERIVE CLOSED-FORM MLE FOR DELTA:\n",
    "   Starting from the log-likelihood:\n",
    "   log L(delta) = sum_i [d_i2 * log(gamma/(gamma+delta)) + (1-d_i2) * log(delta/(gamma+delta))]\n",
    "\n",
    "   Take derivative with respect to delta, set equal to zero, and solve.\n",
    "   HINT: The solution will be symmetric to the gamma case.\n",
    "\n",
    "   Expected form: delta_hat = f(gamma, sum(d_i2), N)\n",
    "\n",
    "2. DERIVE FISHER INFORMATION FOR DELTA:\n",
    "   I_N(delta) = -N * E[d^2 log f / d(delta)^2]\n",
    "\n",
    "   This gives you the standard error: SE(delta_hat) = sqrt(1 / I_N(delta_hat))\n",
    "\n",
    "3. IMPLEMENT THE FUNCTIONS BELOW:\n",
    "\"\"\"\n",
    "\n",
    "def loglik_delta_t2(delta: float, d_data: np.ndarray, gamma: float) -> float:\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for delta in T=2 model (gamma known).\n",
    "\n",
    "    TODO: Implement this function\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    delta : float\n",
    "        Parameter to estimate (prior failures)\n",
    "    d_data : np.ndarray\n",
    "        Decision data (N x 2), column 1 = t=2 decisions\n",
    "    gamma : float\n",
    "        Known prior successes parameter\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    neg_loglik : float\n",
    "        Negative log-likelihood (for minimization)\n",
    "\n",
    "    HINT: This is symmetric to loglik_gamma_t2. Replace gamma <-> delta\n",
    "          and note that p = gamma/(gamma+delta), so 1-p = delta/(gamma+delta)\n",
    "    \"\"\"\n",
    "    # TODO: YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "def delta_estimator_closed_form(d_data: np.ndarray, gamma: float) -> float:\n",
    "    \"\"\"\n",
    "    Closed-form MLE for delta in T=2 model.\n",
    "\n",
    "    TODO: Derive and implement the closed-form solution\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    d_data : np.ndarray\n",
    "        Decision data (N x 2)\n",
    "    gamma : float\n",
    "        Known prior successes parameter\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    delta_hat : float\n",
    "        MLE estimate of delta\n",
    "\n",
    "    HINT: Follow the derivation from Section 4.1, but solve for delta instead.\n",
    "          The score equation is:\n",
    "          sum_i [d_i2 * (-1/(gamma+delta)) + (1-d_i2) * (1/delta - 1/(gamma+delta))] = 0\n",
    "    \"\"\"\n",
    "    # TODO: YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "def delta_std_error(delta_hat: float, d_data: np.ndarray, gamma: float) -> float:\n",
    "    \"\"\"\n",
    "    Standard error for delta MLE in T=2 model.\n",
    "\n",
    "    TODO: Derive and implement Fisher information for delta\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    delta_hat : float\n",
    "        Estimated delta\n",
    "    d_data : np.ndarray\n",
    "        Decision data\n",
    "    gamma : float\n",
    "        Known gamma\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    se : float\n",
    "        Standard error\n",
    "\n",
    "    HINT: The Fisher information should be symmetric to the gamma case.\n",
    "          I_N(delta) = N * gamma / [delta * (gamma + delta)^2]\n",
    "    \"\"\"\n",
    "    # TODO: YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PSEUDO-CODE FOR ESTIMATION EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "PSEUDO-CODE: Estimating delta with varying sample sizes\n",
    "\n",
    "# Step 1: Set true parameters\n",
    "gamma_true = 3.0\n",
    "delta_true = 2.0  # This is what we want to estimate\n",
    "w_true = 0.55\n",
    "beta_true = 0.96\n",
    "\n",
    "# Step 2: Generate simulated data\n",
    "for each agent i:\n",
    "    1. Draw ability: xi_i ~ Beta(gamma_true, delta_true)\n",
    "    2. Period 1: All agents invent, observe outcome x_i1 ~ Bernoulli(xi_i)\n",
    "    3. Update beliefs:\n",
    "       - If x_i1 = 1 (success): (gamma_i2, delta_i2) = (gamma_true + 1, delta_true)\n",
    "       - If x_i1 = 0 (failure): (gamma_i2, delta_i2) = (gamma_true, delta_true + 1)\n",
    "    4. Period 2: Agent invents if gamma_i2/(gamma_i2+delta_i2) >= w_true\n",
    "       Store decision d_i2 in {0, 1}\n",
    "\n",
    "# Step 3: For each sample size N in [50, 500, 10000, 100000, 500000]:\n",
    "    a. Take first N observations from simulated data\n",
    "    b. Compute empirical hazard: h2 = (# who quit at t=2) / N\n",
    "\n",
    "    c. Method 1 - Closed form:\n",
    "       - Compute delta_hat using closed-form formula\n",
    "       - Compute standard error using Fisher information\n",
    "\n",
    "    d. Method 2 - Numerical optimization:\n",
    "       - Minimize negative log-likelihood using scipy.optimize\n",
    "       - Extract standard error from Hessian\n",
    "\n",
    "    e. Store results\n",
    "\n",
    "# Step 4: Create comparison table\n",
    "Print table showing:\n",
    "- Sample size N\n",
    "- Empirical hazard h2\n",
    "- Closed-form estimate and SE\n",
    "- Numerical estimate and SE\n",
    "- Bias = delta_hat - delta_true\n",
    "\n",
    "# Step 5: Verify equivalence\n",
    "Check that both methods give the same answer (up to numerical precision)\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# TODO: IMPLEMENT YOUR SOLUTION HERE\n",
    "# ============================================================================\n",
    "\n",
    "# After implementing the three functions above, use this template:\n",
    "\n",
    "def run_delta_estimation_experiment():\n",
    "    \"\"\"\n",
    "    TODO: Complete this function to run the full estimation experiment\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"T=2 MODEL: ESTIMATING DELTA (gamma known)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # TODO: Generate data\n",
    "    # TODO: Loop over sample sizes\n",
    "    # TODO: Estimate using both methods\n",
    "    # TODO: Print results table\n",
    "\n",
    "    pass\n",
    "\n",
    "# Uncomment when ready to run:\n",
    "# run_delta_estimation_experiment()"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.3. Five-periods Model Estimation [Question 4]\n",
    "\n",
    "In this section, we extend the T=2 analysis to a five-period model (T=5). The key advantage of longer panels is **richer information**: instead of observing only $h_2$, we now observe hazard rates $\\{h_2, h_3, h_4, h_5\\}$, which should improve parameter identification.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand how to compute theoretical hazard rates by tracking all possible histories\n",
    "2. Implement backward induction for T=5\n",
    "3. Estimate parameters using different information sets\n",
    "4. Compare estimation efficiency across specifications\n",
    "\n",
    "---\n",
    "\n",
    "#### 4.3.1. Theoretical Hazard Rates\n",
    "\n",
    "The hazard rate at period $t$ is the probability that an agent quits at $t$ conditional on having invented through period $t-1$:\n",
    "\n",
    "$$h_t(w) = \\mathbb{P}(d_t=0 \\mid d_1=1, d_2=1, \\ldots, d_{t-1}=1)$$\n",
    "\n",
    "To compute $h_t$, we enumerate all possible success/failure sequences from period 1 to $t-1$, weight each path by its probability, and sum the quit probabilities.\n",
    "\n",
    "**Key Notation:**\n",
    "- $d_t$: Decision at period $t$ (0=quit, 1=invent)\n",
    "- $x_t$: Outcome at period $t$ if inventing (0=failure, 1=success)\n",
    "- $(\\gamma_t, \\delta_t)$: Belief state at period $t$\n",
    "- $d^*(\\gamma_t, \\delta_t)$: Optimal decision rule from solving the DP\n",
    "- History: Sequence of past outcomes, e.g., $(x_1, x_2, \\ldots, x_{t-1})$\n",
    "\n",
    "**Belief Evolution:**\n",
    "Starting from $(\\gamma_1, \\delta_1) = (\\gamma_0, \\delta_0)$:\n",
    "- After success ($x_t=1$): $(\\gamma_{t+1}, \\delta_{t+1}) = (\\gamma_t + 1, \\delta_t)$\n",
    "- After failure ($x_t=0$): $(\\gamma_{t+1}, \\delta_{t+1}) = (\\gamma_t, \\delta_t + 1)$\n",
    "- If quit ($d_t=0$): Beliefs don't update\n",
    "\n",
    "---\n",
    "\n",
    "#### 4.3.2. Formula for $h_2, h_3, h_4, h_5$\n",
    "\n",
    "Let's derive hazard formulas step by step.\n",
    "\n",
    "**Period 2: $h_2$**\n",
    "\n",
    "At $t=1$, everyone starts with $(\\gamma_0, \\delta_0)$. Assume all agents invent at $t=1$ (we choose parameters to ensure this). The period 1 outcome determines beliefs at $t=2$:\n",
    "\n",
    "$$h_2 = \\mathbb{P}(d_2=0 \\mid d_1=1)$$\n",
    "\n",
    "There are two possible histories:\n",
    "1. **Success path** ($x_1=1$): Beliefs become $(\\gamma_0+1, \\delta_0)$\n",
    "   - Probability: $p_0 = \\frac{\\gamma_0}{\\gamma_0+\\delta_0}$\n",
    "   - Quit if $d^*(\\gamma_0+1, \\delta_0) = 0$\n",
    "\n",
    "2. **Failure path** ($x_1=0$): Beliefs become $(\\gamma_0, \\delta_0+1)$\n",
    "   - Probability: $1-p_0 = \\frac{\\delta_0}{\\gamma_0+\\delta_0}$\n",
    "   - Quit if $d^*(\\gamma_0, \\delta_0+1) = 0$\n",
    "\n",
    "$$h_2 = p_0 \\cdot [1-d^*(\\gamma_0+1, \\delta_0)] + (1-p_0) \\cdot [1-d^*(\\gamma_0, \\delta_0+1)]$$\n",
    "\n",
    "---\n",
    "\n",
    "**Period 3: $h_3$**\n",
    "\n",
    "At $t=2$, agents who invented at $t=1$ have updated beliefs. Their period 2 outcome determines beliefs at $t=3$. There are four possible 2-period histories:\n",
    "\n",
    "$$h_3 = \\mathbb{P}(d_3=0 \\mid d_1=1, d_2=1) = \\frac{\\mathbb{P}(d_1=1, d_2=1, d_3=0)}{\\ P(d_1=1, d_2=1)}$$\n",
    "\n",
    "Enumerate all paths that lead to inventing through $t=2$:\n",
    "\n",
    "1. **(Success, Success)**: $x_1=1, x_2=1$ → Beliefs: $(\\gamma_0+2, \\delta_0)$\n",
    "   - Path probability: $\\frac{\\gamma_0}{\\gamma_0+\\delta_0} \\cdot \\frac{\\gamma_0+1}{\\gamma_0+\\delta_0+1} \\cdot d^*(\\gamma_0, \\delta_0) \\cdot d^*(\\gamma_0+1, \\delta_0)$\n",
    "   - Quits at $t=3$ if: $d^*(\\gamma_0+2, \\delta_0) = 0$\n",
    "\n",
    "2. **(Success, Failure)**: $x_1=1, x_2=0$ → Beliefs: $(\\gamma_0+1, \\delta_0+1)$\n",
    "   - Path probability: $\\frac{\\gamma_0}{\\gamma_0+\\delta_0} \\cdot \\frac{\\delta_0}{\\gamma_0+\\delta_0+1} \\cdot d^*(\\gamma_0, \\delta_0) \\cdot d^*(\\gamma_0+1, \\delta_0)$\n",
    "   - Quits at $t=3$ if: $d^*(\\gamma_0+1, \\delta_0+1) = 0$\n",
    "\n",
    "3. **(Failure, Success)**: $x_1=0, x_2=1$ → Beliefs: $(\\gamma_0+1, \\delta_0+1)$\n",
    "   - Path probability: $\\frac{\\delta_0}{\\gamma_0+\\delta_0} \\cdot \\frac{\\gamma_0}{\\gamma_0+\\delta_0+1} \\cdot d^*(\\gamma_0, \\delta_0) \\cdot d^*(\\gamma_0, \\delta_0+1)$\n",
    "   - Quits at $t=3$ if: $d^*(\\gamma_0+1, \\delta_0+1) = 0$\n",
    "\n",
    "4. **(Failure, Failure)**: $x_1=0, x_2=0$ → Beliefs: $(\\gamma_0, \\delta_0+2)$\n",
    "   - Path probability: $\\frac{\\delta_0}{\\gamma_0+\\delta_0} \\cdot \\frac{\\delta_0+1}{\\gamma_0+\\delta_0+1} \\cdot d^*(\\gamma_0, \\delta_0) \\cdot d^*(\\gamma_0, \\delta_0+1)$\n",
    "   - Quits at $t=3$ if: $d^*(\\gamma_0, \\delta_0+2) = 0$\n",
    "\n",
    "$$h_3 = \\frac{\\sum_{\\text{4 paths}} [\\text{path prob} \\times \\text{quit indicator}]}{\\sum_{\\text{4 paths}} [\\text{path prob}]}$$\n",
    "\n",
    "The denominator normalizes by the survival probability through $t=2$: $(1-h_1)(1-h_2)$.\n",
    "\n",
    "---\n",
    "\n",
    "**Period 4 and 5: $h_4, h_5$**\n",
    "\n",
    "The same logic extends to $h_4$ (8 paths of length 3) and $h_5$ (16 paths of length 4). The formula becomes:\n",
    "\n",
    "$$h_t = \\frac{\\sum_{\\text{all paths of length } t-1} \\mathbb{P}(\\text{path}) \\cdot [1-d^*(\\text{final state})] \\cdot \\prod_{s=1}^{t-1} d^*(\\text{state at } s)}{\\sum_{\\text{all paths}} \\mathbb{P}(\\text{path}) \\cdot \\prod_{s=1}^{t-1} d^*(\\text{state at } s)}$$\n",
    "\n",
    "**Implementation Strategy:**\n",
    "1. Solve the DP to get decision rules $d^*(\\gamma, \\delta)$ for all states\n",
    "2. For each period $t$, enumerate $2^{t-1}$ binary paths\n",
    "3. For each path, compute: \n",
    "   - Path probability (product of success/failure probabilities)\n",
    "   - Whether all decisions along path are \"invent\"\n",
    "   - Whether agent quits at final node\n",
    "4. Sum to get $h_t$"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:47.147084Z",
     "start_time": "2025-11-02T04:22:47.144559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# QUESTION 4: Computing Theoretical Hazard Rates for T=5\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "OBJECTIVE\n",
    "---------\n",
    "Compute theoretical hazard rates h_t for t=2,3,4,5 and use them for MLE estimation.\n",
    "\n",
    "Hazard rate definition: h_t = P(quit at t | invented through t-1)\n",
    "\n",
    "This is the probability that an agent quits at period t, conditional on having\n",
    "CHOSEN to invent in all previous periods 1, 2, ..., t-1.\n",
    "\n",
    "\n",
    "STEP 1: Solve the Dynamic Programming Problem\n",
    "----------------------------------------------\n",
    "Set up and solve the T=5 model:\n",
    "\n",
    "    from dataclasses import dataclass\n",
    "\n",
    "    @dataclass\n",
    "    class ModelParameters:\n",
    "        gamma_0: float  # Initial prior successes\n",
    "        delta_0: float  # Initial prior failures\n",
    "        w: float        # Outside wage\n",
    "        beta: float     # Discount factor\n",
    "        T: int          # Time horizon\n",
    "\n",
    "    # Create model\n",
    "    params = ModelParameters(gamma_0=3.0, delta_0=2.0, w=0.55, beta=0.96, T=5)\n",
    "    model = BayesianLearningModel(params)\n",
    "    model.solve()  # Backward induction\n",
    "\n",
    "    # Now we have:\n",
    "    # model.D[t][gamma_idx, delta_idx] ∈ {0, 1}\n",
    "    #   where 0 = quit, 1 = invent\n",
    "    # model.V[t][gamma_idx, delta_idx] = value function\n",
    "\n",
    "\n",
    "STEP 2: Understand State Evolution\n",
    "-----------------------------------\n",
    "Beliefs evolve based on outcomes:\n",
    "\n",
    "    Initial state: (γ₁, δ₁) = (γ₀, δ₀)\n",
    "\n",
    "    After period t with decision d_t and outcome x_t:\n",
    "        if d_t = 0 (quit):\n",
    "            No further updates (agent exits)\n",
    "\n",
    "        if d_t = 1 (invent):\n",
    "            Observe outcome x_t ~ Bernoulli(γ_t / (γ_t + δ_t))\n",
    "\n",
    "            if x_t = 1 (success):\n",
    "                (γ_{t+1}, δ_{t+1}) = (γ_t + 1, δ_t)\n",
    "\n",
    "            if x_t = 0 (failure):\n",
    "                (γ_{t+1}, δ_{t+1}) = (γ_t, δ_t + 1)\n",
    "\n",
    "\n",
    "STEP 3: Enumerate All Possible Histories\n",
    "-----------------------------------------\n",
    "For period t, we need to track all possible outcome sequences from periods 1 to t-1.\n",
    "\n",
    "    from itertools import product\n",
    "\n",
    "    # Number of possible outcome sequences\n",
    "    t=2: 2^1 = 2   paths → [(0,), (1,)]\n",
    "    t=3: 2^2 = 4   paths → [(0,0), (0,1), (1,0), (1,1)]\n",
    "    t=4: 2^3 = 8   paths\n",
    "    t=5: 2^4 = 16  paths\n",
    "\n",
    "    # Generate all paths of length t-1\n",
    "    outcome_paths = list(product([0, 1], repeat=t-1))\n",
    "\n",
    "    # Each path is a tuple: (x_1, x_2, ..., x_{t-1})\n",
    "    # where x_s ∈ {0, 1} is the outcome at period s\n",
    "\n",
    "\n",
    "STEP 4: Compute Hazard Rate h_t\n",
    "--------------------------------\n",
    "For each outcome path, track:\n",
    "  1. Whether the agent CHOOSES to invent at each period\n",
    "  2. The probability of the path\n",
    "  3. Whether the agent quits at period t\n",
    "\n",
    "\n",
    "STEP 5: Compute All Hazard Rates\n",
    "---------------------------------\n",
    "    hazards = {}\n",
    "    for t in [2, 3, 4, 5]:\n",
    "        hazards[t] = compute_hazard_t(model, gamma_0, delta_0, t)\n",
    "\n",
    "    print(\"Theoretical hazards:\")\n",
    "    print(f\"h_2 = {hazards[2]:.4f}\")\n",
    "    print(f\"h_3 = {hazards[3]:.4f}\")\n",
    "    print(f\"h_4 = {hazards[4]:.4f}\")\n",
    "    print(f\"h_5 = {hazards[5]:.4f}\")\n",
    "\n",
    "\n",
    "STEP 6: Simulate Data\n",
    "---------------------\n",
    "Generate observed data from the model:\n",
    "\n",
    "\n",
    "STEP 7: Compute Empirical Hazard Rates\n",
    "---------------------------------------\n",
    "\n",
    "\n",
    "STEP 8: Maximum Likelihood Estimation\n",
    "--------------------------------------\n",
    "Estimate a structural parameter (e.g., w) using hazard information.\n",
    "\n",
    "Likelihood for each period:\n",
    "    L_t(θ) = Binomial(k_t | n_t, h_t(θ))\n",
    "\n",
    "where:\n",
    "    n_t = number of agents who invented through t-1\n",
    "    k_t = number who quit at t\n",
    "    h_t(θ) = theoretical hazard at t given parameter θ\n",
    "\n",
    "Log-likelihood:\n",
    "    log L(θ) = Σ_t [k_t log(h_t(θ)) + (n_t - k_t) log(1 - h_t(θ))]\n",
    "\n",
    "\n",
    "\n",
    "STEP 9: Compare Information Sets\n",
    "---------------------------------\n",
    "Estimate using different amounts of hazard information:\n",
    "\n",
    "    info_sets = [\n",
    "        ([2], \"Only h_2\"),\n",
    "        ([2, 3], \"h_2 and h_3\"),\n",
    "        ([2, 3, 4], \"h_2, h_3, h_4\"),\n",
    "        ([2, 3, 4, 5], \"All hazards\")\n",
    "    ]\n",
    "\n",
    "    # Standard errors can be computed from Hessian\n",
    "    # More information → smaller standard errors\n",
    "\n",
    "================================================================================\"\"\"\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOBJECTIVE\\n---------\\nCompute theoretical hazard rates h_t for t=2,3,4,5 and use them for MLE estimation.\\n\\nHazard rate definition: h_t = P(quit at t | invented through t-1)\\n\\nThis is the probability that an agent quits at period t, conditional on having\\nCHOSEN to invent in all previous periods 1, 2, ..., t-1.\\n\\n\\nSTEP 1: Solve the Dynamic Programming Problem\\n----------------------------------------------\\nSet up and solve the T=5 model:\\n\\n    from dataclasses import dataclass\\n\\n    @dataclass\\n    class ModelParameters:\\n        gamma_0: float  # Initial prior successes\\n        delta_0: float  # Initial prior failures\\n        w: float        # Outside wage\\n        beta: float     # Discount factor\\n        T: int          # Time horizon\\n\\n    # Create model\\n    params = ModelParameters(gamma_0=3.0, delta_0=2.0, w=0.55, beta=0.96, T=5)\\n    model = BayesianLearningModel(params)\\n    model.solve()  # Backward induction\\n\\n    # Now we have:\\n    # model.D[t][gamma_idx, delta_idx] ∈ {0, 1}\\n    #   where 0 = quit, 1 = invent\\n    # model.V[t][gamma_idx, delta_idx] = value function\\n\\n\\nSTEP 2: Understand State Evolution\\n-----------------------------------\\nBeliefs evolve based on outcomes:\\n\\n    Initial state: (γ₁, δ₁) = (γ₀, δ₀)\\n\\n    After period t with decision d_t and outcome x_t:\\n        if d_t = 0 (quit):\\n            No further updates (agent exits)\\n\\n        if d_t = 1 (invent):\\n            Observe outcome x_t ~ Bernoulli(γ_t / (γ_t + δ_t))\\n\\n            if x_t = 1 (success):\\n                (γ_{t+1}, δ_{t+1}) = (γ_t + 1, δ_t)\\n\\n            if x_t = 0 (failure):\\n                (γ_{t+1}, δ_{t+1}) = (γ_t, δ_t + 1)\\n\\n\\nSTEP 3: Enumerate All Possible Histories\\n-----------------------------------------\\nFor period t, we need to track all possible outcome sequences from periods 1 to t-1.\\n\\n    from itertools import product\\n\\n    # Number of possible outcome sequences\\n    t=2: 2^1 = 2   paths → [(0,), (1,)]\\n    t=3: 2^2 = 4   paths → [(0,0), (0,1), (1,0), (1,1)]\\n    t=4: 2^3 = 8   paths\\n    t=5: 2^4 = 16  paths\\n\\n    # Generate all paths of length t-1\\n    outcome_paths = list(product([0, 1], repeat=t-1))\\n\\n    # Each path is a tuple: (x_1, x_2, ..., x_{t-1})\\n    # where x_s ∈ {0, 1} is the outcome at period s\\n\\n\\nSTEP 4: Compute Hazard Rate h_t\\n--------------------------------\\nFor each outcome path, track:\\n  1. Whether the agent CHOOSES to invent at each period\\n  2. The probability of the path\\n  3. Whether the agent quits at period t\\n\\n\\nSTEP 5: Compute All Hazard Rates\\n---------------------------------\\n    hazards = {}\\n    for t in [2, 3, 4, 5]:\\n        hazards[t] = compute_hazard_t(model, gamma_0, delta_0, t)\\n\\n    print(\"Theoretical hazards:\")\\n    print(f\"h_2 = {hazards[2]:.4f}\")\\n    print(f\"h_3 = {hazards[3]:.4f}\")\\n    print(f\"h_4 = {hazards[4]:.4f}\")\\n    print(f\"h_5 = {hazards[5]:.4f}\")\\n\\n\\nSTEP 6: Simulate Data\\n---------------------\\nGenerate observed data from the model:\\n\\n\\nSTEP 7: Compute Empirical Hazard Rates\\n---------------------------------------\\n\\n\\nSTEP 8: Maximum Likelihood Estimation\\n--------------------------------------\\nEstimate a structural parameter (e.g., w) using hazard information.\\n\\nLikelihood for each period:\\n    L_t(θ) = Binomial(k_t | n_t, h_t(θ))\\n\\nwhere:\\n    n_t = number of agents who invented through t-1\\n    k_t = number who quit at t\\n    h_t(θ) = theoretical hazard at t given parameter θ\\n\\nLog-likelihood:\\n    log L(θ) = Σ_t [k_t log(h_t(θ)) + (n_t - k_t) log(1 - h_t(θ))]\\n\\n\\n\\nSTEP 9: Compare Information Sets\\n---------------------------------\\nEstimate using different amounts of hazard information:\\n\\n    info_sets = [\\n        ([2], \"Only h_2\"),\\n        ([2, 3], \"h_2 and h_3\"),\\n        ([2, 3, 4], \"h_2, h_3, h_4\"),\\n        ([2, 3, 4, 5], \"All hazards\")\\n    ]\\n\\n    # Standard errors can be computed from Hessian\\n    # More information → smaller standard errors\\n\\n================================================================================'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.4. Infinite Horizon Estimation [Question 5]\n",
    "\n",
    "Compute the hazard rate at the sixth period, $h_6$. What is the hazard rate given your structural parameters? Do you think this hazard rate would be useful to improve the MLE estimate? Present a table of estimation result of the structural parameter of your choice (either $\\gamma$ or $\\delta$) in $T= \\infty$ case. Discuss what you find as i) N increases and/or ii) more information on ht is used.\n",
    "\n",
    "Suppose now that individuals live forever. The lifetime utility is now $\\sum_{t=1}^{\\infty}\\beta^{t-1}(d_t\\mathbf{1} \\{x_t=1\\} + (1-d_t)w)$.\n",
    "\n",
    "We consider the Bellman representation:\n",
    "$$\n",
    "\\begin{align*}\n",
    "V(\\gamma,\\delta,w,\\beta)=\\underset{d\\in\\{0,1\\}}{\\max}\\; d \\frac{\\gamma}{\\gamma+\\delta} + (1-d)w + \\beta \\left(d\\left(\\frac{\\gamma}{\\gamma+\\delta}V(\\gamma+1,\\delta,w,\\beta)+\\frac{\\delta}{\\gamma+\\delta}V(\\gamma,\\delta+1,w,\\beta)\\right)+(1-d)V(\\gamma,\\delta,w,\\beta)\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "The following code shows value function iteration (VFI) and data simulation.\n",
    "\n",
    "**VFI**\n",
    "- Prepare a large enough grid of $\\gamma$ and $\\delta$ so that the value function is well-approximated for the infinite horizon case. In my code, I set the grid to be $(T+1)\\times(T+1)$ where $T$ is the time horizon I wish to solve the value function (100 is big enough, I think).\n",
    "- Make a guess for the value function. I chose $V(\\gamma,\\delta,w,\\beta)=\\frac{w}{1-\\beta}$ for all $\\gamma,\\delta$ (can you see why?)\n",
    "- With your initial guess, treat it as if it is indeed your value function. Put it on the RHS.\n",
    "- We will find a new value for the value function for all values of $\\gamma,\\delta$ in the grid by evaluating the Bellman representation. Note that if $\\gamma$ and $\\delta$ in the value function are on the boundary of the grid, we will tell the value function to not to increase the parameters (i.e., we are 'clipping' the grid of all possible values).\n",
    "- We just \"updated\" our value function!\n",
    "- Now, we have two value functions: our initial guess and the updated one. Are they significantly different? If so, toss the original guess and keep the updated one.\n",
    "- Put the updated one on the RHS in the Bellman representation and keep updating until the update is insignificant.\n",
    "- Contraction mapping theorem tells you that you can find the value function in this way.\n",
    "\n",
    "**Data simulation**\n",
    "- Let's first decide the values for the known structural parameters, $\\gamma, \\delta, w$, and $\\beta$.\n",
    "- You got your value function from VFI. Now, you can find the policy function by finding the \"argmax\".\n",
    "- With the policy function, let's give $N$ agents some initial $\\gamma$ and $\\delta$. Based on your policy function, you can tell whether the agents will invent or not (let's make the data so that everyone invents in the first period).\n",
    "- Now, update the parameters based on the outcome of invention, and check with policy function what the agents will choose. Note also that I \"clipped\" the updating the parameters if the agent has reached to the boundary of the grid, but this will not really happen since I simulated the date up to $t=100$ so that even in the worst case the parameters are not on the boundary.\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T04:22:47.153918Z",
     "start_time": "2025-11-02T04:22:47.151745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# QUESTION 5: Infinite Horizon Model (T = ∞)\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "OBJECTIVE\n",
    "---------\n",
    "Solve infinite horizon model using Value Function Iteration (VFI) instead of\n",
    "backward induction. Compute h_6 and discuss estimation.\n",
    "\n",
    "\n",
    "STEP 1: Understand the Infinite Horizon Bellman Equation\n",
    "---------------------------------------------------------\n",
    "When T = ∞, we can't use backward induction. Instead, we find the fixed point\n",
    "of the Bellman operator.\n",
    "\n",
    "Bellman equation:\n",
    "    V(γ, δ) = max{ V_invent(γ, δ), V_quit(γ, δ) }\n",
    "\n",
    "where:\n",
    "    V_quit(γ, δ) = w / (1 - β)\n",
    "        [Perpetual flow of w discounted at rate β]\n",
    "\n",
    "    V_invent(γ, δ) = p + β·E[V(γ', δ') | γ, δ]\n",
    "                    = p + β·[p·V(γ+1, δ) + (1-p)·V(γ, δ+1)]\n",
    "        where p = γ / (γ + δ)\n",
    "        [Current period flow utility + discounted expected continuation value]\n",
    "\n",
    "The value function V(γ, δ) is the unique fixed point satisfying this equation.\n",
    "\n",
    "\n",
    "STEP 2: Set Up State Space Grid\n",
    "--------------------------------\n",
    "\n",
    "STEP 3: Initialize Value Function\n",
    "----------------------------------\n",
    "\n",
    "STEP 4: Value Function Iteration (VFI)\n",
    "---------------------------------------\n",
    "Repeatedly apply the Bellman operator until convergence\n",
    "\n",
    "STEP 5: Simulate Agents Under Infinite Horizon Policy\n",
    "------------------------------------------------------\n",
    "\n",
    "STEP 6: Compute Hazard Rates\n",
    "-----------------------------\n",
    "Use the same function as in Question 4\n",
    "\n",
    "STEP 7: Analyze h_6 and Its Usefulness\n",
    "---------------------------------------\n",
    "    # Solve infinite horizon model\n",
    "\n",
    "    # Simulate\n",
    "\n",
    "    # Compare h_6 to h_5, h_4, ...\n",
    "    # If h_6 ≈ h_5, then period 6 adds little new information\n",
    "    # If h_6 << h_5, learning is still happening\n",
    "\n",
    "STEP 8: MLE Estimation with Different Sample Sizes\n",
    "---------------------------------------------------\n",
    "Study how estimates improve as N increases\n",
    "\n",
    "STEP 9: Compare Information Sets\n",
    "---------------------------------\n",
    "    info_sets = [\n",
    "        ([2], \"Only h_2\"),\n",
    "        ([2, 3], \"h_2, h_3\"),\n",
    "        ([2, 3, 4, 5], \"h_2 through h_5\"),\n",
    "        ([2, 3, 4, 5, 6], \"All hazards (including h_6)\")\n",
    "    ]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOBJECTIVE\\n---------\\nSolve infinite horizon model using Value Function Iteration (VFI) instead of\\nbackward induction. Compute h_6 and discuss estimation.\\n\\n\\nSTEP 1: Understand the Infinite Horizon Bellman Equation\\n---------------------------------------------------------\\nWhen T = ∞, we can\\'t use backward induction. Instead, we find the fixed point\\nof the Bellman operator.\\n\\nBellman equation:\\n    V(γ, δ) = max{ V_invent(γ, δ), V_quit(γ, δ) }\\n\\nwhere:\\n    V_quit(γ, δ) = w / (1 - β)\\n        [Perpetual flow of w discounted at rate β]\\n\\n    V_invent(γ, δ) = p + β·E[V(γ\\', δ\\') | γ, δ]\\n                    = p + β·[p·V(γ+1, δ) + (1-p)·V(γ, δ+1)]\\n        where p = γ / (γ + δ)\\n        [Current period flow utility + discounted expected continuation value]\\n\\nThe value function V(γ, δ) is the unique fixed point satisfying this equation.\\n\\n\\nSTEP 2: Set Up State Space Grid\\n--------------------------------\\n\\nSTEP 3: Initialize Value Function\\n----------------------------------\\n\\nSTEP 4: Value Function Iteration (VFI)\\n---------------------------------------\\nRepeatedly apply the Bellman operator until convergence\\n\\nSTEP 5: Simulate Agents Under Infinite Horizon Policy\\n------------------------------------------------------\\n\\nSTEP 6: Compute Hazard Rates\\n-----------------------------\\nUse the same function as in Question 4\\n\\nSTEP 7: Analyze h_6 and Its Usefulness\\n---------------------------------------\\n    # Solve infinite horizon model\\n\\n    # Simulate\\n\\n    # Compare h_6 to h_5, h_4, ...\\n    # If h_6 ≈ h_5, then period 6 adds little new information\\n    # If h_6 << h_5, learning is still happening\\n\\nSTEP 8: MLE Estimation with Different Sample Sizes\\n---------------------------------------------------\\nStudy how estimates improve as N increases\\n\\nSTEP 9: Compare Information Sets\\n---------------------------------\\n    info_sets = [\\n        ([2], \"Only h_2\"),\\n        ([2, 3], \"h_2, h_3\"),\\n        ([2, 3, 4, 5], \"h_2 through h_5\"),\\n        ([2, 3, 4, 5, 6], \"All hazards (including h_6)\")\\n    ]\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
